[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Eman Ajmal is a social analyst."
  },
  {
    "objectID": "Assignment4.html",
    "href": "Assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Difference between a weak and a strong entity set:\n\nA strong entity set can be identified uniquely by its own attributes, such as an Employee with an Employee ID.\nA weak entity set on the other hand, cannot be identified by its own attributes alone and depends on a strong entity (also known as the owner entity) for identification. An example could be a Dependent entity that relies on the Employee entity for its identification (e.g., an employee’s dependent might be identified by the Employee ID and Dependent’s name)."
  },
  {
    "objectID": "Assignment4.html#question-1",
    "href": "Assignment4.html#question-1",
    "title": "Assignment 4",
    "section": "",
    "text": "Difference between a weak and a strong entity set:\n\nA strong entity set can be identified uniquely by its own attributes, such as an Employee with an Employee ID.\nA weak entity set on the other hand, cannot be identified by its own attributes alone and depends on a strong entity (also known as the owner entity) for identification. An example could be a Dependent entity that relies on the Employee entity for its identification (e.g., an employee’s dependent might be identified by the Employee ID and Dependent’s name)."
  },
  {
    "objectID": "Assignment4.html#question-2",
    "href": "Assignment4.html#question-2",
    "title": "Assignment 4",
    "section": "Question 2",
    "text": "Question 2\nDesign an E-R diagram for keeping track of the scoring statistics of your favorite sports team. You should store the matches played, the scores in each match, the players in each match, and individual player scoring statistics for each match. Summary statistics should be modeled as derived attributes with an explanation as to how they are computed.\n\nDraw the E-R diagram using draw.io.\nExpand to all teams in the league (Hint: add team entity)"
  },
  {
    "objectID": "Assignment4.html#question-3",
    "href": "Assignment4.html#question-3",
    "title": "Assignment 4",
    "section": "Question 3",
    "text": "Question 3\nSQL exercise:\n\nWrite an SQL query using the university schema to find the ID of each student who has never taken a course at the university. Do this using no subqueries and no set operations (use an outer join).\n\n\n\nConsider the following database, write a query to find the ID of each employee with no manager. Note that an employee may simply have no manager listed or may have a null manager(use natural left outer join).\n\nSELECT e.ID\nFROM employee e\nOUTER JOIN manages m ON e.ID = m.ID\nWHERE m.manager_id IS NULL;"
  },
  {
    "objectID": "Assignment6.html",
    "href": "Assignment6.html",
    "title": "Assignment 6",
    "section": "",
    "text": "Question 1\n\nUsing JSON YouTube: YouTube uses JSON to deliver video metadata, comments, and user interactions through its API. The JSON structure is hierarchical and includes nested fields for video IDs, thumbnails, and statistics such as views and likes. YouTube’s backend likely uses BigQuery and other Google Cloud services to store and manage data. The API fetches this data in JSON format, commonly consumed by frontend JavaScript frameworks.\nUsing XML W3Schools: W3Schools offers XML data examples, including a sample file that provides information on books. The XML structure uses descriptive tags such as , , and . The data can be accessed using XML parsers in JavaScript or server-side languages. Technologies like DOM and XSLT are used to query or transform the XML structure for web display.\n\n\n\nQuestion 2\n\n\n\nSELECT student.ID\nFROM student\nLEFT JOIN advisor ON student.ID = advisor.s_ID\nWHERE advisor.i_ID IS NULL;\n\n\n\nSELECT I.ID, I.name\nFROM instructor I\nWHERE NOT EXISTS (\n  SELECT *\n  FROM course C\n  WHERE C.dept_name = I.dept_name\n    AND NOT EXISTS (\n      SELECT *\n      FROM teaches T\n      WHERE T.ID = I.ID AND T.course_id = C.course_id\n    )\n)\nORDER BY I.name;\n\n\nQuestion 3\n\n\n\n\n   id     name              dept_name      salary\n1  63395  McKinnon          Cybernetics    94333.99\n2  78699  Pingr             Statistics     59303.62\n3  96895  Mird              Marketing      119921.41\n4  4233   Luo               English        88791.45\n5  4034   Murata            Athletics      61387.56\n6  50885  Konstantinides    Languages      32570.50\n\n\n\n   id     name              dept_name      salary\n1  63395  McKinnon          Cybernetics    94333.99\n2  78699  Pingr             Statistics     59303.62\n3  96895  Mird              Marketing      119921.41\n4  4233   Luo               English        88791.45\n5  4034   Murata            Athletics      61387.56\n6  50885  Konstantinides    Languages      32570.50\n\n\n\n    id       name           dept_name       tot_cred\n1   79352    Rumat          Finance          100\n2   76672    Miliko         Statistics       116\n3   14182    Moszkowski     Civil Eng.       73\n4   44985    Prieto         Biology          91\n5   44271    Sowerby        English          108\n6   40897    Coppens        Math             58\n\n\n\n   id       name           dept_name        salary\n1  63395    McKinnon       Cybernetics      94333.99\n2  78699    Pingr          Statistics       59303.62\n3  96895    Mird           Marketing        119921.41\n4  4233     Luo            English          88791.45\n5  4034     Murata         Athletics        61387.56\n6  50885    Konstantinides Languages        32570.50\n7  79653    Levine         Elec. Eng.       89805.83\n8  50330    Shuming        Physics          108011.81\n9  80759    Queiroz        Biology          45538.32\n10 73623    Sullivan       Elec. Eng.       90038.09\n11 97302    Bertolino      Mech. Eng.       51647.57\n12 57180    Hau            Accounting       43966.29\n13 74420    Voronina       Physics          121141.99\n14 35579    Soisalon-...   Psychology       62579.61\n15 31955    Moreira        Accounting       71351.42\n16 37687    Arias          Statistics       104563.38\n17 6569     Mingoz         Finance          105311.38\n18 16807    Yazdi          Athletics        98333.65\n19 14365    Lembr          Accounting       32241.56\n20 90643    Choll          Statistics       57807.09\n21 81991    Valtchev       Biology          77036.18\n22 95030    Arinb          Statistics       54805.11\n23 15347    Bawa           Athletics        72140.88\n24 74426    Kenje          Marketing        106554.73\n25 42782    Vicentino      Elec. Eng.       34272.67\n26 58558    Dusserre       Marketing        66143.25\n27 63287    Jaekel         Athletics        103146.87\n28 59795    Desyl          Languages        48803.38\n29 22591    DAgostino      Psychology       59706.49\n30 48570    Sarkar         Pol. Sci.        87549.80\n31 79081    Ullman         Accounting       47307.10\n32 52647    Bancilhon      Pol. Sci.        87958.01\n33 25946    Liley          Languages        90891.69\n34 36897    Morris         Marketing        43770.36\n35 72553    Yin            English          46397.59\n36 3199     Gustafsson     Elec. Eng.       82534.37\n37 34175    Bondi          Comp. Sci.       115469.11\n38 48507    Lent           Mech. Eng.       107978.47\n39 65931    Pimenta        Cybernetics      79866.95\n40 3335     Bourrier       Comp. Sci.       80797.83\n41 64871    Gutierrez      Statistics       45310.53\n42 95709    Sakurai        English          118143.98\n43 43779    Romero         Astronomy        79070.08\n44 77346    Mahmoud        Geology          99382.59\n45 28097    Kean           English          35023.18\n46 90376    Bietzk         Cybernetics      117836.50\n47 28400    Atanassov      Statistics       84982.92\n48 41930    Tung           Athletics        50482.03\n49 19368    Wieland        Pol. Sci.        124651.41\n50 99052    Dale           Cybernetics      93348.83"
  },
  {
    "objectID": "Assignment3.html",
    "href": "Assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Write SQL codes to get a list of:\n\nStudents IDs (hint: from the takes relation)\n\n\n\nInstructors\n\n\n\nDepartments\n\n\n\n\n\nWrite in SQL codes to do following queries:\n\nFind the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\n\n\n\nAdd grades to the list\n\n\n\nFind the ID and name of each student who has not taken any course offered before 2017.\n\n\n\nFor each department, find the maximum salary of instructors in that department. You may assume that every department has at least one instructor.\n\n\n\nFind the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\n\n\n\nAdd names to the list\n\n\n\n\n\nFind instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)"
  },
  {
    "objectID": "Assignment3.html#question-2",
    "href": "Assignment3.html#question-2",
    "title": "Assignment 3",
    "section": "",
    "text": "Write SQL codes to get a list of:\n\nStudents IDs (hint: from the takes relation)\n\n\n\nInstructors\n\n\n\nDepartments"
  },
  {
    "objectID": "Assignment3.html#question-3",
    "href": "Assignment3.html#question-3",
    "title": "Assignment 3",
    "section": "",
    "text": "Write in SQL codes to do following queries:\n\nFind the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\n\n\n\nAdd grades to the list\n\n\n\nFind the ID and name of each student who has not taken any course offered before 2017.\n\n\n\nFor each department, find the maximum salary of instructors in that department. You may assume that every department has at least one instructor.\n\n\n\nFind the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\n\n\n\nAdd names to the list"
  },
  {
    "objectID": "Assignment3.html#question-4",
    "href": "Assignment3.html#question-4",
    "title": "Assignment 3",
    "section": "",
    "text": "Find instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Name and describe three applications you have used that employed a database system to store and access persistent data (e.g., airlines, online trade, banking, university system).\n\n\n\nHow it Uses Databases: Stores customer data, transactions, and account details.\nReasoning: Ensures secure, fast, and reliable access to banking information.\n\n\n\n\n\nHow it Uses Databases: Manages product listings, customer orders, and payments.\nReasoning: Uses a relational database to store structured data and provide recommendations.\n\n\n\n\n\nHow it Uses Databases: Stores student records, enrollments, and grades.\nReasoning: Ensures students and faculty have real-time access to course materials and academic records.\n\n\n\n\n\nPropose three applications in domain projects (e.g., criminology, economics, brain science, etc.). Be sure to include: 1.Purpose 2.Functions 3.Simple interface design\n\n\n\nPurpose: Measures and tracks social well-being indicators like mental health, employment, and community engagement.\nFunctions:\n\nCollects survey responses on life satisfaction, stress levels, and social interactions.\nTracks changes in employment, education, and local crime rates.\nProvides visual reports and insights for policymakers and community leaders.\n\nSimple Interface Design: A dashboard displays key indicators like mental health scores, employment trends, and community engagement in charts and graphs. Users can fill out surveys to update data, and policymakers can generate reports with visual insights. The interface includes a homepage overview, survey page, and a report export option.\n\n\n\n\n\nPurpose: Analyzes public sentiment on social issues by gathering data from social media and news sources.\nFunctions:\n\nCollects and categorizes posts on topics like inequality, human rights, and policy changes.\nUses sentiment analysis to determine whether opinions are positive, negative, or neutral.\nGenerates reports on trending social issues over time.\n\nSimple Interface Design: Users enter keywords to analyze public sentiment from news and social media, displayed through a word cloud, sentiment pie chart, and regional heatmap. A live feed shows relevant posts, and a report generation page allows data exports. The interface is search-focused with real-time insights.\n\n\n\n\n\nPurpose: Tracks data on economic and social mobility to understand inequality trends in different regions.\nFunctions:\n\nStores data on income, education, and employment trends across communities.\nProvides comparison charts on social mobility rates based on location, gender, and ethnicity.\nRecommends policies and resources to improve access to opportunities.\n\nSimple Interface Design: An interactive heat map and dashboard display income trends, education levels, and employment data by region. Users can compare multiple areas, view bar charts, and access policy recommendations. The interface supports searching, visual comparisons, and report downloads.\n\n\n\n\nIf data can be retrieved efficiently and effectively, why is data mining needed?\nData mining is essential because it allows us to extract meaningful insights from large datasets beyond just retrieving stored data. Below are key reasons why data mining is needed:\n\n\n\n\nDatabases can retrieve data efficiently, but they don’t automatically find patterns or trends.\n\n\n\n\n\nData mining helps predict future trends based on historical data.\n\n\n\n\n\nBusinesses use data mining to make data-driven decisions rather than relying on intuition.\n\n\n\n\n\nBig data is too large and complex for traditional database queries alone. Data mining helps extract valuable insights from massive datasets.\n\n\n\n\n\nCompanies use data mining to gain a competitive edge by identifying market trends, customer preferences, and business risks.\n\n\n\n\n\nDescribe at least three tables that might be used to store information in a social network/social media system such as Twitter or Reddit.\nA social media platform like Twitter or Reddit would require multiple tables for structured data management. Below are three essential tables:\n\n\n\nStores information about users.\n\n\n\nColumn Name\nDescription\n\n\n\n\nUser ID\nUnique identifier for each user\n\n\nUsername\nDisplay name of the user\n\n\nEmail\nContact email\n\n\nPassword\nEncrypted password\n\n\nDate Joined\nDate when the user registered\n\n\n\n\n\n\n\n\nStores tweets or Reddit posts.\n\n\n\nColumn Name\nDescription\n\n\n\n\nPost ID\nUnique identifier for each post\n\n\nUser ID\nID of the user who posted\n\n\nPost Content\nText or media content\n\n\nCreated At\nTimestamp of post creation\n\n\nLikes Count\nNumber of likes\n\n\nComments Count\nNumber of comments\n\n\n\n\n\n\n\n\nStores relationships between users.\n\n\n\nColumn Name\nDescription\n\n\n\n\nFollower ID\nID of the user who follows\n\n\nFollowed ID\nID of the user being followed\n\n\nDate Followed\nTimestamp of when the follow occurred"
  },
  {
    "objectID": "Assignment1.html#question-1",
    "href": "Assignment1.html#question-1",
    "title": "Assignment 1",
    "section": "",
    "text": "Name and describe three applications you have used that employed a database system to store and access persistent data (e.g., airlines, online trade, banking, university system).\n\n\n\nHow it Uses Databases: Stores customer data, transactions, and account details.\nReasoning: Ensures secure, fast, and reliable access to banking information.\n\n\n\n\n\nHow it Uses Databases: Manages product listings, customer orders, and payments.\nReasoning: Uses a relational database to store structured data and provide recommendations.\n\n\n\n\n\nHow it Uses Databases: Stores student records, enrollments, and grades.\nReasoning: Ensures students and faculty have real-time access to course materials and academic records."
  },
  {
    "objectID": "Assignment1.html#question-2",
    "href": "Assignment1.html#question-2",
    "title": "Assignment 1",
    "section": "",
    "text": "Propose three applications in domain projects (e.g., criminology, economics, brain science, etc.). Be sure to include: 1.Purpose 2.Functions 3.Simple interface design\n\n\n\nPurpose: Measures and tracks social well-being indicators like mental health, employment, and community engagement.\nFunctions:\n\nCollects survey responses on life satisfaction, stress levels, and social interactions.\nTracks changes in employment, education, and local crime rates.\nProvides visual reports and insights for policymakers and community leaders.\n\nSimple Interface Design: A dashboard displays key indicators like mental health scores, employment trends, and community engagement in charts and graphs. Users can fill out surveys to update data, and policymakers can generate reports with visual insights. The interface includes a homepage overview, survey page, and a report export option.\n\n\n\n\n\nPurpose: Analyzes public sentiment on social issues by gathering data from social media and news sources.\nFunctions:\n\nCollects and categorizes posts on topics like inequality, human rights, and policy changes.\nUses sentiment analysis to determine whether opinions are positive, negative, or neutral.\nGenerates reports on trending social issues over time.\n\nSimple Interface Design: Users enter keywords to analyze public sentiment from news and social media, displayed through a word cloud, sentiment pie chart, and regional heatmap. A live feed shows relevant posts, and a report generation page allows data exports. The interface is search-focused with real-time insights.\n\n\n\n\n\nPurpose: Tracks data on economic and social mobility to understand inequality trends in different regions.\nFunctions:\n\nStores data on income, education, and employment trends across communities.\nProvides comparison charts on social mobility rates based on location, gender, and ethnicity.\nRecommends policies and resources to improve access to opportunities.\n\nSimple Interface Design: An interactive heat map and dashboard display income trends, education levels, and employment data by region. Users can compare multiple areas, view bar charts, and access policy recommendations. The interface supports searching, visual comparisons, and report downloads.\n\n\n\n\nIf data can be retrieved efficiently and effectively, why is data mining needed?\nData mining is essential because it allows us to extract meaningful insights from large datasets beyond just retrieving stored data. Below are key reasons why data mining is needed:\n\n\n\n\nDatabases can retrieve data efficiently, but they don’t automatically find patterns or trends.\n\n\n\n\n\nData mining helps predict future trends based on historical data.\n\n\n\n\n\nBusinesses use data mining to make data-driven decisions rather than relying on intuition.\n\n\n\n\n\nBig data is too large and complex for traditional database queries alone. Data mining helps extract valuable insights from massive datasets.\n\n\n\n\n\nCompanies use data mining to gain a competitive edge by identifying market trends, customer preferences, and business risks."
  },
  {
    "objectID": "Assignment1.html#question-6",
    "href": "Assignment1.html#question-6",
    "title": "Assignment 1",
    "section": "",
    "text": "Describe at least three tables that might be used to store information in a social network/social media system such as Twitter or Reddit.\nA social media platform like Twitter or Reddit would require multiple tables for structured data management. Below are three essential tables:\n\n\n\nStores information about users.\n\n\n\nColumn Name\nDescription\n\n\n\n\nUser ID\nUnique identifier for each user\n\n\nUsername\nDisplay name of the user\n\n\nEmail\nContact email\n\n\nPassword\nEncrypted password\n\n\nDate Joined\nDate when the user registered\n\n\n\n\n\n\n\n\nStores tweets or Reddit posts.\n\n\n\nColumn Name\nDescription\n\n\n\n\nPost ID\nUnique identifier for each post\n\n\nUser ID\nID of the user who posted\n\n\nPost Content\nText or media content\n\n\nCreated At\nTimestamp of post creation\n\n\nLikes Count\nNumber of likes\n\n\nComments Count\nNumber of comments\n\n\n\n\n\n\n\n\nStores relationships between users.\n\n\n\nColumn Name\nDescription\n\n\n\n\nFollower ID\nID of the user who follows\n\n\nFollowed ID\nID of the user being followed\n\n\nDate Followed\nTimestamp of when the follow occurred"
  },
  {
    "objectID": "Assignment2.html",
    "href": "Assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Defines the structure of a relation, specifying its attributes and data types.\n\n\n\nA table containing rows that follow the schema.\n\n\n\nA specific snapshot of data in the relation at a given time.\n\n\n\n\n\n\n\n\n\n\nSchema\nRelation (Table)\nInstance\n\n\n\n\nStudent(ID, Name, Major, Year)\nA table storing student records\nA collection of student data at a specific moment"
  },
  {
    "objectID": "Assignment2.html#question-1-differences-between-relation-schema-relation-and-instance",
    "href": "Assignment2.html#question-1-differences-between-relation-schema-relation-and-instance",
    "title": "Assignment 2",
    "section": "",
    "text": "Defines the structure of a relation, specifying its attributes and data types.\n\n\n\nA table containing rows that follow the schema.\n\n\n\nA specific snapshot of data in the relation at a given time.\n\n\n\n\n\n\n\n\n\n\nSchema\nRelation (Table)\nInstance\n\n\n\n\nStudent(ID, Name, Major, Year)\nA table storing student records\nA collection of student data at a specific moment"
  },
  {
    "objectID": "Assignment2.html#question-2-schema-diagram-for-bank-database",
    "href": "Assignment2.html#question-2-schema-diagram-for-bank-database",
    "title": "Assignment 2",
    "section": "Question 2: Schema Diagram for Bank Database",
    "text": "Question 2: Schema Diagram for Bank Database\nA bank database schema includes the following entities:\n\nBRANCH Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nbranch_name\nName of the branch\n\n\nbranch_city\nLocation of branch\n\n\nassets\nTotal branch assets\n\n\n\n\n\nACCOUNT Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nbranch_name\nAssociated branch (🔗 FK → Branch)\n\n\naccount_number\nUnique account ID\n\n\nbalance\nAccount balance\n\n\n\n\n\nLOAN Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nloan_number\nUnique loan ID\n\n\nbranch_name\nIssuing branch (🔗 FK → Branch)\n\n\namount\nLoan amount\n\n\n\n\n\nDEPOSITOR Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nID\nCustomer ID (🔗 FK → Customer)\n\n\naccount_number\nLinked account number (🔗 FK → Account)\n\n\n\n\n\nBORROWER Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nID\nCustomer ID (🔗 FK → Customer)\n\n\nloan_number\nLinked loan number (🔗 FK → Loan)\n\n\n\n\n\nCUSTOMER Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nID\nUnique customer ID\n\n\ncst_name\nCustomer’s name\n\n\ncst_street\nStreet address\n\n\ncst_city\nCity of residence\n\n\n\n\n\nRelationships\n\nOne Branch → Many Accounts (branch_name as FK in Account)\n\nOne Branch → Many Loans (branch_name as FK in Loan)\n\nOne Customer → Many Accounts (via Depositor table)\n\nOne Account → Many Customers (Joint accounts, via Depositor)\n\nOne Customer → Many Loans (via Borrower table)\n\nOne Loan → Many Customers (Co-borrowers, via Borrower)"
  },
  {
    "objectID": "Assignment2.html#question-3-primary-and-foreign-keys-for-the-bank-database",
    "href": "Assignment2.html#question-3-primary-and-foreign-keys-for-the-bank-database",
    "title": "Assignment 2",
    "section": "Question 3: Primary and Foreign Keys for the Bank Database",
    "text": "Question 3: Primary and Foreign Keys for the Bank Database\n\nPrimary Keys\n\nBranch: branch_name\nAccount: account_number\nLoan: loan_number\nDepositor: (ID, account_number)\nBorrower: (ID, loan_number)\nCustomer: ID\n\n\n\nForeign Keys\n\nAccount: branch_name\nLoan: branch_name\nDepositor: ID, account_number\nBorrower: ID, loan_number"
  },
  {
    "objectID": "Assignment5.html",
    "href": "Assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Question 1: E-R Diagram as a Graph\nAn E-R (Entity-Relationship) diagram can be viewed as a graph, where entities are nodes and relationships are edges. Understanding its structure helps ensure database integrity and effective design.\n\n\na) The graph is disconnected\nA disconnected graph means that some parts of the schema are isolated and do not connect to others. In terms of an enterprise schema, this implies that certain entities or relationships are not integrated with the rest of the database system.\nExample: Imagine we have a Doctor entity and a Patient entity, but no relationship (like Treats) connecting them. These two entities are disconnected in the E-R diagram.\nImplications: - Indicates missing relationships or a design flaw. - Can result in incomplete or difficult-to-query data. - Hurts the ability to enforce referential integrity.\n\n\nb) The graph has a cycle\nA cycle in an E-R diagram means that starting from one entity, you can follow a path of relationships and return to the same entity.\nExample: Doctor → Patient → Test → Doctor\nThis forms a loop (cycle) in the schema.\nImplications: - Not inherently wrong – cycles often represent real-world interconnectedness. - Can complicate queries, especially recursive ones. - May require careful normalization or handling to avoid data anomalies.\n\n\nQuestion 3: Why Do We Have Weak Entity Sets?\nWhile it’s possible to convert weak entity sets to strong ones by adding attributes, we still use weak entity sets in practice due to design clarity and efficiency.\nReasons We Use Weak Entity Sets:\n\nA weak entity cannot be uniquely identified by its own attributes.\nIt depends on a related strong entity for uniqueness.\n\nWhy Not Always Use Strong Entities?\n\nMaking weak entities strong can require adding redundant or artificial keys, which is not always natural.\nWeak entities simplify data modeling for real-world hierarchical relationships.\nHelps maintain referential integrity and logical clarity in the schema.\n\n\n\nQuestion 4\n\n\na: SQL Queries\nSchema:\nemployee(ID, name, street, city)\nworks(ID, company_id, salary)\ncompany(company_id, name, city)\nmanages(employee_id, manager_id)\n\nFind ID and name of employees who live in the same city as the location of their company:\n\nSELECT e.ID, e.name\nFROM employee AS e, works AS w, company AS c\nWHERE e.ID = w.ID AND w.company_id = c.company_id AND e.city = c.city;\n\nFind ID and name of employees who live on the same street and in the same city as their manager:\n\nSELECT e.ID, e.name\nFROM employee AS e, manages AS m, employee AS mngr\nWHERE e.ID = m.employee_id AND m.manager_id = mngr.ID\nAND e.city = mngr.city AND e.street = mngr.street;\n\nFind ID and name of employees who earn more than the average salary of all employees in their company:\n\nSELECT e.ID, e.name\nFROM employee AS e, works AS w\nWHERE e.ID = w.ID AND w.salary &gt; ( SELECT AVG(w2.salary)\nFROM works AS w2\nWHERE w2.company_id = w.company_id\n\n\nb: What’s wrong with this SQL query?\nSELECT name, title\nFROM instructor NATURAL JOIN teaches NATURAL JOIN section NATURAL JOIN course\nWHERE semester = ‘Spring’ AND year = 2017;\nProblem: This query uses NATURAL JOIN, which joins tables based on all columns with the same name. This can cause unintended matches, especially if tables have generic column names like ID or course_id.\nFix: Use explicit JOINs to clearly specify matching fields.\nCorrected Query:\nSELECT i.name, c.title\nFROM instructor AS i\nJOIN teaches AS t ON i.ID = t.ID\nJOIN section AS s ON t.course_id = s.course_id\nAND t.sec_id = s.sec_id\nAND t.semester = s.semester\nAND t.year = s.year JOIN course AS c ON s.course_id = c.course_id\nWHERE s.semester = ‘Spring’ AND s.year = 2017;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I am Eman Ajmal",
    "section": "",
    "text": "I’m a social data analyst passionate about turning real-world problems into actionable insights.\nI care deeply about development, equity, and evidence-based reform, and I’m driven by the belief that small interventions, rooted in data can spark long-term impact.\nCurrently pursuing my Master’s at UT Dallas, I focus on the intersection of data, people, and policy, and aim to build systems that inform, include, and improve.\n\n\n\n“Data is more than numbers — it’s people, stories, and change waiting to happen.”\n\n\n\nConnect with me"
  },
  {
    "objectID": "Project.html",
    "href": "Project.html",
    "title": "Project",
    "section": "",
    "text": "Eman Ajmal is a social analyst.\n\nplot(mtcars)"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "Richardson, Texas 75080\nPortfolio"
  },
  {
    "objectID": "CV.html#education",
    "href": "CV.html#education",
    "title": "CV",
    "section": "",
    "text": "University of Texas at Dallas — MS, Social Data Analytics and Research\nGPA: 3.89 | Expected December 2026\nForman Christian College, Lahore, Pakistan — BS, Sociology (Minor in Education)\nGPA: 3.47 | Graduated January 2024\nIstanbul Aydun University, Istanbul, Turkey — Digital Marketing, Delightful Summer Programme\nAugust 2022"
  },
  {
    "objectID": "CV.html#skills",
    "href": "CV.html#skills",
    "title": "CV",
    "section": "",
    "text": "Research & Data Analysis: Quantitative & qualitative research, survey design, SPSS, R Studio, SQL, data visualization, reporting\nMarketing Tools: Canva, Meta Ads, DV360, Google Ads, Keyword Analysis, Sprinklr\nTechnical: Microsoft Office, Advanced Excel, SketchUp, Slack, Discord, Zoom\nSoft Skills: Creativity, Adaptability, Communication, Problem Solving, Critical Thinking"
  },
  {
    "objectID": "CV.html#professional-experience",
    "href": "CV.html#professional-experience",
    "title": "CV",
    "section": "",
    "text": "Policy Research Analyst\nMay 2025 – Present | Richardson, TX\n- Analyze socio-economic datasets.\n- Draft policy indicators and summaries using insights from state and nonprofit sources.\n- Collaborate with faculty and research staff to refine data visualizations and narratives for public reporting.\n\n\n\nLead Social Listening\nApril 2024 – September 2024 | Lahore, Pakistan\n- Led sentiment/Regression analysis using Sprinklr & SPSS to uncover key trends and improve brand perception by 15%.\n- Streamlined the content review and feedback process by integrating real-time data, enhancing stakeholder collaboration and increasing operational efficiency.\n\n\n\nResearch & Development Intern\nOctober 2023 – December 2023 | Lahore, Pakistan\n- Conducted qualitative and quantitative research for the Gender Parity Report 2022, analyzing large-scale datasets to identify trends in gender equity across Punjab.\n- Collaborated with cross-functional teams to develop evidence-based policy recommendations aimed at improving gender parity and social outcomes."
  },
  {
    "objectID": "CV.html#academic-project-experience",
    "href": "CV.html#academic-project-experience",
    "title": "CV",
    "section": "",
    "text": "Thesis – Legal Awareness Among Female University Students in Lahore\nSeptember 2023 – January 2024\n- Led a comprehensive research study on legal knowledge among female students by surveying over 300 participants, uncovering critical gaps in legal literacy and identifying key opportunities for academic empowerment.\n- Developed and presented data-driven recommendations to academic stakeholders, driving a 25% increase in student engagement with legal literacy initiatives and demonstrating impactful leadership in addressing contemporary societal challenges."
  },
  {
    "objectID": "CV.html#publications-research",
    "href": "CV.html#publications-research",
    "title": "CV",
    "section": "",
    "text": "The Efficacy of Battering Intervention and Prevention Programs\nInstitute for Urban Policy Research (UTD), Texas Council on Family Violence — 2025\n- Contributed as Research Assistant on a statewide evaluation examining the impact of Battering Intervention and Prevention Programs (BIPPs) on reducing domestic violence recidivism across Texas. Analyzed program data and outcomes for over 1,600 participants, supporting evidence-based policy recommendations."
  },
  {
    "objectID": "CV.html#leadership-experience",
    "href": "CV.html#leadership-experience",
    "title": "CV",
    "section": "",
    "text": "International Center Global Ambassador (UTD) — Vice President\n\nArt Junction (FCCU) — Director PR\n\nAghaaz (NGO) — Executive Council"
  },
  {
    "objectID": "CV.html#keywords",
    "href": "CV.html#keywords",
    "title": "CV",
    "section": "",
    "text": "R Studio, SQL, SPSS, Sprinklr, Data Visualization, Policy Research, Public Reporting, Stakeholder Engagement"
  },
  {
    "objectID": "CV.html#institute-for-urban-policy-research-utd",
    "href": "CV.html#institute-for-urban-policy-research-utd",
    "title": "CV",
    "section": "Institute for Urban Policy Research (UTD)",
    "text": "Institute for Urban Policy Research (UTD)\nPolicy Research Analyst\nMay 2025 – Present | Richardson, TX\n- Analyze socio-economic datasets.\n- Draft policy indicators and summaries using insights from state and nonprofit sources.\n- Collaborate with faculty and research staff to refine data visualizations and narratives for public reporting."
  },
  {
    "objectID": "CV.html#nestlé-pakistan",
    "href": "CV.html#nestlé-pakistan",
    "title": "CV",
    "section": "Nestlé Pakistan",
    "text": "Nestlé Pakistan\nLead Social Listening\nApril 2024 – September 2024 | Lahore, Pakistan\n- Led sentiment/Regression analysis using Sprinklr & SPSS to uncover key trends and improve brand perception by 15%.\n- Streamlined the content review and feedback process by integrating real-time data, enhancing stakeholder collaboration and increasing operational efficiency."
  },
  {
    "objectID": "CV.html#punjab-commission-on-the-status-of-women",
    "href": "CV.html#punjab-commission-on-the-status-of-women",
    "title": "CV",
    "section": "Punjab Commission on the Status of Women",
    "text": "Punjab Commission on the Status of Women\nResearch & Development Intern\nOctober 2023 – December 2023 | Lahore, Pakistan\n- Conducted qualitative and quantitative research for the Gender Parity Report 2022, analyzing large-scale datasets to identify trends in gender equity across Punjab.\n- Collaborated with cross-functional teams to develop evidence-based policy recommendations aimed at improving gender parity and social outcomes."
  },
  {
    "objectID": "EPPS6302_Assignment1.html",
    "href": "EPPS6302_Assignment1.html",
    "title": "Part 4 — Write a one page note on designing your website",
    "section": "",
    "text": "I created my website using Quarto and GitHub Pages to share my assignments, CV, and projects. The layout is simple and professional, with a top navigation bar for easy access to pages like Home, CV, and course assignments. A right-side “On this page” panel helps visitors jump to sections quickly.\nThe site uses the Cosmo theme for clean styling and responsive design. Content is written in Markdown within .qmd files, making it easy to update and maintain. Images and files are stored in organized folders and linked using relative paths. I render the site locally and push updates to GitHub, where it’s automatically deployed.\nMy goal was to build a clear, functional, and easy-to-navigate portfolio that highlights my work while remaining simple to maintain and expand in the future."
  },
  {
    "objectID": "EPPS6302_Assignment1.html#episode-overview",
    "href": "EPPS6302_Assignment1.html#episode-overview",
    "title": "Assignment 1",
    "section": "Episode Overview",
    "text": "Episode Overview\nIn this episode of Data Framed, hosts Adel Nehme and Richie Cotton welcome Karen Ng, Head of Product at HubSpot, to discuss the evolving landscape of team dynamics where humans and AI agents collaborate side by side. The conversation centers on practical strategies for integrating AI agents into workflows and optimizing team performance within this hybrid environment."
  },
  {
    "objectID": "EPPS6302_Assignment1.html#main-theme",
    "href": "EPPS6302_Assignment1.html#main-theme",
    "title": "Assignment 1",
    "section": "Main Theme",
    "text": "Main Theme\nThe central theme of the episode is the rise of human-plus-agent hybrid teams work environments in which AI systems share responsibilities with human workers. Karen Ng breaks down how organizations can effectively blend AI capabilities with human oversight, ensuring both productivity and ethical integrity."
  },
  {
    "objectID": "EPPS6302_Assignment1.html#key-takeaways-insights",
    "href": "EPPS6302_Assignment1.html#key-takeaways-insights",
    "title": "Assignment 1",
    "section": "Key Takeaways & Insights",
    "text": "Key Takeaways & Insights\n\nIntegration Strategy\nHybrid teams succeed when roles are clearly defined, and AI systems are introduced where they augment not replace human judgment.\n\n\nTraining & Evaluation\nEstablishing best practices for training AI agents includes designing them for continuous learning and setting evaluation criteria aligned with team goals.\n\n\nData Quality & Governance\nA robust governance framework is crucial. AI agents must operate on accurate, reliable data while maintaining transparency and ethical standards.\n\n\nPractical Use Cases\nIndustries like customer support, sales, and data processing have seen measurable benefits by integrating AI agents to handle recurring tasks freeing humans to focus on complex, strategic work."
  },
  {
    "objectID": "EPPS6302_Assignment1.html#personal-reflection",
    "href": "EPPS6302_Assignment1.html#personal-reflection",
    "title": "Assignment 1",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nThis episode offers a forward-looking lens into how AI is reshaping team workflows. Karen Ng’s insights are grounded in real-world applications; her role at HubSpot lends credibility and depth to the discussion. I appreciated the balanced approach highlighting AI’s strengths while acknowledging the importance of human judgment, governance, and alignment with ethical standards.\nI find the emphasis on governance particularly impactful. Too often, the conversation around AI overlooks the importance of data integrity and trust. This episode prompts listeners to think critically about implementing AI in a way that enhances not erodes team coherence and accountability."
  },
  {
    "objectID": "EPPS6302_Assignment1.html#conclusion",
    "href": "EPPS6302_Assignment1.html#conclusion",
    "title": "Assignment 1",
    "section": "Conclusion",
    "text": "Conclusion\nEpisode 319 of Data Framed delivers a timely and practical exploration of hybrid human-AI teamwork. It’s especially valuable for product managers, data team leaders, and anyone working at the intersection of AI and organizational design. Whether you’re exploring AI adoption or refining existing deployments, this episode offers a thoughtful roadmap for doing so responsibly and effectively."
  },
  {
    "objectID": "GTrends.html",
    "href": "GTrends.html",
    "title": "GTrends",
    "section": "",
    "text": "```## EPPS 6302 Methods of Data Collection and Production ## Google Trends with R\ninstall.packages(“gtrendsR”) library(gtrendsR) TrumpHarrisElection = gtrends(c(“Trump”,“Harris”,“election”), onlyInterest = TRUE, geo = “US”, gprop = “web”, time = “today+5-y”, category = 0, ) # last five years the_df=TrumpHarrisElection$interest_over_time plot(TrumpHarrisElection) tg = gtrends(“tariff”, time = “all”)\n\nExample: Tariff, China military, Taiwan\n\n\n\nplot(gtrends(c(“tariff”), time = “all”)) data(“countries”) plot(gtrends(c(“tariff”), geo = “GB”, time = “all”)) plot(gtrends(c(“tariff”), geo = c(“US”,“GB”,“TW”), time = “all”)) tg_iot = tg\\(interest_over_time\ntct = gtrends(c(\"tariff\",\"China military\", \"Taiwan\"), time = \"all\")\ntct = data.frame(tct\\)interest_over_time) plot(gtrends(c(“tariff”,“China military”, “Taiwan”), time = “all”))"
  },
  {
    "objectID": "EPPS6302_Assignment2.html",
    "href": "EPPS6302_Assignment2.html",
    "title": "Overview",
    "section": "",
    "text": "#Assignment 2"
  },
  {
    "objectID": "EPPS6302_Assignment2.html#google-trends-website-export",
    "href": "EPPS6302_Assignment2.html#google-trends-website-export",
    "title": "Assignment 2",
    "section": "1) Google Trends website export",
    "text": "1) Google Trends website export"
  },
  {
    "objectID": "EPPS6302_Assignment2.html#r-gtrendsr-plot",
    "href": "EPPS6302_Assignment2.html#r-gtrendsr-plot",
    "title": "Assignment 2",
    "section": "2) R (gtrendsR) plot",
    "text": "2) R (gtrendsR) plot"
  },
  {
    "objectID": "EPPS6302_Assignment3.html",
    "href": "EPPS6302_Assignment3.html",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Source: U.S. Census Bureau, ACS 2023 5-year via tidycensus."
  },
  {
    "objectID": "EPPS6302_Assignment3.html#map",
    "href": "EPPS6302_Assignment3.html#map",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Source: U.S. Census Bureau, ACS 2023 5-year via tidycensus."
  },
  {
    "objectID": "EPPS6356_Assignment1.html",
    "href": "EPPS6356_Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "The classic Anscombe’s quartet consists of four (x, y) datasets that share the same means, variances, correlations, and regression coefficients — yet their scatterplots are visually very different. Below is my reproduction of the four datasets with their common least-squares line.\n\n\n\n\nAnscombe’s quartet demonstrates that datasets with identical statistical summaries can tell very different stories when visualized. While all four share the same means, variances, correlations, and regression lines, their scatterplots reveal distinct problems: a roughly linear set, a curved relationship, a single extreme outlier, and a high-leverage point that determines the slope. Without visualization, these issues remain hidden and lead to incorrect conclusions. To address this, start with graphs (scatterplots plus a smooth), then check residuals, leverage, and Cook’s distance to test assumptions. When assumptions are violated, use robust regression, apply transformations, add nonlinear terms (e.g., a quadratic), or collect additional data to improve coverage across (x). This workflow ensures models reflect the true pattern rather than just matching aggregates."
  },
  {
    "objectID": "EPPS6356_Assignment1.html#anscombe-1973-analysis-solutions",
    "href": "EPPS6356_Assignment1.html#anscombe-1973-analysis-solutions",
    "title": "Assignment 1",
    "section": "",
    "text": "The classic Anscombe’s quartet consists of four (x, y) datasets that share the same means, variances, correlations, and regression coefficients — yet their scatterplots are visually very different. Below is my reproduction of the four datasets with their common least-squares line.\n\n\n\n\nAnscombe’s quartet demonstrates that datasets with identical statistical summaries can tell very different stories when visualized. While all four share the same means, variances, correlations, and regression lines, their scatterplots reveal distinct problems: a roughly linear set, a curved relationship, a single extreme outlier, and a high-leverage point that determines the slope. Without visualization, these issues remain hidden and lead to incorrect conclusions. To address this, start with graphs (scatterplots plus a smooth), then check residuals, leverage, and Cook’s distance to test assumptions. When assumptions are violated, use robust regression, apply transformations, add nonlinear terms (e.g., a quadratic), or collect additional data to improve coverage across (x). This workflow ensures models reflect the true pattern rather than just matching aggregates."
  },
  {
    "objectID": "EPPS6356_Assignment1.html#seasonal-plot-custom-winter-palette",
    "href": "EPPS6356_Assignment1.html#seasonal-plot-custom-winter-palette",
    "title": "Assignment 1",
    "section": "2) Seasonal Plot — Custom “Winter” Palette",
    "text": "2) Seasonal Plot — Custom “Winter” Palette\nI re-themed the plot by replacing the original fall color with a cool-tone winter palette."
  },
  {
    "objectID": "EPPS6356_Assignment1.html#critique-of-a-published-chart",
    "href": "EPPS6356_Assignment1.html#critique-of-a-published-chart",
    "title": "Assignment 1",
    "section": "3) Critique of a Published Chart",
    "text": "3) Critique of a Published Chart\nSource: The New York Times — “COVID-19 in the U.S.: Latest Map and Case Counts”\nhttps://www.nytimes.com/interactive/2021/us/covid-cases.html\nClaim & audience. The interactive aims to show how COVID-19 cases evolve across U.S. geographies over time, serving a general audience tracking regional surges and declines.\nWhat works. - The map + time-series pairing supports both geographic and temporal exploration. - Clean styling minimizes clutter; hover interactions surface detail on demand.\nIssues:\n\nColor encoding/contrast. Mid-range shades can be hard to distinguish on some displays, reducing perceptual separation between moderate and high values.\n\nAxis/baseline clarity. In some views the y-axis does not start at zero; if truncation isn’t explicitly noted, fluctuations can appear exaggerated.\n\nContext/annotation. Major events (e.g., vaccine rollout, variant waves) are not annotated on timelines, forcing readers to infer causes of spikes.\n\nLegend proximity & cognitive load. Legends/scales can sit far from the immediate focus, increasing eye travel during comparison.\n\nAccessibility. Palette choices may challenge color-vision deficiencies; thin strokes lower legibility on mobile.\n\nStatic use case. Exporting a static image removes tooltips/hover states, so meaning can be lost without added labels.\n\nImprovements:\n\nAdopt a colorblind-safe, high-contrast sequential palette (e.g., viridis) to better separate mid/high values.\n\nStart y-axes at zero where feasible, or clearly mark breaks when truncation is necessary.\n\nAdd lightweight annotations at key dates (eligibility expansions, variant peaks) to anchor interpretation.\n\nMove legend/scale closer to the plot area or directly label lines/peaks to reduce eye travel.\n\nUse slightly thicker strokes and on-chart labels to retain meaning in static exports."
  },
  {
    "objectID": "EPPS6356_Assignment3.html",
    "href": "EPPS6356_Assignment3.html",
    "title": "Assignment 3 — Base R and ggplot2",
    "section": "",
    "text": "The six classic base R plots were recreated and displayed together in a 3×2 layout.\nThe figure below includes:\n- A manually constructed scatterplot (“Bird 131”) with customized axes and dual labels.\n- A histogram showing normally distributed data with a superimposed normal curve.\n- A stacked bar chart for the VADeaths dataset with values labeled within each segment.\n- Boxplots comparing ToothGrowth length by supplement type (OJ vs VC) and dose.\n- A 3D perspective surface generated using the persp() function.\n- A pie chart illustrating category shares using a gray tone color ramp.\n\n\n\nI chose the ToothGrowth boxplot. It compares tooth length across Vitamin C doses (0.5, 1, 2 mg) and supplement type (OJ vs VC).\nEach box shows the median (center line) and IQR (box height), with whiskers extending to data within 1.5×IQR; any points beyond would be outliers.\nFrom the figure, tooth length increases with dose, and at lower doses (0.5, 1 mg), OJ often shows higher medians than VC; by 2 mg, the two supplements are similar, indicating a diminishing difference at the highest dose."
  },
  {
    "objectID": "EPPS6356_Assignment3.html#a-explain-one-chart-boxplot-toothgrowth",
    "href": "EPPS6356_Assignment3.html#a-explain-one-chart-boxplot-toothgrowth",
    "title": "Assignment 3 — Base R and ggplot2",
    "section": "",
    "text": "I chose the ToothGrowth boxplot. It compares tooth length across Vitamin C doses (0.5, 1, 2 mg) and supplement type (OJ vs VC).\nEach box shows the median (center line) and IQR (box height), with whiskers extending to data within 1.5×IQR; any points beyond would be outliers.\nFrom the figure, tooth length increases with dose, and at lower doses (0.5, 1 mg), OJ often shows higher medians than VC; by 2 mg, the two supplements are similar, indicating a diminishing difference at the highest dose."
  },
  {
    "objectID": "EPPS6356_Assignment3.html#a-compare-the-regression-models",
    "href": "EPPS6356_Assignment3.html#a-compare-the-regression-models",
    "title": "Assignment 3 — Base R and ggplot2",
    "section": "2(a) Compare the regression models",
    "text": "2(a) Compare the regression models\nAll four datasets in Anscombe’s Quartet produce nearly identical regression statistics — similar slopes, intercepts, and (R^2) values (approximately 0.67).\nHowever, their visual patterns are completely different.\nSet 1 represents a true linear relationship, Set 2 has a curved (nonlinear) trend, Set 3 contains a strong outlier that influences the regression, and Set 4 includes one high-leverage point that distorts the fitted line.\nThis demonstrates that numerical summaries alone can be misleading and highlights the importance of data visualization."
  },
  {
    "objectID": "EPPS6356_Assignment3.html#b-compare-different-plotting-styles",
    "href": "EPPS6356_Assignment3.html#b-compare-different-plotting-styles",
    "title": "Assignment 3 — Base R and ggplot2",
    "section": "2(b) Compare different plotting styles",
    "text": "2(b) Compare different plotting styles\nThe first figure shows the default base plots for the four Anscombe datasets, while the second uses customized colors, symbols, and line types for clearer differentiation and readability.\n\n\nComparison paragraph:\nThe styled version communicates the differences between datasets much more effectively.\nIn Set 1, the linear trend is clear; in Set 2, curvature is visible; in Set 3, an outlier becomes obvious; and in Set 4, one leverage point dominates.\nThis comparison shows that thoughtful visual design — through colors, line types, and shapes — enhances interpretability and brings out patterns hidden in plain summaries."
  },
  {
    "objectID": "EPPS6356_Assignment2.html",
    "href": "EPPS6356_Assignment2.html",
    "title": "Assignment 2 — Base R Graphics (Murrell)",
    "section": "",
    "text": "Run Paul Murrell’s base R graphics programs (murrell01.R) line by line. Note the changes as low-level functions are added.\n\n\n\n# Start plotting from basics\nplot(pressure, pch = 16,\n     xlab = \"Temperature (Celsius)\",\n     ylab = \"Pressure (mm Hg)\",\n     main = \"Pressure vs Temperature\")\ntext(150, 600, \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\nplot(pressure, pch = 5,\n     xlab = \"Temperature (Celsius)\",\n     ylab = \"Pressure (mm Hg)\",\n     main = \"Pressure vs Temperature (pch=5)\")\ntext(150, 600, \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n\n\n# Window + axes + layers\npar(mfrow = c(1,1), las = 1, mar = c(4,4,2,4), cex = 0.9)\n\nx  &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, 0.8, 0.5, 0.45, 0.4, 0.3)\n\nplot.new()\nplot.window(xlim = range(as.numeric(x), na.rm = TRUE), ylim = c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch = 16, cex = 1.0)\npoints(x, y2, pch = 1, cex = 1.0)\naxis(1, at = seq(0, 16, 4))\naxis(2, at = seq(0, 6, 2))\naxis(4, at = seq(0, 6, 2))\nbox(bty = \"u\")\nmtext(\"Travel Time (s)\", side = 1, line = 2)\nmtext(\"Responses per Travel\", side = 2, line = 2)\nmtext(\"Responses per Second\", side = 4, line = 2)\ntext(4, 5, \"Bird 131\")\n\n\n\n\n\n\n\n\n\n# Histogram\nset.seed(1)\nY &lt;- rnorm(50)\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA\nxg &lt;- seq(-3.5, 3.5, 0.1)\nhist(Y, breaks = seq(-3.5, 3.5), col = \"gray80\", freq = FALSE,\n     main = \"Normal sample\", xlab = \"Z\", ylab = \"Density\")\nlines(xg, dnorm(xg), lwd = 2)\n\n\n\n\n\n\n\n\n\n# Barplot\npar(mar = c(2, 3.1, 2, 2.1))\nmidpts &lt;- barplot(VADeaths,\n                  col = gray(0.1 + seq(1,9,2)/11),\n                  names = rep(\"\", 4),\n                  ylab = \"Death rate (per 1000)\",\n                  main = \"VADeaths\")\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at = midpts, side = 1, line = 0.5, cex = 0.7)\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# Boxplot\npar(mar = c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset = supp == \"VC\", col = \"white\",\n        xlab = \"\", ylab = \"tooth length\", ylim = c(0, 35),\n        main = \"ToothGrowth by dose & supplement\")\nmtext(\"Vitamin C dose (mg)\", side = 1, line = 2.5, cex = 0.9)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        subset = supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"),\n       fill = c(\"white\",\"gray\"), bty = \"n\")\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# persp() demo surface\nx &lt;- seq(-10, 10, length = 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2 + y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f); z[is.na(z)] &lt;- 1\npar(mar = c(2, 2.5, 3, 2))\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5,\n      ticktype = \"detailed\", xlab = \"X\", ylab = \"Y\", zlab = \"Z\",\n      main = \"persp(): demo surface\")\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# Pie chart\npar(mar = c(0, 2, 1, 2), xpd = FALSE, cex = 0.8)\npie.sales &lt;- c(0.12, 0.30, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\",\"Cherry\",\"Apple\",\"Boston Cream\",\"Other\",\"Vanilla\")\npie(pie.sales, col = gray(seq(0.3, 1.0, length = 6)),\n    main = \"Pie chart demo\")"
  },
  {
    "objectID": "EPPS6356_Assignment2.html#original",
    "href": "EPPS6356_Assignment2.html#original",
    "title": "Assignment 2 — Base R Graphics (Murrell)",
    "section": "",
    "text": "# Start plotting from basics\nplot(pressure, pch = 16,\n     xlab = \"Temperature (Celsius)\",\n     ylab = \"Pressure (mm Hg)\",\n     main = \"Pressure vs Temperature\")\ntext(150, 600, \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\nplot(pressure, pch = 5,\n     xlab = \"Temperature (Celsius)\",\n     ylab = \"Pressure (mm Hg)\",\n     main = \"Pressure vs Temperature (pch=5)\")\ntext(150, 600, \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n\n\n# Window + axes + layers\npar(mfrow = c(1,1), las = 1, mar = c(4,4,2,4), cex = 0.9)\n\nx  &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, 0.8, 0.5, 0.45, 0.4, 0.3)\n\nplot.new()\nplot.window(xlim = range(as.numeric(x), na.rm = TRUE), ylim = c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch = 16, cex = 1.0)\npoints(x, y2, pch = 1, cex = 1.0)\naxis(1, at = seq(0, 16, 4))\naxis(2, at = seq(0, 6, 2))\naxis(4, at = seq(0, 6, 2))\nbox(bty = \"u\")\nmtext(\"Travel Time (s)\", side = 1, line = 2)\nmtext(\"Responses per Travel\", side = 2, line = 2)\nmtext(\"Responses per Second\", side = 4, line = 2)\ntext(4, 5, \"Bird 131\")\n\n\n\n\n\n\n\n\n\n# Histogram\nset.seed(1)\nY &lt;- rnorm(50)\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA\nxg &lt;- seq(-3.5, 3.5, 0.1)\nhist(Y, breaks = seq(-3.5, 3.5), col = \"gray80\", freq = FALSE,\n     main = \"Normal sample\", xlab = \"Z\", ylab = \"Density\")\nlines(xg, dnorm(xg), lwd = 2)\n\n\n\n\n\n\n\n\n\n# Barplot\npar(mar = c(2, 3.1, 2, 2.1))\nmidpts &lt;- barplot(VADeaths,\n                  col = gray(0.1 + seq(1,9,2)/11),\n                  names = rep(\"\", 4),\n                  ylab = \"Death rate (per 1000)\",\n                  main = \"VADeaths\")\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at = midpts, side = 1, line = 0.5, cex = 0.7)\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# Boxplot\npar(mar = c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset = supp == \"VC\", col = \"white\",\n        xlab = \"\", ylab = \"tooth length\", ylim = c(0, 35),\n        main = \"ToothGrowth by dose & supplement\")\nmtext(\"Vitamin C dose (mg)\", side = 1, line = 2.5, cex = 0.9)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        subset = supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"),\n       fill = c(\"white\",\"gray\"), bty = \"n\")\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# persp() demo surface\nx &lt;- seq(-10, 10, length = 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2 + y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f); z[is.na(z)] &lt;- 1\npar(mar = c(2, 2.5, 3, 2))\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5,\n      ticktype = \"detailed\", xlab = \"X\", ylab = \"Y\", zlab = \"Z\",\n      main = \"persp(): demo surface\")\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# Pie chart\npar(mar = c(0, 2, 1, 2), xpd = FALSE, cex = 0.8)\npie.sales &lt;- c(0.12, 0.30, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\",\"Cherry\",\"Apple\",\"Boston Cream\",\"Other\",\"Vanilla\")\npie(pie.sales, col = gray(seq(0.3, 1.0, length = 6)),\n    main = \"Pie chart demo\")"
  },
  {
    "objectID": "EPPS6356_Assignment2.html#base-r-functions-used",
    "href": "EPPS6356_Assignment2.html#base-r-functions-used",
    "title": "Assignment 2 — Base R Graphics (Murrell)",
    "section": "2.1 Base R Functions Used",
    "text": "2.1 Base R Functions Used\n\npar() – Sets or queries graphical parameters such as margins, axis style, and layout.\n\nlines() – Adds connected line segments to an existing plot.\n\npoints() – Adds individual points (symbols) to an existing plot.\n\naxis() – Draws tick marks and axis labels on plots.\n\nbox() – Adds a box (border) around the current plotting region.\n\ntext() – Writes text inside the plotting area at specified coordinates.\n\nmtext() – Adds text in the margins (outer areas) of the plot.\n\nhist() – Creates histograms for frequency distributions.\n\nboxplot() – Displays data distributions by quartiles.\n\nlegend() – Adds legends identifying plotted elements.\n\npersp() – Produces 3-D perspective surface plots.\n\nnames() – Retrieves or sets the names of variables in a data frame.\n\npie() – Creates pie-chart visualizations."
  },
  {
    "objectID": "EPPS6356_Assignment2.html#plots-and-results",
    "href": "EPPS6356_Assignment2.html#plots-and-results",
    "title": "Assignment 2 — Base R Graphics (Murrell)",
    "section": "2.2 Plots and Results",
    "text": "2.2 Plots and Results\n\nMake sure the file HPI_data.csv is in the same folder as this .qmd.\n\n\n# Read and normalize column names from the HPI CSV. This maps your Excel-like\n# headers to the short names used below, and ensures numeric types.\nHPI_raw &lt;- read.csv(\"HPI_data.csv\", stringsAsFactors = FALSE, check.names = TRUE)\n\npick &lt;- function(df, ...) {\n  opts &lt;- c(...)\n  hit &lt;- opts[opts %in% names(df)]\n  if (!length(hit)) stop(\"Missing expected columns: \", paste(opts, collapse=\" | \"))\n  df[[hit[1]]]\n}\n\nHPI &lt;- data.frame(\n  GDP_capita = suppressWarnings(as.numeric(pick(HPI_raw, \"GDP_capita\", \"GDP.per.capita....\"))),\n  LifeExp    = suppressWarnings(as.numeric(pick(HPI_raw, \"LifeExp\", \"Life.Expectancy..years.\"))),\n  Wellbeing  = suppressWarnings(as.numeric(pick(HPI_raw, \"Wellbeing\", \"Ladder.of.life..Wellbeing...0.10.\"))),\n  Carbon     = suppressWarnings(as.numeric(pick(HPI_raw, \"Carbon\", \"Carbon.Footprint..tCO2e.\"))),\n  HPI        = suppressWarnings(as.numeric(pick(HPI_raw, \"HPI\"))),\n  Population = suppressWarnings(as.numeric(pick(HPI_raw, \"Population\", \"Population..thousands.\"))),\n  Continent  = pick(HPI_raw, \"Continent\")\n)\n\n\n# Scatter with regression line — small, solid points\nscat &lt;- subset(HPI, is.finite(LifeExp) & is.finite(Wellbeing))\npar(las = 1, mar = c(4,4,2,1), cex = 0.9)\nplot(scat$LifeExp, scat$Wellbeing, pch = 16, cex = 1.0, col = \"steelblue\",\n     xlab = \"Life Expectancy (years)\", ylab = \"Wellbeing (0–10)\",\n     main = \"Life Expectancy vs Wellbeing (HPI)\")\nabline(lm(Wellbeing ~ LifeExp, data = scat), lwd = 2)\nbox()\n\n\n\n\n\n\n\n\n\n# Custom axes & labels — same point size as above\nplot(scat$LifeExp, scat$Wellbeing, axes = FALSE, pch = 16, cex = 1.0, col = \"steelblue\",\n     xlab = \"\", ylab = \"\", main = \"Custom axes & labels (HPI)\")\naxis(1); axis(2); box()\ntext(x = mean(scat$LifeExp, na.rm = TRUE),\n     y = max(scat$Wellbeing, na.rm = TRUE),\n     labels = \"Positive association\", pos = 3)\nmtext(\"Life Expectancy (Years)\", side = 1, line = 3)\nmtext(\"Wellbeing (0–10)\",      side = 2, line = 3)\n\n\n\n\n\n\n\n\n\n# Histogram + density\nwb &lt;- scat$Wellbeing[is.finite(scat$Wellbeing)]\nhist(wb, breaks = 6, col = \"gray80\",\n     main = \"Distribution of Wellbeing (HPI)\",\n     xlab = \"Wellbeing (0–10)\", ylab = \"Density\")\nif (length(unique(wb)) &gt; 1) lines(density(wb), lwd = 2)\n\n\n\n\n\n\n\n\n\n# Boxplot by continent\nbp &lt;- subset(HPI, is.finite(HPI) & !is.na(Continent))\nboxplot(HPI ~ as.factor(Continent), data = bp, col = \"white\",\n        ylab = \"Happy Planet Index (HPI)\", xlab = \"Continent\",\n        main = \"HPI by Continent\")\nmtext(\"Grouped distribution across regions\", side = 3, line = 0.5, cex = 0.8)\n\n\n\n\n\n\n\n\n\n# GDP vs Life Expectancy — solid points\ngdp &lt;- subset(HPI, is.finite(GDP_capita) & is.finite(LifeExp))\nplot(gdp$GDP_capita, gdp$LifeExp, pch = 16, cex = 1.0, col = \"tomato\",\n     xlab = \"GDP per Capita (USD)\", ylab = \"Life Expectancy (years)\",\n     main = \"GDP vs Life Expectancy (HPI)\")\nlegend(\"bottomright\", legend = \"Country points\", pch = 16, col = \"tomato\", bty = \"n\")\n\n\n\n\n\n\n\n\n\n# persp(): demo surface (with clear axes & ticks)\nx &lt;- seq(-10, 10, length = 30); y &lt;- x\nf &lt;- function(x,y) {\n  r &lt;- sqrt(x^2 + y^2)\n  out &lt;- 10 * sin(r) / r\n  out[!is.finite(out)] &lt;- 10\n  out\n}\nz &lt;- outer(x, y, f)\npar(mar = c(2, 2.5, 3, 2))\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5,\n      ticktype = \"detailed\",\n      xlab = \"X\", ylab = \"Y\", zlab = \"Z\",\n      col = \"lightblue\", border = \"gray30\",\n      main = \"persp(): demo surface\")\n\n\n\n\n\n\n\n\n\n# OPTIONAL: Interpolated HPI surface using akima (only if enough data)\nif (!requireNamespace(\"akima\", quietly = TRUE)) {\n  install.packages(\"akima\")\n}\nif (requireNamespace(\"akima\", quietly = TRUE)) {\n  library(akima)\n  df &lt;- subset(HPI, is.finite(GDP_capita) & is.finite(Carbon) & is.finite(HPI))\n  if (nrow(df) &gt; 10 && length(unique(df$GDP_capita)) &gt; 3 && length(unique(df$Carbon)) &gt; 3) {\n    interp_result &lt;- with(df, akima::interp(x = GDP_capita, y = Carbon, z = HPI, duplicate = \"mean\"))\n    par(mar = c(2, 2.5, 3, 2))\n    persp(interp_result$x, interp_result$y, interp_result$z,\n          theta = 30, phi = 30, expand = 0.5,\n          ticktype = \"detailed\",\n          xlab = \"GDP per Capita\", ylab = \"Carbon (tCO2e/capita)\", zlab = \"HPI\",\n          col = \"lightblue\", border = \"gray30\",\n          main = \"HPI Surface: GDP vs Carbon (interpolated)\")\n  } else {\n    message(\"Not enough distinct points for interpolation; skipping HPI surface.\")\n  }\n}"
  },
  {
    "objectID": "PrepareforClass2.html",
    "href": "PrepareforClass2.html",
    "title": "Prepare for Class 2",
    "section": "",
    "text": "“Teaching to See” is a 37-minute primer on visual literacy and studio discipline. Inge Druckrey, drawing on the Basel School tradition, shows how designers learn to see—not merely to look at objects, but to perceive relationships: proportion, rhythm, weight, contrast, spacing, and alignment. The film’s early exercises are intentionally humble—parallel lines, curves, letterforms, cropping—because they train attention. Druckrey’s core idea is that the eye must be calibrated through deliberate practice so it can make fine distinctions and judge quality without relying on rules of thumb. That calibration then guides the hand, which becomes a precise instrument rather than a source of accidental marks.\nTwo themes recur. Sensitivity to contrast (thin/thick, light/dark, dense/open): students push differences until they can control them, which later allows subtlety. Structure and continuity: how a curve carries energy across space; how a line’s direction implies movement; how negative space shapes the figure as much as strokes do. By iterating studies and pinning work to the wall, students learn comparative judgment—why one arrangement breathes while another feels cramped; why a tiny shift in alignment or spacing can turn awkwardness into clarity. Druckrey frames this as ethics as much as technique: careful seeing is respect for the viewer’s time.\nThe film also connects craft to technology. Good digital typography, interface spacing, and iconography all sit on the same foundations as pen-and-ink calligraphy: consistent stroke logic, optical corrections for perception, and hierarchy established through size, weight, and white space. The cameo references to Steve Jobs underline this continuity—humanist calligraphy informed the Macintosh’s attention to typography, kerning, and proportionally spaced fonts—reminding us that useful software inherits from analog craft. Ultimately, “Teaching to See” argues that design education is less about memorizing rules and more about training judgment. The reward is work that is quiet, exact, and humane—design that “disappears” because its structure is so well seen."
  },
  {
    "objectID": "PrepareforClass2.html#one-page-review",
    "href": "PrepareforClass2.html#one-page-review",
    "title": "Prepare for Class 2",
    "section": "",
    "text": "“Teaching to See” is a 37-minute primer on visual literacy and studio discipline. Inge Druckrey, drawing on the Basel School tradition, shows how designers learn to see—not merely to look at objects, but to perceive relationships: proportion, rhythm, weight, contrast, spacing, and alignment. The film’s early exercises are intentionally humble—parallel lines, curves, letterforms, cropping—because they train attention. Druckrey’s core idea is that the eye must be calibrated through deliberate practice so it can make fine distinctions and judge quality without relying on rules of thumb. That calibration then guides the hand, which becomes a precise instrument rather than a source of accidental marks.\nTwo themes recur. Sensitivity to contrast (thin/thick, light/dark, dense/open): students push differences until they can control them, which later allows subtlety. Structure and continuity: how a curve carries energy across space; how a line’s direction implies movement; how negative space shapes the figure as much as strokes do. By iterating studies and pinning work to the wall, students learn comparative judgment—why one arrangement breathes while another feels cramped; why a tiny shift in alignment or spacing can turn awkwardness into clarity. Druckrey frames this as ethics as much as technique: careful seeing is respect for the viewer’s time.\nThe film also connects craft to technology. Good digital typography, interface spacing, and iconography all sit on the same foundations as pen-and-ink calligraphy: consistent stroke logic, optical corrections for perception, and hierarchy established through size, weight, and white space. The cameo references to Steve Jobs underline this continuity—humanist calligraphy informed the Macintosh’s attention to typography, kerning, and proportionally spaced fonts—reminding us that useful software inherits from analog craft. Ultimately, “Teaching to See” argues that design education is less about memorizing rules and more about training judgment. The reward is work that is quiet, exact, and humane—design that “disappears” because its structure is so well seen."
  },
  {
    "objectID": "PrepareforClass2.html#guided-questions",
    "href": "PrepareforClass2.html#guided-questions",
    "title": "Prepare for Class 2",
    "section": "Guided Questions",
    "text": "Guided Questions\n\n1) Learn thin/thick and curve/linearity from Druckrey\nDruckrey teaches contrast and line quality through repetitive, controlled studies. Using broad- and fine-nib tools, students draw series of parallel strokes to feel how pressure and angle create thin–thick modulation. Curve drills—long, continuous S-curves and circles—build continuity and pacing, while straight-line studies enforce steadiness and alignment. By comparing dozens of iterations on the wall, students learn to judge consistency of stroke, even spacing, clean joins at corners, and transitions where a curve straightens into a line. The aim is sensitivity: to control contrast deliberately rather than by accident.\n\n\n2) How did Holmes describe and contrast the eye and the hand in writing?\nNigel Holmes’s point (as echoed in the film’s commentary) is that the eye is fast, comparative, and judgment-driven, while the hand is slow, mechanical, and trainable. The eye anticipates form—spacing, direction, and weight—and immediately perceives unevenness; the hand must practice until it can execute what the eye intends. Good drawing and writing align the two: the seeing leads, the hand follows. When they are out of sync you get wobbly lines, inconsistent spacing, and tired letterforms; when aligned, the marks feel alive and effortless.\n\n\n3) Why is calligraphy important to Steve Jobs and the design of Mac computers?\nJobs famously audited a calligraphy course at Reed College. From it he absorbed the grammar of letterforms—stroke contrast, serif logic, rhythm, kerning, and optical spacing. Those lessons shaped the original Macintosh: multiple typefaces, proportionally spaced fonts, and attention to typographic detail were prioritized in software and hardware. The humanist values of calligraphy—clarity, proportion, and respect for the reader—became product principles.\n\n\n4) Differentiate geometric accuracy and optical accuracy\nGeometric accuracy is what a ruler or coordinates say is equal; optical accuracy is what looks equal. Because of human perception, purely geometric equality often appears wrong. Designers make “optical corrections”: circles and curved letters overshoot the baseline and cap height; counters are opened to avoid clogging; horizontal and vertical spacing is adjusted so masses feel even; centered objects are nudged to compensate for visual weight. In training the eye, Druckrey stresses judging with perception first and letting geometry serve that judgment."
  },
  {
    "objectID": "EPPS6356_PrepareforClass2.html",
    "href": "EPPS6356_PrepareforClass2.html",
    "title": "Prepare for Class 2",
    "section": "",
    "text": "“Teaching to See” is a 37-minute primer on visual literacy and studio discipline. Inge Druckrey, drawing on the Basel School tradition, shows how designers learn to see—not merely to look at objects, but to perceive relationships: proportion, rhythm, weight, contrast, spacing, and alignment. The film’s early exercises are intentionally humble—parallel lines, curves, letterforms, cropping—because they train attention. Druckrey’s core idea is that the eye must be calibrated through deliberate practice so it can make fine distinctions and judge quality without relying on rules of thumb. That calibration then guides the hand, which becomes a precise instrument rather than a source of accidental marks.\nTwo themes recur. Sensitivity to contrast (thin/thick, light/dark, dense/open): students push differences until they can control them, which later allows subtlety. Structure and continuity: how a curve carries energy across space; how a line’s direction implies movement; how negative space shapes the figure as much as strokes do. By iterating studies and pinning work to the wall, students learn comparative judgment—why one arrangement breathes while another feels cramped; why a tiny shift in alignment or spacing can turn awkwardness into clarity. Druckrey frames this as ethics as much as technique: careful seeing is respect for the viewer’s time.\nThe film also connects craft to technology. Good digital typography, interface spacing, and iconography all sit on the same foundations as pen-and-ink calligraphy: consistent stroke logic, optical corrections for perception, and hierarchy established through size, weight, and white space. The cameo references to Steve Jobs underline this continuity—humanist calligraphy informed the Macintosh’s attention to typography, kerning, and proportionally spaced fonts—reminding us that useful software inherits from analog craft. Ultimately, “Teaching to See” argues that design education is less about memorizing rules and more about training judgment. The reward is work that is quiet, exact, and humane—design that “disappears” because its structure is so well seen."
  },
  {
    "objectID": "EPPS6356_PrepareforClass2.html#one-page-review",
    "href": "EPPS6356_PrepareforClass2.html#one-page-review",
    "title": "Prepare for Class 2",
    "section": "",
    "text": "“Teaching to See” is a 37-minute primer on visual literacy and studio discipline. Inge Druckrey, drawing on the Basel School tradition, shows how designers learn to see—not merely to look at objects, but to perceive relationships: proportion, rhythm, weight, contrast, spacing, and alignment. The film’s early exercises are intentionally humble—parallel lines, curves, letterforms, cropping—because they train attention. Druckrey’s core idea is that the eye must be calibrated through deliberate practice so it can make fine distinctions and judge quality without relying on rules of thumb. That calibration then guides the hand, which becomes a precise instrument rather than a source of accidental marks.\nTwo themes recur. Sensitivity to contrast (thin/thick, light/dark, dense/open): students push differences until they can control them, which later allows subtlety. Structure and continuity: how a curve carries energy across space; how a line’s direction implies movement; how negative space shapes the figure as much as strokes do. By iterating studies and pinning work to the wall, students learn comparative judgment—why one arrangement breathes while another feels cramped; why a tiny shift in alignment or spacing can turn awkwardness into clarity. Druckrey frames this as ethics as much as technique: careful seeing is respect for the viewer’s time.\nThe film also connects craft to technology. Good digital typography, interface spacing, and iconography all sit on the same foundations as pen-and-ink calligraphy: consistent stroke logic, optical corrections for perception, and hierarchy established through size, weight, and white space. The cameo references to Steve Jobs underline this continuity—humanist calligraphy informed the Macintosh’s attention to typography, kerning, and proportionally spaced fonts—reminding us that useful software inherits from analog craft. Ultimately, “Teaching to See” argues that design education is less about memorizing rules and more about training judgment. The reward is work that is quiet, exact, and humane—design that “disappears” because its structure is so well seen."
  },
  {
    "objectID": "EPPS6356_PrepareforClass2.html#guided-questions",
    "href": "EPPS6356_PrepareforClass2.html#guided-questions",
    "title": "Prepare for Class 2",
    "section": "Guided Questions",
    "text": "Guided Questions\n\n1) Learn thin/thick and curve/linearity from Druckrey\nDruckrey teaches contrast and line quality through repetitive, controlled studies. Using broad- and fine-nib tools, students draw series of parallel strokes to feel how pressure and angle create thin–thick modulation. Curve drills—long, continuous S-curves and circles—build continuity and pacing, while straight-line studies enforce steadiness and alignment. By comparing dozens of iterations on the wall, students learn to judge consistency of stroke, even spacing, clean joins at corners, and transitions where a curve straightens into a line. The aim is sensitivity: to control contrast deliberately rather than by accident.\n\n\n2) How did Holmes describe and contrast the eye and the hand in writing?\nNigel Holmes’s point (as echoed in the film’s commentary) is that the eye is fast, comparative, and judgment-driven, while the hand is slow, mechanical, and trainable. The eye anticipates form—spacing, direction, and weight—and immediately perceives unevenness; the hand must practice until it can execute what the eye intends. Good drawing and writing align the two: the seeing leads, the hand follows. When they are out of sync you get wobbly lines, inconsistent spacing, and tired letterforms; when aligned, the marks feel alive and effortless.\n\n\n3) Why is calligraphy important to Steve Jobs and the design of Mac computers?\nJobs famously audited a calligraphy course at Reed College. From it he absorbed the grammar of letterforms—stroke contrast, serif logic, rhythm, kerning, and optical spacing. Those lessons shaped the original Macintosh: multiple typefaces, proportionally spaced fonts, and attention to typographic detail were prioritized in software and hardware. The humanist values of calligraphy—clarity, proportion, and respect for the reader—became product principles.\n\n\n4) Differentiate geometric accuracy and optical accuracy\nGeometric accuracy is what a ruler or coordinates say is equal; optical accuracy is what looks equal. Because of human perception, purely geometric equality often appears wrong. Designers make “optical corrections”: circles and curved letters overshoot the baseline and cap height; counters are opened to avoid clogging; horizontal and vertical spacing is adjusted so masses feel even; centered objects are nudged to compensate for visual weight. In training the eye, Druckrey stresses judging with perception first and letting geometry serve that judgment."
  },
  {
    "objectID": "EPPS6356_PrepareforClass3.html",
    "href": "EPPS6356_PrepareforClass3.html",
    "title": "Prepare for Class 3",
    "section": "",
    "text": "Geoff McGhee’s documentary Journalism in the Age of Data explores how visualization became central to modern storytelling. Through interviews with data journalists at The New York Times, The Guardian, and Gapminder, the film reveals a shift: journalists no longer just write stories—they design arguments that readers can explore visually. The craft moves beyond illustration; graphics are the narrative.\nAcademic visualization, by contrast, tends to emphasize analytical precision, statistical rigor, and reproducibility. In scholarly work, clarity and neutrality are paramount; visuals must represent data truthfully, often at the expense of aesthetic engagement. Journalistic visualization, on the other hand, embraces emotion and immediacy. It borrows from design to guide attention—using color, motion, and annotation to shape interpretation. McGhee frames this as a collaboration between story and structure: the journalist curates context so that readers can navigate complex issues like climate change or budgets intuitively.\nAnother key distinction lies in audience. Academic visualizations address expert readers who can decode abstract charts; journalistic ones target general publics who need the visualization to invite curiosity rather than prove a hypothesis. Yet both domains share a mission: to make invisible systems visible. As McGhee notes, good visualization demands technical literacy and moral restraint—knowing what to emphasize and what to omit.\nUltimately, Journalism in the Age of Data bridges the gap between scientific visualization and narrative design. It argues that the journalist’s role has expanded from storyteller to explainer, translator, and data designer. When done well, this synthesis turns raw information into understanding—an act both analytic and artistic."
  },
  {
    "objectID": "EPPS6356_PrepareforClass3.html#one-page-review-journalism-in-the-age-of-data-geoff-mcghee-2011",
    "href": "EPPS6356_PrepareforClass3.html#one-page-review-journalism-in-the-age-of-data-geoff-mcghee-2011",
    "title": "Prepare for Class 3",
    "section": "",
    "text": "Geoff McGhee’s documentary Journalism in the Age of Data explores how visualization became central to modern storytelling. Through interviews with data journalists at The New York Times, The Guardian, and Gapminder, the film reveals a shift: journalists no longer just write stories—they design arguments that readers can explore visually. The craft moves beyond illustration; graphics are the narrative.\nAcademic visualization, by contrast, tends to emphasize analytical precision, statistical rigor, and reproducibility. In scholarly work, clarity and neutrality are paramount; visuals must represent data truthfully, often at the expense of aesthetic engagement. Journalistic visualization, on the other hand, embraces emotion and immediacy. It borrows from design to guide attention—using color, motion, and annotation to shape interpretation. McGhee frames this as a collaboration between story and structure: the journalist curates context so that readers can navigate complex issues like climate change or budgets intuitively.\nAnother key distinction lies in audience. Academic visualizations address expert readers who can decode abstract charts; journalistic ones target general publics who need the visualization to invite curiosity rather than prove a hypothesis. Yet both domains share a mission: to make invisible systems visible. As McGhee notes, good visualization demands technical literacy and moral restraint—knowing what to emphasize and what to omit.\nUltimately, Journalism in the Age of Data bridges the gap between scientific visualization and narrative design. It argues that the journalist’s role has expanded from storyteller to explainer, translator, and data designer. When done well, this synthesis turns raw information into understanding—an act both analytic and artistic."
  },
  {
    "objectID": "PrepareforClass4.html",
    "href": "PrepareforClass4.html",
    "title": "Prepare for Class 4",
    "section": "",
    "text": "Edward Tufte’s lecture The Future of Data Analysis extends his lifelong campaign for clarity and integrity in how information is displayed. He opens with a simple but radical idea: seeing well is thinking well. In a world overflowing with dashboards, metrics, and automated analytics, Tufte argues that the next frontier of data analysis is not technological—it is philosophical. What matters most is cultivating the human capacity to notice patterns, relationships, and meaning hidden within complexity. Tools can calculate, but only the human mind can interpret.\nThroughout the talk, Tufte revisits his central design principles—truthfulness, precision, and beauty—and re-frames them for an era dominated by algorithms. He traces a line from Galileo’s telescopic sketches to today’s interactive data dashboards, reminding the audience that visualization has always been a cognitive technology: a way to extend perception and reasoning. When we visualize data, we are literally “making thought visible.” Yet he warns that as graphics become more automated, the analyst’s curiosity and skepticism can erode. “Never let the software think for you,” he says, stressing that each chart must emerge from a clear analytical question rather than a template.\nA major theme is integration—of quantitative accuracy with qualitative understanding. Tufte envisions the analyst of the future as both scientist and artist, someone who can write statistical code and also judge visual proportion, hierarchy, and rhythm. This combination of reasoning and design, he suggests, produces visualizations that work—they inform without distorting and persuade without manipulating. He calls for “high data density with low noise,” meaning that every pixel should carry meaning and every visual choice should serve evidence, not decoration.\nThe talk also critiques the overuse of dashboards and “chartjunk.” When organizations rely on flashy but shallow summaries, they risk mistaking performance metrics for knowledge. Tufte argues that genuine understanding requires context: side-by-side comparisons, annotated evidence, and time-series continuity. His principle of small multiples—repeated, comparable views—remains one of the most powerful ways to show change without exaggeration. He contrasts this disciplined approach with sensational graphs that truncate axes or hide uncertainty, calling such distortions ethical failures, not mere design flaws.\nAnother striking idea is the role of narrative in analysis. Tufte insists that visualization should not replace verbal explanation but enhance it. Numbers and stories must coexist: data show what happened, while narrative clarifies why it matters. The most effective visualizations invite the viewer to reason, not just react. They slow perception down enough for reflection—a quality increasingly rare in the fast-scroll world of media and business intelligence tools.\nBy the end, Tufte situates the “future” of data analysis in a moral framework. The abundance of data amplifies both our power and our responsibility. Analysts and designers are now interpreters of reality; their choices can clarify truth or conceal it. He challenges his audience to treat every visualization as an ethical document—something that must earn trust through accuracy, transparency, and aesthetic restraint.\nUltimately, Tufte’s message is timeless: technology may evolve, but the principles of clear thinking do not. The real progress in data analysis will come not from more dashboards, but from deeper attention. The analyst of the future is not a technician running models, but a human observer capable of transforming raw information into understanding.\n\n\n“The future of data analysis is not a machine. It’s a human being with better eyesight.” — Edward Tufte"
  },
  {
    "objectID": "PrepareforClass4.html#one-page-review-the-future-of-data-analysis-edward-tufte-2016",
    "href": "PrepareforClass4.html#one-page-review-the-future-of-data-analysis-edward-tufte-2016",
    "title": "Prepare for Class 4",
    "section": "",
    "text": "Edward Tufte’s lecture The Future of Data Analysis extends his lifelong campaign for clarity and integrity in how information is displayed. He opens with a simple but radical idea: seeing well is thinking well. In a world overflowing with dashboards, metrics, and automated analytics, Tufte argues that the next frontier of data analysis is not technological—it is philosophical. What matters most is cultivating the human capacity to notice patterns, relationships, and meaning hidden within complexity. Tools can calculate, but only the human mind can interpret.\nThroughout the talk, Tufte revisits his central design principles—truthfulness, precision, and beauty—and re-frames them for an era dominated by algorithms. He traces a line from Galileo’s telescopic sketches to today’s interactive data dashboards, reminding the audience that visualization has always been a cognitive technology: a way to extend perception and reasoning. When we visualize data, we are literally “making thought visible.” Yet he warns that as graphics become more automated, the analyst’s curiosity and skepticism can erode. “Never let the software think for you,” he says, stressing that each chart must emerge from a clear analytical question rather than a template.\nA major theme is integration—of quantitative accuracy with qualitative understanding. Tufte envisions the analyst of the future as both scientist and artist, someone who can write statistical code and also judge visual proportion, hierarchy, and rhythm. This combination of reasoning and design, he suggests, produces visualizations that work—they inform without distorting and persuade without manipulating. He calls for “high data density with low noise,” meaning that every pixel should carry meaning and every visual choice should serve evidence, not decoration.\nThe talk also critiques the overuse of dashboards and “chartjunk.” When organizations rely on flashy but shallow summaries, they risk mistaking performance metrics for knowledge. Tufte argues that genuine understanding requires context: side-by-side comparisons, annotated evidence, and time-series continuity. His principle of small multiples—repeated, comparable views—remains one of the most powerful ways to show change without exaggeration. He contrasts this disciplined approach with sensational graphs that truncate axes or hide uncertainty, calling such distortions ethical failures, not mere design flaws.\nAnother striking idea is the role of narrative in analysis. Tufte insists that visualization should not replace verbal explanation but enhance it. Numbers and stories must coexist: data show what happened, while narrative clarifies why it matters. The most effective visualizations invite the viewer to reason, not just react. They slow perception down enough for reflection—a quality increasingly rare in the fast-scroll world of media and business intelligence tools.\nBy the end, Tufte situates the “future” of data analysis in a moral framework. The abundance of data amplifies both our power and our responsibility. Analysts and designers are now interpreters of reality; their choices can clarify truth or conceal it. He challenges his audience to treat every visualization as an ethical document—something that must earn trust through accuracy, transparency, and aesthetic restraint.\nUltimately, Tufte’s message is timeless: technology may evolve, but the principles of clear thinking do not. The real progress in data analysis will come not from more dashboards, but from deeper attention. The analyst of the future is not a technician running models, but a human observer capable of transforming raw information into understanding.\n\n\n“The future of data analysis is not a machine. It’s a human being with better eyesight.” — Edward Tufte"
  },
  {
    "objectID": "PrepareforClass5.html",
    "href": "PrepareforClass5.html",
    "title": "Prepare for Class 5",
    "section": "",
    "text": "Big data has reshaped how we understand social behavior, public health, and markets—but with its scale comes illusion. The Parable of Google Flu, published in Nature, remains a cautionary tale. In 2008, Google’s algorithm predicted flu outbreaks by analyzing search queries, claiming to forecast epidemics faster than the CDC. For a brief period, the results looked revolutionary—until they stopped working. The algorithm overestimated flu cases by more than 140%, revealing that the apparent success of “big data” can mask deeper analytical flaws.\nThe first pitfall is data without theory. Massive datasets tempt analysts to treat correlation as causation. Without grounding in domain knowledge, we misinterpret noise as signal. In the Google Flu case, rising media attention and public anxiety—rather than infection rates—drove search trends. This created a self-reinforcing feedback loop that the model mistook for real-world growth. Tufte’s warning about “seeing without thinking” echoes here: visualization and data alike require interpretation, not just computation.\nAnother pitfall is algorithmic opacity. Big data often relies on proprietary, ever-changing systems. When inputs, models, or weighting methods are unknown, replication becomes impossible. This erodes scientific transparency and accountability—two pillars of credible data analysis. Furthermore, as datasets grow, bias does not disappear; it scales. If underlying data are incomplete, unrepresentative, or contextually skewed, bigger datasets simply reproduce error with greater confidence.\nClosely related is the danger of overfitting and overparameterization. In large models, especially machine learning systems, it’s easy to fit data so perfectly that the model memorizes patterns rather than generalizes them. This creates high accuracy in training but failure in prediction. Overfitting is a form of overconfidence disguised as precision—it reflects modeling power used without restraint. The solution, as both Hadley Wickham and Andrew Gelman emphasize, lies in simplicity, cross-validation, and clarity: build models that explain rather than impress.\nEthically, the pitfalls of big data remind us that quantity does not equal quality. A million observations cannot compensate for poor measurement, biased sampling, or conceptual confusion. The role of the analyst is to filter wisely, visualize responsibly, and remain skeptical of “too-good-to-be-true” correlations. In the end, meaningful data analysis depends not on how much data we have, but on how thoughtfully we use it."
  },
  {
    "objectID": "PrepareforClass5.html#one-page-note-big-data-pitfalls-and-overfitting",
    "href": "PrepareforClass5.html#one-page-note-big-data-pitfalls-and-overfitting",
    "title": "Prepare for Class 5",
    "section": "",
    "text": "Big data has reshaped how we understand social behavior, public health, and markets—but with its scale comes illusion. The Parable of Google Flu, published in Nature, remains a cautionary tale. In 2008, Google’s algorithm predicted flu outbreaks by analyzing search queries, claiming to forecast epidemics faster than the CDC. For a brief period, the results looked revolutionary—until they stopped working. The algorithm overestimated flu cases by more than 140%, revealing that the apparent success of “big data” can mask deeper analytical flaws.\nThe first pitfall is data without theory. Massive datasets tempt analysts to treat correlation as causation. Without grounding in domain knowledge, we misinterpret noise as signal. In the Google Flu case, rising media attention and public anxiety—rather than infection rates—drove search trends. This created a self-reinforcing feedback loop that the model mistook for real-world growth. Tufte’s warning about “seeing without thinking” echoes here: visualization and data alike require interpretation, not just computation.\nAnother pitfall is algorithmic opacity. Big data often relies on proprietary, ever-changing systems. When inputs, models, or weighting methods are unknown, replication becomes impossible. This erodes scientific transparency and accountability—two pillars of credible data analysis. Furthermore, as datasets grow, bias does not disappear; it scales. If underlying data are incomplete, unrepresentative, or contextually skewed, bigger datasets simply reproduce error with greater confidence.\nClosely related is the danger of overfitting and overparameterization. In large models, especially machine learning systems, it’s easy to fit data so perfectly that the model memorizes patterns rather than generalizes them. This creates high accuracy in training but failure in prediction. Overfitting is a form of overconfidence disguised as precision—it reflects modeling power used without restraint. The solution, as both Hadley Wickham and Andrew Gelman emphasize, lies in simplicity, cross-validation, and clarity: build models that explain rather than impress.\nEthically, the pitfalls of big data remind us that quantity does not equal quality. A million observations cannot compensate for poor measurement, biased sampling, or conceptual confusion. The role of the analyst is to filter wisely, visualize responsibly, and remain skeptical of “too-good-to-be-true” correlations. In the end, meaningful data analysis depends not on how much data we have, but on how thoughtfully we use it."
  },
  {
    "objectID": "PrepareforClass5.html#hadley-wickham-data-visualization-and-data-science-embl-summary-and-commentary",
    "href": "PrepareforClass5.html#hadley-wickham-data-visualization-and-data-science-embl-summary-and-commentary",
    "title": "Prepare for Class 5",
    "section": "Hadley Wickham: Data Visualization and Data Science (EMBL) — Summary and Commentary",
    "text": "Hadley Wickham: Data Visualization and Data Science (EMBL) — Summary and Commentary\nIn his EMBL talk, Hadley Wickham explains that data visualization is the bridge between data and understanding. He introduces the “grammar of graphics” — a structured way to describe visualizations as combinations of data, aesthetic mappings, and geometries. This framework, implemented in his ggplot2 package, allows analysts to move from ad-hoc plotting to reproducible, composable graphics. Every visual element in a chart—points, lines, bars, colors, or facets—corresponds to a decision in the grammar, making visual design transparent and logical.\nWickham also connects visualization to the broader workflow of data science: importing, tidying, transforming, visualizing, and modeling. He emphasizes that visualization is not the end of analysis but the middle of thinking—a feedback loop that helps analysts notice patterns, spot errors, and refine questions. The best insights often emerge while visualizing, not after modeling.\nAnother key contribution is his advocacy for tidy data principles, where each variable forms a column, each observation a row, and each dataset a table. This structure allows visualization tools and analytical packages to interoperate seamlessly. Combined with ggplot2, dplyr, and tidyr, the tidyverse creates an ecosystem where clarity, readability, and reproducibility come first.\nWickham’s main message parallels Tufte’s: clarity is ethics in data visualization. The goal is not to impress audiences with complexity but to reveal truth with simplicity. In practice, this means designing graphics that emphasize structure over style, accuracy over decoration, and transparency over spectacle. His work has transformed visualization from a craft of intuition into a system of reasoning—a grammar that turns data into readable, trustworthy stories.\n\n\n“The purpose of visualization is not to show data; it’s to make people think about data.” — Hadley Wickham"
  },
  {
    "objectID": "EPPS6356_PrepareforClass5.html",
    "href": "EPPS6356_PrepareforClass5.html",
    "title": "Prepare for Class 5",
    "section": "",
    "text": "Big data has reshaped how we understand social behavior, public health, and markets—but with its scale comes illusion. The Parable of Google Flu, published in Nature, remains a cautionary tale. In 2008, Google’s algorithm predicted flu outbreaks by analyzing search queries, claiming to forecast epidemics faster than the CDC. For a brief period, the results looked revolutionary—until they stopped working. The algorithm overestimated flu cases by more than 140%, revealing that the apparent success of “big data” can mask deeper analytical flaws.\nThe first pitfall is data without theory. Massive datasets tempt analysts to treat correlation as causation. Without grounding in domain knowledge, we misinterpret noise as signal. In the Google Flu case, rising media attention and public anxiety—rather than infection rates—drove search trends. This created a self-reinforcing feedback loop that the model mistook for real-world growth. Tufte’s warning about “seeing without thinking” echoes here: visualization and data alike require interpretation, not just computation.\nAnother pitfall is algorithmic opacity. Big data often relies on proprietary, ever-changing systems. When inputs, models, or weighting methods are unknown, replication becomes impossible. This erodes scientific transparency and accountability—two pillars of credible data analysis. Furthermore, as datasets grow, bias does not disappear; it scales. If underlying data are incomplete, unrepresentative, or contextually skewed, bigger datasets simply reproduce error with greater confidence.\nClosely related is the danger of overfitting and overparameterization. In large models, especially machine learning systems, it’s easy to fit data so perfectly that the model memorizes patterns rather than generalizes them. This creates high accuracy in training but failure in prediction. Overfitting is a form of overconfidence disguised as precision—it reflects modeling power used without restraint. The solution, as both Hadley Wickham and Andrew Gelman emphasize, lies in simplicity, cross-validation, and clarity: build models that explain rather than impress.\nEthically, the pitfalls of big data remind us that quantity does not equal quality. A million observations cannot compensate for poor measurement, biased sampling, or conceptual confusion. The role of the analyst is to filter wisely, visualize responsibly, and remain skeptical of “too-good-to-be-true” correlations. In the end, meaningful data analysis depends not on how much data we have, but on how thoughtfully we use it."
  },
  {
    "objectID": "EPPS6356_PrepareforClass5.html#one-page-note-big-data-pitfalls-and-overfitting",
    "href": "EPPS6356_PrepareforClass5.html#one-page-note-big-data-pitfalls-and-overfitting",
    "title": "Prepare for Class 5",
    "section": "",
    "text": "Big data has reshaped how we understand social behavior, public health, and markets—but with its scale comes illusion. The Parable of Google Flu, published in Nature, remains a cautionary tale. In 2008, Google’s algorithm predicted flu outbreaks by analyzing search queries, claiming to forecast epidemics faster than the CDC. For a brief period, the results looked revolutionary—until they stopped working. The algorithm overestimated flu cases by more than 140%, revealing that the apparent success of “big data” can mask deeper analytical flaws.\nThe first pitfall is data without theory. Massive datasets tempt analysts to treat correlation as causation. Without grounding in domain knowledge, we misinterpret noise as signal. In the Google Flu case, rising media attention and public anxiety—rather than infection rates—drove search trends. This created a self-reinforcing feedback loop that the model mistook for real-world growth. Tufte’s warning about “seeing without thinking” echoes here: visualization and data alike require interpretation, not just computation.\nAnother pitfall is algorithmic opacity. Big data often relies on proprietary, ever-changing systems. When inputs, models, or weighting methods are unknown, replication becomes impossible. This erodes scientific transparency and accountability—two pillars of credible data analysis. Furthermore, as datasets grow, bias does not disappear; it scales. If underlying data are incomplete, unrepresentative, or contextually skewed, bigger datasets simply reproduce error with greater confidence.\nClosely related is the danger of overfitting and overparameterization. In large models, especially machine learning systems, it’s easy to fit data so perfectly that the model memorizes patterns rather than generalizes them. This creates high accuracy in training but failure in prediction. Overfitting is a form of overconfidence disguised as precision—it reflects modeling power used without restraint. The solution, as both Hadley Wickham and Andrew Gelman emphasize, lies in simplicity, cross-validation, and clarity: build models that explain rather than impress.\nEthically, the pitfalls of big data remind us that quantity does not equal quality. A million observations cannot compensate for poor measurement, biased sampling, or conceptual confusion. The role of the analyst is to filter wisely, visualize responsibly, and remain skeptical of “too-good-to-be-true” correlations. In the end, meaningful data analysis depends not on how much data we have, but on how thoughtfully we use it."
  },
  {
    "objectID": "EPPS6356_PrepareforClass5.html#hadley-wickham-data-visualization-and-data-science-embl-summary-and-commentary",
    "href": "EPPS6356_PrepareforClass5.html#hadley-wickham-data-visualization-and-data-science-embl-summary-and-commentary",
    "title": "Prepare for Class 5",
    "section": "Hadley Wickham: Data Visualization and Data Science (EMBL) — Summary and Commentary",
    "text": "Hadley Wickham: Data Visualization and Data Science (EMBL) — Summary and Commentary\nIn his EMBL talk, Hadley Wickham explains that data visualization is the bridge between data and understanding. He introduces the “grammar of graphics” — a structured way to describe visualizations as combinations of data, aesthetic mappings, and geometries. This framework, implemented in his ggplot2 package, allows analysts to move from ad-hoc plotting to reproducible, composable graphics. Every visual element in a chart—points, lines, bars, colors, or facets—corresponds to a decision in the grammar, making visual design transparent and logical.\nWickham also connects visualization to the broader workflow of data science: importing, tidying, transforming, visualizing, and modeling. He emphasizes that visualization is not the end of analysis but the middle of thinking—a feedback loop that helps analysts notice patterns, spot errors, and refine questions. The best insights often emerge while visualizing, not after modeling.\nAnother key contribution is his advocacy for tidy data principles, where each variable forms a column, each observation a row, and each dataset a table. This structure allows visualization tools and analytical packages to interoperate seamlessly. Combined with ggplot2, dplyr, and tidyr, the tidyverse creates an ecosystem where clarity, readability, and reproducibility come first.\nWickham’s main message parallels Tufte’s: clarity is ethics in data visualization. The goal is not to impress audiences with complexity but to reveal truth with simplicity. In practice, this means designing graphics that emphasize structure over style, accuracy over decoration, and transparency over spectacle. His work has transformed visualization from a craft of intuition into a system of reasoning—a grammar that turns data into readable, trustworthy stories.\n\n\n“The purpose of visualization is not to show data; it’s to make people think about data.” — Hadley Wickham"
  },
  {
    "objectID": "EPPS6356_PrepareforClass4.html",
    "href": "EPPS6356_PrepareforClass4.html",
    "title": "Prepare for Class 4",
    "section": "",
    "text": "Edward Tufte’s lecture The Future of Data Analysis extends his lifelong campaign for clarity and integrity in how information is displayed. He opens with a simple but radical idea: seeing well is thinking well. In a world overflowing with dashboards, metrics, and automated analytics, Tufte argues that the next frontier of data analysis is not technological—it is philosophical. What matters most is cultivating the human capacity to notice patterns, relationships, and meaning hidden within complexity. Tools can calculate, but only the human mind can interpret.\nThroughout the talk, Tufte revisits his central design principles—truthfulness, precision, and beauty—and re-frames them for an era dominated by algorithms. He traces a line from Galileo’s telescopic sketches to today’s interactive data dashboards, reminding the audience that visualization has always been a cognitive technology: a way to extend perception and reasoning. When we visualize data, we are literally “making thought visible.” Yet he warns that as graphics become more automated, the analyst’s curiosity and skepticism can erode. “Never let the software think for you,” he says, stressing that each chart must emerge from a clear analytical question rather than a template.\nA major theme is integration—of quantitative accuracy with qualitative understanding. Tufte envisions the analyst of the future as both scientist and artist, someone who can write statistical code and also judge visual proportion, hierarchy, and rhythm. This combination of reasoning and design, he suggests, produces visualizations that work—they inform without distorting and persuade without manipulating. He calls for “high data density with low noise,” meaning that every pixel should carry meaning and every visual choice should serve evidence, not decoration.\nThe talk also critiques the overuse of dashboards and “chartjunk.” When organizations rely on flashy but shallow summaries, they risk mistaking performance metrics for knowledge. Tufte argues that genuine understanding requires context: side-by-side comparisons, annotated evidence, and time-series continuity. His principle of small multiples—repeated, comparable views—remains one of the most powerful ways to show change without exaggeration. He contrasts this disciplined approach with sensational graphs that truncate axes or hide uncertainty, calling such distortions ethical failures, not mere design flaws.\nAnother striking idea is the role of narrative in analysis. Tufte insists that visualization should not replace verbal explanation but enhance it. Numbers and stories must coexist: data show what happened, while narrative clarifies why it matters. The most effective visualizations invite the viewer to reason, not just react. They slow perception down enough for reflection—a quality increasingly rare in the fast-scroll world of media and business intelligence tools.\nBy the end, Tufte situates the “future” of data analysis in a moral framework. The abundance of data amplifies both our power and our responsibility. Analysts and designers are now interpreters of reality; their choices can clarify truth or conceal it. He challenges his audience to treat every visualization as an ethical document—something that must earn trust through accuracy, transparency, and aesthetic restraint.\nUltimately, Tufte’s message is timeless: technology may evolve, but the principles of clear thinking do not. The real progress in data analysis will come not from more dashboards, but from deeper attention. The analyst of the future is not a technician running models, but a human observer capable of transforming raw information into understanding.\n\n\n“The future of data analysis is not a machine. It’s a human being with better eyesight.” — Edward Tufte"
  },
  {
    "objectID": "EPPS6356_PrepareforClass4.html#one-page-review-the-future-of-data-analysis-edward-tufte-2016",
    "href": "EPPS6356_PrepareforClass4.html#one-page-review-the-future-of-data-analysis-edward-tufte-2016",
    "title": "Prepare for Class 4",
    "section": "",
    "text": "Edward Tufte’s lecture The Future of Data Analysis extends his lifelong campaign for clarity and integrity in how information is displayed. He opens with a simple but radical idea: seeing well is thinking well. In a world overflowing with dashboards, metrics, and automated analytics, Tufte argues that the next frontier of data analysis is not technological—it is philosophical. What matters most is cultivating the human capacity to notice patterns, relationships, and meaning hidden within complexity. Tools can calculate, but only the human mind can interpret.\nThroughout the talk, Tufte revisits his central design principles—truthfulness, precision, and beauty—and re-frames them for an era dominated by algorithms. He traces a line from Galileo’s telescopic sketches to today’s interactive data dashboards, reminding the audience that visualization has always been a cognitive technology: a way to extend perception and reasoning. When we visualize data, we are literally “making thought visible.” Yet he warns that as graphics become more automated, the analyst’s curiosity and skepticism can erode. “Never let the software think for you,” he says, stressing that each chart must emerge from a clear analytical question rather than a template.\nA major theme is integration—of quantitative accuracy with qualitative understanding. Tufte envisions the analyst of the future as both scientist and artist, someone who can write statistical code and also judge visual proportion, hierarchy, and rhythm. This combination of reasoning and design, he suggests, produces visualizations that work—they inform without distorting and persuade without manipulating. He calls for “high data density with low noise,” meaning that every pixel should carry meaning and every visual choice should serve evidence, not decoration.\nThe talk also critiques the overuse of dashboards and “chartjunk.” When organizations rely on flashy but shallow summaries, they risk mistaking performance metrics for knowledge. Tufte argues that genuine understanding requires context: side-by-side comparisons, annotated evidence, and time-series continuity. His principle of small multiples—repeated, comparable views—remains one of the most powerful ways to show change without exaggeration. He contrasts this disciplined approach with sensational graphs that truncate axes or hide uncertainty, calling such distortions ethical failures, not mere design flaws.\nAnother striking idea is the role of narrative in analysis. Tufte insists that visualization should not replace verbal explanation but enhance it. Numbers and stories must coexist: data show what happened, while narrative clarifies why it matters. The most effective visualizations invite the viewer to reason, not just react. They slow perception down enough for reflection—a quality increasingly rare in the fast-scroll world of media and business intelligence tools.\nBy the end, Tufte situates the “future” of data analysis in a moral framework. The abundance of data amplifies both our power and our responsibility. Analysts and designers are now interpreters of reality; their choices can clarify truth or conceal it. He challenges his audience to treat every visualization as an ethical document—something that must earn trust through accuracy, transparency, and aesthetic restraint.\nUltimately, Tufte’s message is timeless: technology may evolve, but the principles of clear thinking do not. The real progress in data analysis will come not from more dashboards, but from deeper attention. The analyst of the future is not a technician running models, but a human observer capable of transforming raw information into understanding.\n\n\n“The future of data analysis is not a machine. It’s a human being with better eyesight.” — Edward Tufte"
  },
  {
    "objectID": "EPPS6302_Assignment4.html",
    "href": "EPPS6302_Assignment4.html",
    "title": "Assignment 4: Webscraping 1",
    "section": "",
    "text": "library(rvest) library(xml2) library(dplyr) library(readr)"
  },
  {
    "objectID": "EPPS6302_Assignment4.html#load-required-packages",
    "href": "EPPS6302_Assignment4.html#load-required-packages",
    "title": "Assignment 4: Webscraping 1",
    "section": "",
    "text": "library(rvest)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union"
  },
  {
    "objectID": "EPPS6302_Assignment4.html#scrape-foreign-exchange-reserves-table-original-task",
    "href": "EPPS6302_Assignment4.html#scrape-foreign-exchange-reserves-table-original-task",
    "title": "Assignment 4: Webscraping 1",
    "section": "2. Scrape Foreign Exchange Reserves Table (Original Task)",
    "text": "2. Scrape Foreign Exchange Reserves Table (Original Task)\n\nurl_fx &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves\"\n\nfx_page &lt;- read_html(url_fx)\n\n# Extract all tables\nfx_tables &lt;- html_table(html_nodes(fx_page, \"table\"), fill = TRUE)\n\n# Use the first table with country-wise reserves\nfx_df &lt;- fx_tables[[1]]\n\n# Fix duplicated column names\nnames(fx_df) &lt;- make.names(names(fx_df), unique = TRUE)\n\n# Rename and clean\nfx_clean &lt;- fx_df %&gt;%\n  rename(\n    Country = 1,\n    Reserves_USD_Billion = 2,\n    Date = 3,\n    Source = 4\n  ) %&gt;%\n  filter(!is.na(Reserves_USD_Billion), Country != \"\") %&gt;%\n  mutate(\n    Reserves_USD_Billion = as.numeric(gsub(\",\", \"\", Reserves_USD_Billion)),\n    Date = str_trim(Date)\n  )\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `Reserves_USD_Billion = as.numeric(gsub(\",\", \"\",\n  Reserves_USD_Billion))`.\nCaused by warning:\n! NAs introduced by coercion\n\nhead(fx_clean)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\nReserves_USD_Billion\nDate\nSource\nForeign.exchange.reserves.2\nForeign.exchange.reserves.3\nLast.reporteddate\nRef.\n\n\n\n\nCountry(as recognized by the U.N.)\nNA\nIncluding gold\nIncluding gold\nExcluding gold\nExcluding gold\nLast reporteddate\nRef.\n\n\nCountry(as recognized by the U.N.)\nNA\nmillions U.S.$\nChange\nmillions U.S.$\nChange\nLast reporteddate\nRef.\n\n\nChina\nNA\n3,643,149\n41,079\n3,389,306\n31,221\n31 Aug 2025\n[3]\n\n\nJapan\nNA\n1,324,210\n19,774\n1,230,940\n16,230\n31 Aug 2025\n[4]\n\n\nSwitzerland\nNA\n1,007,710\n13,935\n897,295\n14,490\n31 Jul 2025\n[5]\n\n\nRussia\nNA\n713,300\n700\n434,487\n1,517\n26 Sep 2025\n[6]"
  },
  {
    "objectID": "EPPS6302_Assignment4.html#scrape-gdp-data-by-country-modified-table",
    "href": "EPPS6302_Assignment4.html#scrape-gdp-data-by-country-modified-table",
    "title": "Assignment 4: Webscraping 1",
    "section": "3. Scrape GDP Data by Country (Modified Table)",
    "text": "3. Scrape GDP Data by Country (Modified Table)\n\nurl_gdp &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n\ngdp_page &lt;- read_html(url_gdp)\n\ngdp_tables &lt;- html_table(html_nodes(gdp_page, \"table\"), fill = TRUE)\n\n# We'll use the second table\ngdp_df &lt;- gdp_tables[[2]]\n\nhead(gdp_df)\n\n\n\n\n\n\n\n\n\nX1\nX2\n\n\n\n\n.mw-parser-output .legend{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .legend-color{display:inline-block;min-width:1.25em;height:1.25em;line-height:1.25;margin:1px 0;text-align:center;border:1px solid black;background-color:transparent;color:black}.mw-parser-output .legend-text{}  &gt; $20 trillion   $10–20 trillion   $5–10 trillion   $1–5 trillion   $750 billion – $1 trillion   $500–750 billion\n$250–500 billion   $100–250 billion   $50–100 billion   $25–50 billion   $5–25 billion   &lt; $5 billion"
  },
  {
    "objectID": "EPPS6302_Assignment4.html#suggest-a-data-plan-for-research",
    "href": "EPPS6302_Assignment4.html#suggest-a-data-plan-for-research",
    "title": "Assignment 4: Webscraping 1",
    "section": "5. Suggest a Data Plan for Research",
    "text": "5. Suggest a Data Plan for Research\n\nTo study global financial preparedness and development levels, I propose merging Wikipedia data on foreign reserves and GDP by country. This will allow researchers to calculate the ratio of foreign reserves to GDP, a useful indicator for macroeconomic stability and risk exposure. Future work may include scraping literacy rate, inflation, or debt data to analyze patterns across regions using R visualizations, regressions, and mapping tools."
  },
  {
    "objectID": "EPPS6302_Assignment4.html#conclusion",
    "href": "EPPS6302_Assignment4.html#conclusion",
    "title": "Assignment 4: Webscraping 1",
    "section": "Conclusion",
    "text": "Conclusion\nThe assignment requirements are satisfied by: (1) reproducing the base scrape, (2) modifying it to another table on the same page, (3) cleaning with a DateCollected variable and removal of extra rows/columns, and (4) outlining a practical acquisition plan. Both cleaned datasets are exported and linked above for download."
  },
  {
    "objectID": "EPPS6302_Assignment5.html",
    "href": "EPPS6302_Assignment5.html",
    "title": "Assignment 5: Government Data API",
    "section": "",
    "text": "This assignment demonstrates how to access and download government data using the GovInfo API, focusing on the ten most recent documents from the Foreign Relations Committee.\nThe script automates data retrieval, cleaning, and export into a CSV file."
  },
  {
    "objectID": "EPPS6302_Assignment5.html#objective",
    "href": "EPPS6302_Assignment5.html#objective",
    "title": "Assignment 5: Government Data API",
    "section": "",
    "text": "This assignment demonstrates how to access and download government data using the GovInfo API, focusing on the ten most recent documents from the Foreign Relations Committee.\nThe script automates data retrieval, cleaning, and export into a CSV file."
  },
  {
    "objectID": "EPPS6302_Assignment5.html#r-script-govt-data-retrieval",
    "href": "EPPS6302_Assignment5.html#r-script-govt-data-retrieval",
    "title": "Assignment 5: Government Data API",
    "section": "2. R Script: Govt Data Retrieval",
    "text": "2. R Script: Govt Data Retrieval\n\n# -------------------------------\n# Load Required Libraries\n# -------------------------------\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(dplyr)\n\n# -------------------------------\n# Define API URL and Parameters\n# -------------------------------\nbase_url &lt;- \"https://api.govinfo.gov/search\"\napi_key &lt;- \"DEMO_KEY\"  # Replace with your actual API key from https://api.govinfo.gov/docs/api/key/\n\nparams &lt;- list(\n  query = \"Foreign Relations Committee\",\n  pageSize = 10,\n  sortBy = \"date\",\n  sortOrder = \"desc\",\n  api_key = api_key\n)\n\n# -------------------------------\n# API Request\n# -------------------------------\nresponse &lt;- GET(base_url, query = params)\n\nif (status_code(response) == 200) {\n  data &lt;- content(response, as = \"text\", encoding = \"UTF-8\")\n  json_data &lt;- fromJSON(data, flatten = TRUE)\n  \n  # Extract relevant fields\n  docs &lt;- json_data$results %&gt;%\n    select(title, collectionCode, dateIssued, govInfoLink = link)\n  \n  # Save to CSV\n  write.csv(docs, \"ForeignRelations_10Docs.csv\", row.names = FALSE)\n  \n  print(\"✅ Data successfully saved as ForeignRelations_10Docs.csv\")\n  head(docs)\n  \n} else {\n  print(paste(\"❌ API request failed with status:\", status_code(response)))\n}\n\n[1] \"❌ API request failed with status: 400\""
  },
  {
    "objectID": "EPPS6302_Assignment5.html#output-preview",
    "href": "EPPS6302_Assignment5.html#output-preview",
    "title": "Assignment 5: Government Data API",
    "section": "3. Output Preview",
    "text": "3. Output Preview\n\n# View the first few results (preview)\nhead(docs)"
  },
  {
    "objectID": "EPPS6302_Assignment5.html#reflection-report",
    "href": "EPPS6302_Assignment5.html#reflection-report",
    "title": "Assignment 5: Government Data API",
    "section": "4. Reflection Report",
    "text": "4. Reflection Report\n\n4.1 Difficulties Encountered\n\nAPI Key Access — The GovInfo API requires a registered key; using the demo key can restrict access or limit the number of requests.\n\nComplex JSON Structure — Extracting relevant elements like title, date, and links required flattening nested lists.\n\nQuery Sensitivity — Generic searches for “Foreign Relations Committee” sometimes returned unrelated documents without filters.\n\nPagination & Sorting — Ensuring the newest records appeared first required sortBy = \"date\" and sortOrder = \"desc\".\n\n\n\n\n4.2 How Usable Was the Scraped Data?\nThe resulting dataset was structured and well-formatted, containing document metadata (titles, collection codes, dates, and links).\nWhile the data is clean and suitable for statistical analysis or visualization, the document content itself (e.g., full hearing transcripts) is not included — only the metadata and URLs.\n\n\n\n4.3 How to Improve\n\nText Extraction: Use pdftools to download and extract text from linked documents for deeper content analysis.\n\nAutomated Scheduling: Implement cronR or taskscheduleR to update datasets regularly.\n\nAdvanced Filtering: Include additional parameters like collectionCode = 'CHRG' to narrow to congressional hearings.\n\nVisualization: Add summary charts (e.g., document counts by year or committee)."
  },
  {
    "objectID": "EPPS6302_Assignment5.html#conclusion",
    "href": "EPPS6302_Assignment5.html#conclusion",
    "title": "Assignment 5: Government Data API",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nThis assignment highlights the importance of API-based government data access for reproducible research.\nThrough this workflow, analysts can directly retrieve structured data, analyze policy documents, and maintain transparency and replicability in social research."
  },
  {
    "objectID": "EPPS6302_Assignment5.html#references",
    "href": "EPPS6302_Assignment5.html#references",
    "title": "Assignment 5: Government Data API",
    "section": "6. References",
    "text": "6. References\n\nU.S. Government Publishing Office. (n.d.). GovInfo API Documentation. https://api.govinfo.gov/docs/\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. O’Reilly Media."
  },
  {
    "objectID": "EPPS6356_Assignment4.html",
    "href": "EPPS6356_Assignment4.html",
    "title": "Assignment 4 — Data Visualization Hackathon",
    "section": "",
    "text": "Overview\nThis page presents four charts produced in R for the Assignment 4 hackathon. The structure mirrors the reference example, with each chart in its own section containing:\n\nA short design rationale (goal, audience, design choices),\nReproducible R code (using ggplot2), and\nThe rendered figure.\n\n\nNote on datasets: To ensure this renders anywhere without external files, this page uses built-in R datasets (mpg, economics) that ship with ggplot2. You can swap in your own data later while keeping the same structure.\n\n\n\n\nReusable Setup\n\n\n\nChart 1 — Scatter with trend & annotation\n\np1 &lt;- ggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(alpha = 0.8, size = 2) +\n  geom_smooth(method = \"loess\", se = TRUE) +\n  labs(\n    title = \"Bigger Engines, Lower MPG\",\n    subtitle = \"Highway MPGs generally decrease as engine displacement increases\",\n    x = \"Engine Displacement (liters)\",\n    y = \"Highway MPG\",\n    caption = \"Source: ggplot2::mpg\"\n  ) +\n  viz_theme()\n\np1\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nChart 2 — Ranked bars with labels\n\nrank_dat &lt;- mpg %&gt;%\n  group_by(manufacturer) %&gt;%\n  summarise(avg_cty = mean(cty, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_cty)) %&gt;%\n  slice_head(n = 12) %&gt;%\n  mutate(manufacturer = reorder(manufacturer, avg_cty))\n\np2 &lt;- ggplot(rank_dat, aes(x = manufacturer, y = avg_cty)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(aes(label = round(avg_cty, 1)), hjust = -0.05, size = 3.2) +\n  expand_limits(y = max(rank_dat$avg_cty) * 1.12) +\n  labs(\n    title = \"Who’s Most Efficient in the City?\",\n    subtitle = \"Average city MPG by manufacturer (top 12)\",\n    x = NULL, y = \"Avg City MPG\",\n    caption = \"Source: ggplot2::mpg\"\n  ) +\n  viz_theme()\n\np2\n\n\n\n\n\n\n\n\n\n\n\nChart 3 — Time series with recession bands\n\nrecessions &lt;- data.frame(\n  start = as.Date(c(\"1973-11-01\", \"1980-01-01\", \"1981-07-01\", \"1990-07-01\", \"2001-03-01\", \"2007-12-01\")),\n  end   = as.Date(c(\"1975-03-01\", \"1980-07-01\", \"1982-11-01\", \"1991-03-01\", \"2001-11-01\", \"2009-06-01\"))\n)\n\np3 &lt;- ggplot(economics, aes(x = date, y = unemploy)) +\n  geom_rect(data = recessions, inherit.aes = FALSE,\n            aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf),\n            alpha = 0.12) +\n  geom_line(linewidth = 0.8) +\n  scale_y_continuous(labels = label_number(scale_cut = cut_si(\"\"))) +\n  labs(\n    title = \"Unemployment Rises in Downturns\",\n    subtitle = \"Shaded bands show select recession periods\",\n    x = NULL, y = \"Unemployed persons\",\n    caption = \"Source: ggplot2::economics; recessions illustrative\"\n  ) +\n  viz_theme()\n\np3\n\n\n\n\n\n\n\n\n\n\n\nChart 4 — Distribution (box/violin)\n\nmpg2 &lt;- mpg %&gt;% mutate(cyl_f = factor(cyl))\n\np4 &lt;- ggplot(mpg2, aes(x = drv, y = hwy)) +\n  geom_violin(trim = TRUE, alpha = 0.6) +\n  geom_boxplot(width = 0.12, outlier.alpha = 0.25) +\n  facet_wrap(~ cyl_f, nrow = 2) +\n  labs(\n    title = \"Efficiency Differs by Drivetrain & Cylinders\",\n    subtitle = \"Distributions of highway MPG across drive types, split by cylinder count\",\n    x = \"Drivetrain (f = FWD, r = RWD, 4 = 4WD)\",\n    y = \"Highway MPG\",\n    caption = \"Source: ggplot2::mpg\"\n  ) +\n  viz_theme()\n\np4\n\nWarning: Groups with fewer than two datapoints have been dropped.\nℹ Set `drop = FALSE` to consider such groups for position adjustment purposes."
  }
]