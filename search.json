[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Eman Ajmal is a social analyst."
  },
  {
    "objectID": "Assignment4.html",
    "href": "Assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Difference between a weak and a strong entity set:\n\nA strong entity set can be identified uniquely by its own attributes, such as an Employee with an Employee ID.\nA weak entity set on the other hand, cannot be identified by its own attributes alone and depends on a strong entity (also known as the owner entity) for identification. An example could be a Dependent entity that relies on the Employee entity for its identification (e.g., an employee’s dependent might be identified by the Employee ID and Dependent’s name)."
  },
  {
    "objectID": "Assignment4.html#question-1",
    "href": "Assignment4.html#question-1",
    "title": "Assignment 4",
    "section": "",
    "text": "Difference between a weak and a strong entity set:\n\nA strong entity set can be identified uniquely by its own attributes, such as an Employee with an Employee ID.\nA weak entity set on the other hand, cannot be identified by its own attributes alone and depends on a strong entity (also known as the owner entity) for identification. An example could be a Dependent entity that relies on the Employee entity for its identification (e.g., an employee’s dependent might be identified by the Employee ID and Dependent’s name)."
  },
  {
    "objectID": "Assignment4.html#question-2",
    "href": "Assignment4.html#question-2",
    "title": "Assignment 4",
    "section": "Question 2",
    "text": "Question 2\nDesign an E-R diagram for keeping track of the scoring statistics of your favorite sports team. You should store the matches played, the scores in each match, the players in each match, and individual player scoring statistics for each match. Summary statistics should be modeled as derived attributes with an explanation as to how they are computed.\n\nDraw the E-R diagram using draw.io.\nExpand to all teams in the league (Hint: add team entity)"
  },
  {
    "objectID": "Assignment4.html#question-3",
    "href": "Assignment4.html#question-3",
    "title": "Assignment 4",
    "section": "Question 3",
    "text": "Question 3\nSQL exercise:\n\nWrite an SQL query using the university schema to find the ID of each student who has never taken a course at the university. Do this using no subqueries and no set operations (use an outer join).\n\n\n\nConsider the following database, write a query to find the ID of each employee with no manager. Note that an employee may simply have no manager listed or may have a null manager(use natural left outer join).\n\nSELECT e.ID\nFROM employee e\nOUTER JOIN manages m ON e.ID = m.ID\nWHERE m.manager_id IS NULL;"
  },
  {
    "objectID": "Assignment6.html",
    "href": "Assignment6.html",
    "title": "Assignment 6",
    "section": "",
    "text": "Question 1\n\nUsing JSON YouTube: YouTube uses JSON to deliver video metadata, comments, and user interactions through its API. The JSON structure is hierarchical and includes nested fields for video IDs, thumbnails, and statistics such as views and likes. YouTube’s backend likely uses BigQuery and other Google Cloud services to store and manage data. The API fetches this data in JSON format, commonly consumed by frontend JavaScript frameworks.\nUsing XML W3Schools: W3Schools offers XML data examples, including a sample file that provides information on books. The XML structure uses descriptive tags such as , , and . The data can be accessed using XML parsers in JavaScript or server-side languages. Technologies like DOM and XSLT are used to query or transform the XML structure for web display.\n\n\n\nQuestion 2\n\n\n\nSELECT student.ID\nFROM student\nLEFT JOIN advisor ON student.ID = advisor.s_ID\nWHERE advisor.i_ID IS NULL;\n\n\n\nSELECT I.ID, I.name\nFROM instructor I\nWHERE NOT EXISTS (\n  SELECT *\n  FROM course C\n  WHERE C.dept_name = I.dept_name\n    AND NOT EXISTS (\n      SELECT *\n      FROM teaches T\n      WHERE T.ID = I.ID AND T.course_id = C.course_id\n    )\n)\nORDER BY I.name;\n\n\nQuestion 3\n\n\n\n\n   id     name              dept_name      salary\n1  63395  McKinnon          Cybernetics    94333.99\n2  78699  Pingr             Statistics     59303.62\n3  96895  Mird              Marketing      119921.41\n4  4233   Luo               English        88791.45\n5  4034   Murata            Athletics      61387.56\n6  50885  Konstantinides    Languages      32570.50\n\n\n\n   id     name              dept_name      salary\n1  63395  McKinnon          Cybernetics    94333.99\n2  78699  Pingr             Statistics     59303.62\n3  96895  Mird              Marketing      119921.41\n4  4233   Luo               English        88791.45\n5  4034   Murata            Athletics      61387.56\n6  50885  Konstantinides    Languages      32570.50\n\n\n\n    id       name           dept_name       tot_cred\n1   79352    Rumat          Finance          100\n2   76672    Miliko         Statistics       116\n3   14182    Moszkowski     Civil Eng.       73\n4   44985    Prieto         Biology          91\n5   44271    Sowerby        English          108\n6   40897    Coppens        Math             58\n\n\n\n   id       name           dept_name        salary\n1  63395    McKinnon       Cybernetics      94333.99\n2  78699    Pingr          Statistics       59303.62\n3  96895    Mird           Marketing        119921.41\n4  4233     Luo            English          88791.45\n5  4034     Murata         Athletics        61387.56\n6  50885    Konstantinides Languages        32570.50\n7  79653    Levine         Elec. Eng.       89805.83\n8  50330    Shuming        Physics          108011.81\n9  80759    Queiroz        Biology          45538.32\n10 73623    Sullivan       Elec. Eng.       90038.09\n11 97302    Bertolino      Mech. Eng.       51647.57\n12 57180    Hau            Accounting       43966.29\n13 74420    Voronina       Physics          121141.99\n14 35579    Soisalon-...   Psychology       62579.61\n15 31955    Moreira        Accounting       71351.42\n16 37687    Arias          Statistics       104563.38\n17 6569     Mingoz         Finance          105311.38\n18 16807    Yazdi          Athletics        98333.65\n19 14365    Lembr          Accounting       32241.56\n20 90643    Choll          Statistics       57807.09\n21 81991    Valtchev       Biology          77036.18\n22 95030    Arinb          Statistics       54805.11\n23 15347    Bawa           Athletics        72140.88\n24 74426    Kenje          Marketing        106554.73\n25 42782    Vicentino      Elec. Eng.       34272.67\n26 58558    Dusserre       Marketing        66143.25\n27 63287    Jaekel         Athletics        103146.87\n28 59795    Desyl          Languages        48803.38\n29 22591    DAgostino      Psychology       59706.49\n30 48570    Sarkar         Pol. Sci.        87549.80\n31 79081    Ullman         Accounting       47307.10\n32 52647    Bancilhon      Pol. Sci.        87958.01\n33 25946    Liley          Languages        90891.69\n34 36897    Morris         Marketing        43770.36\n35 72553    Yin            English          46397.59\n36 3199     Gustafsson     Elec. Eng.       82534.37\n37 34175    Bondi          Comp. Sci.       115469.11\n38 48507    Lent           Mech. Eng.       107978.47\n39 65931    Pimenta        Cybernetics      79866.95\n40 3335     Bourrier       Comp. Sci.       80797.83\n41 64871    Gutierrez      Statistics       45310.53\n42 95709    Sakurai        English          118143.98\n43 43779    Romero         Astronomy        79070.08\n44 77346    Mahmoud        Geology          99382.59\n45 28097    Kean           English          35023.18\n46 90376    Bietzk         Cybernetics      117836.50\n47 28400    Atanassov      Statistics       84982.92\n48 41930    Tung           Athletics        50482.03\n49 19368    Wieland        Pol. Sci.        124651.41\n50 99052    Dale           Cybernetics      93348.83"
  },
  {
    "objectID": "Assignment3.html",
    "href": "Assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Write SQL codes to get a list of:\n\nStudents IDs (hint: from the takes relation)\n\n\n\nInstructors\n\n\n\nDepartments\n\n\n\n\n\nWrite in SQL codes to do following queries:\n\nFind the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\n\n\n\nAdd grades to the list\n\n\n\nFind the ID and name of each student who has not taken any course offered before 2017.\n\n\n\nFor each department, find the maximum salary of instructors in that department. You may assume that every department has at least one instructor.\n\n\n\nFind the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\n\n\n\nAdd names to the list\n\n\n\n\n\nFind instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)"
  },
  {
    "objectID": "Assignment3.html#question-2",
    "href": "Assignment3.html#question-2",
    "title": "Assignment 3",
    "section": "",
    "text": "Write SQL codes to get a list of:\n\nStudents IDs (hint: from the takes relation)\n\n\n\nInstructors\n\n\n\nDepartments"
  },
  {
    "objectID": "Assignment3.html#question-3",
    "href": "Assignment3.html#question-3",
    "title": "Assignment 3",
    "section": "",
    "text": "Write in SQL codes to do following queries:\n\nFind the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\n\n\n\nAdd grades to the list\n\n\n\nFind the ID and name of each student who has not taken any course offered before 2017.\n\n\n\nFor each department, find the maximum salary of instructors in that department. You may assume that every department has at least one instructor.\n\n\n\nFind the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\n\n\n\nAdd names to the list"
  },
  {
    "objectID": "Assignment3.html#question-4",
    "href": "Assignment3.html#question-4",
    "title": "Assignment 3",
    "section": "",
    "text": "Find instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Name and describe three applications you have used that employed a database system to store and access persistent data (e.g., airlines, online trade, banking, university system).\n\n\n\nHow it Uses Databases: Stores customer data, transactions, and account details.\nReasoning: Ensures secure, fast, and reliable access to banking information.\n\n\n\n\n\nHow it Uses Databases: Manages product listings, customer orders, and payments.\nReasoning: Uses a relational database to store structured data and provide recommendations.\n\n\n\n\n\nHow it Uses Databases: Stores student records, enrollments, and grades.\nReasoning: Ensures students and faculty have real-time access to course materials and academic records.\n\n\n\n\n\nPropose three applications in domain projects (e.g., criminology, economics, brain science, etc.). Be sure to include: 1.Purpose 2.Functions 3.Simple interface design\n\n\n\nPurpose: Measures and tracks social well-being indicators like mental health, employment, and community engagement.\nFunctions:\n\nCollects survey responses on life satisfaction, stress levels, and social interactions.\nTracks changes in employment, education, and local crime rates.\nProvides visual reports and insights for policymakers and community leaders.\n\nSimple Interface Design: A dashboard displays key indicators like mental health scores, employment trends, and community engagement in charts and graphs. Users can fill out surveys to update data, and policymakers can generate reports with visual insights. The interface includes a homepage overview, survey page, and a report export option.\n\n\n\n\n\nPurpose: Analyzes public sentiment on social issues by gathering data from social media and news sources.\nFunctions:\n\nCollects and categorizes posts on topics like inequality, human rights, and policy changes.\nUses sentiment analysis to determine whether opinions are positive, negative, or neutral.\nGenerates reports on trending social issues over time.\n\nSimple Interface Design: Users enter keywords to analyze public sentiment from news and social media, displayed through a word cloud, sentiment pie chart, and regional heatmap. A live feed shows relevant posts, and a report generation page allows data exports. The interface is search-focused with real-time insights.\n\n\n\n\n\nPurpose: Tracks data on economic and social mobility to understand inequality trends in different regions.\nFunctions:\n\nStores data on income, education, and employment trends across communities.\nProvides comparison charts on social mobility rates based on location, gender, and ethnicity.\nRecommends policies and resources to improve access to opportunities.\n\nSimple Interface Design: An interactive heat map and dashboard display income trends, education levels, and employment data by region. Users can compare multiple areas, view bar charts, and access policy recommendations. The interface supports searching, visual comparisons, and report downloads.\n\n\n\n\nIf data can be retrieved efficiently and effectively, why is data mining needed?\nData mining is essential because it allows us to extract meaningful insights from large datasets beyond just retrieving stored data. Below are key reasons why data mining is needed:\n\n\n\n\nDatabases can retrieve data efficiently, but they don’t automatically find patterns or trends.\n\n\n\n\n\nData mining helps predict future trends based on historical data.\n\n\n\n\n\nBusinesses use data mining to make data-driven decisions rather than relying on intuition.\n\n\n\n\n\nBig data is too large and complex for traditional database queries alone. Data mining helps extract valuable insights from massive datasets.\n\n\n\n\n\nCompanies use data mining to gain a competitive edge by identifying market trends, customer preferences, and business risks.\n\n\n\n\n\nDescribe at least three tables that might be used to store information in a social network/social media system such as Twitter or Reddit.\nA social media platform like Twitter or Reddit would require multiple tables for structured data management. Below are three essential tables:\n\n\n\nStores information about users.\n\n\n\nColumn Name\nDescription\n\n\n\n\nUser ID\nUnique identifier for each user\n\n\nUsername\nDisplay name of the user\n\n\nEmail\nContact email\n\n\nPassword\nEncrypted password\n\n\nDate Joined\nDate when the user registered\n\n\n\n\n\n\n\n\nStores tweets or Reddit posts.\n\n\n\nColumn Name\nDescription\n\n\n\n\nPost ID\nUnique identifier for each post\n\n\nUser ID\nID of the user who posted\n\n\nPost Content\nText or media content\n\n\nCreated At\nTimestamp of post creation\n\n\nLikes Count\nNumber of likes\n\n\nComments Count\nNumber of comments\n\n\n\n\n\n\n\n\nStores relationships between users.\n\n\n\nColumn Name\nDescription\n\n\n\n\nFollower ID\nID of the user who follows\n\n\nFollowed ID\nID of the user being followed\n\n\nDate Followed\nTimestamp of when the follow occurred"
  },
  {
    "objectID": "Assignment1.html#question-1",
    "href": "Assignment1.html#question-1",
    "title": "Assignment 1",
    "section": "",
    "text": "Name and describe three applications you have used that employed a database system to store and access persistent data (e.g., airlines, online trade, banking, university system).\n\n\n\nHow it Uses Databases: Stores customer data, transactions, and account details.\nReasoning: Ensures secure, fast, and reliable access to banking information.\n\n\n\n\n\nHow it Uses Databases: Manages product listings, customer orders, and payments.\nReasoning: Uses a relational database to store structured data and provide recommendations.\n\n\n\n\n\nHow it Uses Databases: Stores student records, enrollments, and grades.\nReasoning: Ensures students and faculty have real-time access to course materials and academic records."
  },
  {
    "objectID": "Assignment1.html#question-2",
    "href": "Assignment1.html#question-2",
    "title": "Assignment 1",
    "section": "",
    "text": "Propose three applications in domain projects (e.g., criminology, economics, brain science, etc.). Be sure to include: 1.Purpose 2.Functions 3.Simple interface design\n\n\n\nPurpose: Measures and tracks social well-being indicators like mental health, employment, and community engagement.\nFunctions:\n\nCollects survey responses on life satisfaction, stress levels, and social interactions.\nTracks changes in employment, education, and local crime rates.\nProvides visual reports and insights for policymakers and community leaders.\n\nSimple Interface Design: A dashboard displays key indicators like mental health scores, employment trends, and community engagement in charts and graphs. Users can fill out surveys to update data, and policymakers can generate reports with visual insights. The interface includes a homepage overview, survey page, and a report export option.\n\n\n\n\n\nPurpose: Analyzes public sentiment on social issues by gathering data from social media and news sources.\nFunctions:\n\nCollects and categorizes posts on topics like inequality, human rights, and policy changes.\nUses sentiment analysis to determine whether opinions are positive, negative, or neutral.\nGenerates reports on trending social issues over time.\n\nSimple Interface Design: Users enter keywords to analyze public sentiment from news and social media, displayed through a word cloud, sentiment pie chart, and regional heatmap. A live feed shows relevant posts, and a report generation page allows data exports. The interface is search-focused with real-time insights.\n\n\n\n\n\nPurpose: Tracks data on economic and social mobility to understand inequality trends in different regions.\nFunctions:\n\nStores data on income, education, and employment trends across communities.\nProvides comparison charts on social mobility rates based on location, gender, and ethnicity.\nRecommends policies and resources to improve access to opportunities.\n\nSimple Interface Design: An interactive heat map and dashboard display income trends, education levels, and employment data by region. Users can compare multiple areas, view bar charts, and access policy recommendations. The interface supports searching, visual comparisons, and report downloads.\n\n\n\n\nIf data can be retrieved efficiently and effectively, why is data mining needed?\nData mining is essential because it allows us to extract meaningful insights from large datasets beyond just retrieving stored data. Below are key reasons why data mining is needed:\n\n\n\n\nDatabases can retrieve data efficiently, but they don’t automatically find patterns or trends.\n\n\n\n\n\nData mining helps predict future trends based on historical data.\n\n\n\n\n\nBusinesses use data mining to make data-driven decisions rather than relying on intuition.\n\n\n\n\n\nBig data is too large and complex for traditional database queries alone. Data mining helps extract valuable insights from massive datasets.\n\n\n\n\n\nCompanies use data mining to gain a competitive edge by identifying market trends, customer preferences, and business risks."
  },
  {
    "objectID": "Assignment1.html#question-6",
    "href": "Assignment1.html#question-6",
    "title": "Assignment 1",
    "section": "",
    "text": "Describe at least three tables that might be used to store information in a social network/social media system such as Twitter or Reddit.\nA social media platform like Twitter or Reddit would require multiple tables for structured data management. Below are three essential tables:\n\n\n\nStores information about users.\n\n\n\nColumn Name\nDescription\n\n\n\n\nUser ID\nUnique identifier for each user\n\n\nUsername\nDisplay name of the user\n\n\nEmail\nContact email\n\n\nPassword\nEncrypted password\n\n\nDate Joined\nDate when the user registered\n\n\n\n\n\n\n\n\nStores tweets or Reddit posts.\n\n\n\nColumn Name\nDescription\n\n\n\n\nPost ID\nUnique identifier for each post\n\n\nUser ID\nID of the user who posted\n\n\nPost Content\nText or media content\n\n\nCreated At\nTimestamp of post creation\n\n\nLikes Count\nNumber of likes\n\n\nComments Count\nNumber of comments\n\n\n\n\n\n\n\n\nStores relationships between users.\n\n\n\nColumn Name\nDescription\n\n\n\n\nFollower ID\nID of the user who follows\n\n\nFollowed ID\nID of the user being followed\n\n\nDate Followed\nTimestamp of when the follow occurred"
  },
  {
    "objectID": "Assignment2.html",
    "href": "Assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Defines the structure of a relation, specifying its attributes and data types.\n\n\n\nA table containing rows that follow the schema.\n\n\n\nA specific snapshot of data in the relation at a given time.\n\n\n\n\n\n\n\n\n\n\nSchema\nRelation (Table)\nInstance\n\n\n\n\nStudent(ID, Name, Major, Year)\nA table storing student records\nA collection of student data at a specific moment"
  },
  {
    "objectID": "Assignment2.html#question-1-differences-between-relation-schema-relation-and-instance",
    "href": "Assignment2.html#question-1-differences-between-relation-schema-relation-and-instance",
    "title": "Assignment 2",
    "section": "",
    "text": "Defines the structure of a relation, specifying its attributes and data types.\n\n\n\nA table containing rows that follow the schema.\n\n\n\nA specific snapshot of data in the relation at a given time.\n\n\n\n\n\n\n\n\n\n\nSchema\nRelation (Table)\nInstance\n\n\n\n\nStudent(ID, Name, Major, Year)\nA table storing student records\nA collection of student data at a specific moment"
  },
  {
    "objectID": "Assignment2.html#question-2-schema-diagram-for-bank-database",
    "href": "Assignment2.html#question-2-schema-diagram-for-bank-database",
    "title": "Assignment 2",
    "section": "Question 2: Schema Diagram for Bank Database",
    "text": "Question 2: Schema Diagram for Bank Database\nA bank database schema includes the following entities:\n\nBRANCH Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nbranch_name\nName of the branch\n\n\nbranch_city\nLocation of branch\n\n\nassets\nTotal branch assets\n\n\n\n\n\nACCOUNT Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nbranch_name\nAssociated branch (🔗 FK → Branch)\n\n\naccount_number\nUnique account ID\n\n\nbalance\nAccount balance\n\n\n\n\n\nLOAN Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nloan_number\nUnique loan ID\n\n\nbranch_name\nIssuing branch (🔗 FK → Branch)\n\n\namount\nLoan amount\n\n\n\n\n\nDEPOSITOR Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nID\nCustomer ID (🔗 FK → Customer)\n\n\naccount_number\nLinked account number (🔗 FK → Account)\n\n\n\n\n\nBORROWER Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nID\nCustomer ID (🔗 FK → Customer)\n\n\nloan_number\nLinked loan number (🔗 FK → Loan)\n\n\n\n\n\nCUSTOMER Table\n\n\n\nColumn Name\nDescription\n\n\n\n\nID\nUnique customer ID\n\n\ncst_name\nCustomer’s name\n\n\ncst_street\nStreet address\n\n\ncst_city\nCity of residence\n\n\n\n\n\nRelationships\n\nOne Branch → Many Accounts (branch_name as FK in Account)\n\nOne Branch → Many Loans (branch_name as FK in Loan)\n\nOne Customer → Many Accounts (via Depositor table)\n\nOne Account → Many Customers (Joint accounts, via Depositor)\n\nOne Customer → Many Loans (via Borrower table)\n\nOne Loan → Many Customers (Co-borrowers, via Borrower)"
  },
  {
    "objectID": "Assignment2.html#question-3-primary-and-foreign-keys-for-the-bank-database",
    "href": "Assignment2.html#question-3-primary-and-foreign-keys-for-the-bank-database",
    "title": "Assignment 2",
    "section": "Question 3: Primary and Foreign Keys for the Bank Database",
    "text": "Question 3: Primary and Foreign Keys for the Bank Database\n\nPrimary Keys\n\nBranch: branch_name\nAccount: account_number\nLoan: loan_number\nDepositor: (ID, account_number)\nBorrower: (ID, loan_number)\nCustomer: ID\n\n\n\nForeign Keys\n\nAccount: branch_name\nLoan: branch_name\nDepositor: ID, account_number\nBorrower: ID, loan_number"
  },
  {
    "objectID": "Assignment5.html",
    "href": "Assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Question 1: E-R Diagram as a Graph\nAn E-R (Entity-Relationship) diagram can be viewed as a graph, where entities are nodes and relationships are edges. Understanding its structure helps ensure database integrity and effective design.\n\n\na) The graph is disconnected\nA disconnected graph means that some parts of the schema are isolated and do not connect to others. In terms of an enterprise schema, this implies that certain entities or relationships are not integrated with the rest of the database system.\nExample: Imagine we have a Doctor entity and a Patient entity, but no relationship (like Treats) connecting them. These two entities are disconnected in the E-R diagram.\nImplications: - Indicates missing relationships or a design flaw. - Can result in incomplete or difficult-to-query data. - Hurts the ability to enforce referential integrity.\n\n\nb) The graph has a cycle\nA cycle in an E-R diagram means that starting from one entity, you can follow a path of relationships and return to the same entity.\nExample: Doctor → Patient → Test → Doctor\nThis forms a loop (cycle) in the schema.\nImplications: - Not inherently wrong – cycles often represent real-world interconnectedness. - Can complicate queries, especially recursive ones. - May require careful normalization or handling to avoid data anomalies.\n\n\nQuestion 3: Why Do We Have Weak Entity Sets?\nWhile it’s possible to convert weak entity sets to strong ones by adding attributes, we still use weak entity sets in practice due to design clarity and efficiency.\nReasons We Use Weak Entity Sets:\n\nA weak entity cannot be uniquely identified by its own attributes.\nIt depends on a related strong entity for uniqueness.\n\nWhy Not Always Use Strong Entities?\n\nMaking weak entities strong can require adding redundant or artificial keys, which is not always natural.\nWeak entities simplify data modeling for real-world hierarchical relationships.\nHelps maintain referential integrity and logical clarity in the schema.\n\n\n\nQuestion 4\n\n\na: SQL Queries\nSchema:\nemployee(ID, name, street, city)\nworks(ID, company_id, salary)\ncompany(company_id, name, city)\nmanages(employee_id, manager_id)\n\nFind ID and name of employees who live in the same city as the location of their company:\n\nSELECT e.ID, e.name\nFROM employee AS e, works AS w, company AS c\nWHERE e.ID = w.ID AND w.company_id = c.company_id AND e.city = c.city;\n\nFind ID and name of employees who live on the same street and in the same city as their manager:\n\nSELECT e.ID, e.name\nFROM employee AS e, manages AS m, employee AS mngr\nWHERE e.ID = m.employee_id AND m.manager_id = mngr.ID\nAND e.city = mngr.city AND e.street = mngr.street;\n\nFind ID and name of employees who earn more than the average salary of all employees in their company:\n\nSELECT e.ID, e.name\nFROM employee AS e, works AS w\nWHERE e.ID = w.ID AND w.salary &gt; ( SELECT AVG(w2.salary)\nFROM works AS w2\nWHERE w2.company_id = w.company_id\n\n\nb: What’s wrong with this SQL query?\nSELECT name, title\nFROM instructor NATURAL JOIN teaches NATURAL JOIN section NATURAL JOIN course\nWHERE semester = ‘Spring’ AND year = 2017;\nProblem: This query uses NATURAL JOIN, which joins tables based on all columns with the same name. This can cause unintended matches, especially if tables have generic column names like ID or course_id.\nFix: Use explicit JOINs to clearly specify matching fields.\nCorrected Query:\nSELECT i.name, c.title\nFROM instructor AS i\nJOIN teaches AS t ON i.ID = t.ID\nJOIN section AS s ON t.course_id = s.course_id\nAND t.sec_id = s.sec_id\nAND t.semester = s.semester\nAND t.year = s.year JOIN course AS c ON s.course_id = c.course_id\nWHERE s.semester = ‘Spring’ AND s.year = 2017;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I am Eman Ajmal",
    "section": "",
    "text": "I’m a social data analyst passionate about turning real-world problems into actionable insights.\nI care deeply about development, equity, and evidence-based reform, and I’m driven by the belief that small interventions, rooted in data can spark long-term impact.\nCurrently pursuing my Master’s at UT Dallas, I focus on the intersection of data, people, and policy, and aim to build systems that inform, include, and improve.\n\n\n\n“Data is more than numbers — it’s people, stories, and change waiting to happen.”\n\n\n\nConnect with me"
  },
  {
    "objectID": "Project.html",
    "href": "Project.html",
    "title": "Project",
    "section": "",
    "text": "Eman Ajmal is a social analyst.\n\nplot(mtcars)"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "Richardson, Texas 75080\nPortfolio"
  },
  {
    "objectID": "CV.html#education",
    "href": "CV.html#education",
    "title": "CV",
    "section": "",
    "text": "University of Texas at Dallas — MS, Social Data Analytics and Research\nGPA: 3.89 | Expected December 2026\nForman Christian College, Lahore, Pakistan — BS, Sociology (Minor in Education)\nGPA: 3.47 | Graduated January 2024\nIstanbul Aydun University, Istanbul, Turkey — Digital Marketing, Delightful Summer Programme\nAugust 2022"
  },
  {
    "objectID": "CV.html#skills",
    "href": "CV.html#skills",
    "title": "CV",
    "section": "",
    "text": "Research & Data Analysis: Quantitative & qualitative research, survey design, SPSS, R Studio, SQL, data visualization, reporting\nMarketing Tools: Canva, Meta Ads, DV360, Google Ads, Keyword Analysis, Sprinklr\nTechnical: Microsoft Office, Advanced Excel, SketchUp, Slack, Discord, Zoom\nSoft Skills: Creativity, Adaptability, Communication, Problem Solving, Critical Thinking"
  },
  {
    "objectID": "CV.html#professional-experience",
    "href": "CV.html#professional-experience",
    "title": "CV",
    "section": "",
    "text": "Policy Research Analyst\nMay 2025 – Present | Richardson, TX\n- Analyze socio-economic datasets.\n- Draft policy indicators and summaries using insights from state and nonprofit sources.\n- Collaborate with faculty and research staff to refine data visualizations and narratives for public reporting.\n\n\n\nLead Social Listening\nApril 2024 – September 2024 | Lahore, Pakistan\n- Led sentiment/Regression analysis using Sprinklr & SPSS to uncover key trends and improve brand perception by 15%.\n- Streamlined the content review and feedback process by integrating real-time data, enhancing stakeholder collaboration and increasing operational efficiency.\n\n\n\nResearch & Development Intern\nOctober 2023 – December 2023 | Lahore, Pakistan\n- Conducted qualitative and quantitative research for the Gender Parity Report 2022, analyzing large-scale datasets to identify trends in gender equity across Punjab.\n- Collaborated with cross-functional teams to develop evidence-based policy recommendations aimed at improving gender parity and social outcomes."
  },
  {
    "objectID": "CV.html#academic-project-experience",
    "href": "CV.html#academic-project-experience",
    "title": "CV",
    "section": "",
    "text": "Thesis – Legal Awareness Among Female University Students in Lahore\nSeptember 2023 – January 2024\n- Led a comprehensive research study on legal knowledge among female students by surveying over 300 participants, uncovering critical gaps in legal literacy and identifying key opportunities for academic empowerment.\n- Developed and presented data-driven recommendations to academic stakeholders, driving a 25% increase in student engagement with legal literacy initiatives and demonstrating impactful leadership in addressing contemporary societal challenges."
  },
  {
    "objectID": "CV.html#publications-research",
    "href": "CV.html#publications-research",
    "title": "CV",
    "section": "",
    "text": "The Efficacy of Battering Intervention and Prevention Programs\nInstitute for Urban Policy Research (UTD), Texas Council on Family Violence — 2025\n- Contributed as Research Assistant on a statewide evaluation examining the impact of Battering Intervention and Prevention Programs (BIPPs) on reducing domestic violence recidivism across Texas. Analyzed program data and outcomes for over 1,600 participants, supporting evidence-based policy recommendations."
  },
  {
    "objectID": "CV.html#leadership-experience",
    "href": "CV.html#leadership-experience",
    "title": "CV",
    "section": "",
    "text": "International Center Global Ambassador (UTD) — Vice President\n\nArt Junction (FCCU) — Director PR\n\nAghaaz (NGO) — Executive Council"
  },
  {
    "objectID": "CV.html#keywords",
    "href": "CV.html#keywords",
    "title": "CV",
    "section": "",
    "text": "R Studio, SQL, SPSS, Sprinklr, Data Visualization, Policy Research, Public Reporting, Stakeholder Engagement"
  },
  {
    "objectID": "CV.html#institute-for-urban-policy-research-utd",
    "href": "CV.html#institute-for-urban-policy-research-utd",
    "title": "CV",
    "section": "Institute for Urban Policy Research (UTD)",
    "text": "Institute for Urban Policy Research (UTD)\nPolicy Research Analyst\nMay 2025 – Present | Richardson, TX\n- Analyze socio-economic datasets.\n- Draft policy indicators and summaries using insights from state and nonprofit sources.\n- Collaborate with faculty and research staff to refine data visualizations and narratives for public reporting."
  },
  {
    "objectID": "CV.html#nestlé-pakistan",
    "href": "CV.html#nestlé-pakistan",
    "title": "CV",
    "section": "Nestlé Pakistan",
    "text": "Nestlé Pakistan\nLead Social Listening\nApril 2024 – September 2024 | Lahore, Pakistan\n- Led sentiment/Regression analysis using Sprinklr & SPSS to uncover key trends and improve brand perception by 15%.\n- Streamlined the content review and feedback process by integrating real-time data, enhancing stakeholder collaboration and increasing operational efficiency."
  },
  {
    "objectID": "CV.html#punjab-commission-on-the-status-of-women",
    "href": "CV.html#punjab-commission-on-the-status-of-women",
    "title": "CV",
    "section": "Punjab Commission on the Status of Women",
    "text": "Punjab Commission on the Status of Women\nResearch & Development Intern\nOctober 2023 – December 2023 | Lahore, Pakistan\n- Conducted qualitative and quantitative research for the Gender Parity Report 2022, analyzing large-scale datasets to identify trends in gender equity across Punjab.\n- Collaborated with cross-functional teams to develop evidence-based policy recommendations aimed at improving gender parity and social outcomes."
  },
  {
    "objectID": "EPPS6302_Assignment1.html",
    "href": "EPPS6302_Assignment1.html",
    "title": "Part 4 — Write a one page note on designing your website",
    "section": "",
    "text": "I created my website using Quarto and GitHub Pages to share my assignments, CV, and projects. The layout is simple and professional, with a top navigation bar for easy access to pages like Home, CV, and course assignments. A right-side “On this page” panel helps visitors jump to sections quickly.\nThe site uses the Cosmo theme for clean styling and responsive design. Content is written in Markdown within .qmd files, making it easy to update and maintain. Images and files are stored in organized folders and linked using relative paths. I render the site locally and push updates to GitHub, where it’s automatically deployed.\nMy goal was to build a clear, functional, and easy-to-navigate portfolio that highlights my work while remaining simple to maintain and expand in the future."
  },
  {
    "objectID": "EPPS6302_Assignment1.html#episode-overview",
    "href": "EPPS6302_Assignment1.html#episode-overview",
    "title": "Assignment 1",
    "section": "Episode Overview",
    "text": "Episode Overview\nIn this episode of Data Framed, hosts Adel Nehme and Richie Cotton welcome Karen Ng, Head of Product at HubSpot, to discuss the evolving landscape of team dynamics where humans and AI agents collaborate side by side. The conversation centers on practical strategies for integrating AI agents into workflows and optimizing team performance within this hybrid environment."
  },
  {
    "objectID": "EPPS6302_Assignment1.html#main-theme",
    "href": "EPPS6302_Assignment1.html#main-theme",
    "title": "Assignment 1",
    "section": "Main Theme",
    "text": "Main Theme\nThe central theme of the episode is the rise of human-plus-agent hybrid teams work environments in which AI systems share responsibilities with human workers. Karen Ng breaks down how organizations can effectively blend AI capabilities with human oversight, ensuring both productivity and ethical integrity."
  },
  {
    "objectID": "EPPS6302_Assignment1.html#key-takeaways-insights",
    "href": "EPPS6302_Assignment1.html#key-takeaways-insights",
    "title": "Assignment 1",
    "section": "Key Takeaways & Insights",
    "text": "Key Takeaways & Insights\n\nIntegration Strategy\nHybrid teams succeed when roles are clearly defined, and AI systems are introduced where they augment not replace human judgment.\n\n\nTraining & Evaluation\nEstablishing best practices for training AI agents includes designing them for continuous learning and setting evaluation criteria aligned with team goals.\n\n\nData Quality & Governance\nA robust governance framework is crucial. AI agents must operate on accurate, reliable data while maintaining transparency and ethical standards.\n\n\nPractical Use Cases\nIndustries like customer support, sales, and data processing have seen measurable benefits by integrating AI agents to handle recurring tasks freeing humans to focus on complex, strategic work."
  },
  {
    "objectID": "EPPS6302_Assignment1.html#personal-reflection",
    "href": "EPPS6302_Assignment1.html#personal-reflection",
    "title": "Assignment 1",
    "section": "Personal Reflection",
    "text": "Personal Reflection\nThis episode offers a forward-looking lens into how AI is reshaping team workflows. Karen Ng’s insights are grounded in real-world applications; her role at HubSpot lends credibility and depth to the discussion. I appreciated the balanced approach highlighting AI’s strengths while acknowledging the importance of human judgment, governance, and alignment with ethical standards.\nI find the emphasis on governance particularly impactful. Too often, the conversation around AI overlooks the importance of data integrity and trust. This episode prompts listeners to think critically about implementing AI in a way that enhances not erodes team coherence and accountability."
  },
  {
    "objectID": "EPPS6302_Assignment1.html#conclusion",
    "href": "EPPS6302_Assignment1.html#conclusion",
    "title": "Assignment 1",
    "section": "Conclusion",
    "text": "Conclusion\nEpisode 319 of Data Framed delivers a timely and practical exploration of hybrid human-AI teamwork. It’s especially valuable for product managers, data team leaders, and anyone working at the intersection of AI and organizational design. Whether you’re exploring AI adoption or refining existing deployments, this episode offers a thoughtful roadmap for doing so responsibly and effectively."
  },
  {
    "objectID": "GTrends.html",
    "href": "GTrends.html",
    "title": "GTrends",
    "section": "",
    "text": "```## EPPS 6302 Methods of Data Collection and Production ## Google Trends with R\ninstall.packages(“gtrendsR”) library(gtrendsR) TrumpHarrisElection = gtrends(c(“Trump”,“Harris”,“election”), onlyInterest = TRUE, geo = “US”, gprop = “web”, time = “today+5-y”, category = 0, ) # last five years the_df=TrumpHarrisElection$interest_over_time plot(TrumpHarrisElection) tg = gtrends(“tariff”, time = “all”)\n\nExample: Tariff, China military, Taiwan\n\n\n\nplot(gtrends(c(“tariff”), time = “all”)) data(“countries”) plot(gtrends(c(“tariff”), geo = “GB”, time = “all”)) plot(gtrends(c(“tariff”), geo = c(“US”,“GB”,“TW”), time = “all”)) tg_iot = tg\\(interest_over_time\ntct = gtrends(c(\"tariff\",\"China military\", \"Taiwan\"), time = \"all\")\ntct = data.frame(tct\\)interest_over_time) plot(gtrends(c(“tariff”,“China military”, “Taiwan”), time = “all”))"
  },
  {
    "objectID": "EPPS6302_Assignment2.html",
    "href": "EPPS6302_Assignment2.html",
    "title": "Overview",
    "section": "",
    "text": "#Assignment 2"
  },
  {
    "objectID": "EPPS6302_Assignment2.html#google-trends-website-export",
    "href": "EPPS6302_Assignment2.html#google-trends-website-export",
    "title": "Assignment 2",
    "section": "1) Google Trends website export",
    "text": "1) Google Trends website export"
  },
  {
    "objectID": "EPPS6302_Assignment2.html#r-gtrendsr-plot",
    "href": "EPPS6302_Assignment2.html#r-gtrendsr-plot",
    "title": "Assignment 2",
    "section": "2) R (gtrendsR) plot",
    "text": "2) R (gtrendsR) plot"
  },
  {
    "objectID": "EPPS6302_Assignment3.html",
    "href": "EPPS6302_Assignment3.html",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Source: U.S. Census Bureau, ACS 2023 5-year via tidycensus."
  },
  {
    "objectID": "EPPS6302_Assignment3.html#map",
    "href": "EPPS6302_Assignment3.html#map",
    "title": "Assignment 3: Mapping Census Data",
    "section": "",
    "text": "Source: U.S. Census Bureau, ACS 2023 5-year via tidycensus."
  },
  {
    "objectID": "EPPS6356_Assignment1.html",
    "href": "EPPS6356_Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "The classic Anscombe’s quartet consists of four (x, y) datasets that share the same means, variances, correlations, and regression coefficients — yet their scatterplots are visually very different. Below is my reproduction of the four datasets with their common least-squares line.\n\n\n\n\nAnscombe’s quartet demonstrates that datasets with identical statistical summaries can tell very different stories when visualized. While all four share the same means, variances, correlations, and regression lines, their scatterplots reveal distinct problems: a roughly linear set, a curved relationship, a single extreme outlier, and a high-leverage point that determines the slope. Without visualization, these issues remain hidden and lead to incorrect conclusions. To address this, start with graphs (scatterplots plus a smooth), then check residuals, leverage, and Cook’s distance to test assumptions. When assumptions are violated, use robust regression, apply transformations, add nonlinear terms (e.g., a quadratic), or collect additional data to improve coverage across (x). This workflow ensures models reflect the true pattern rather than just matching aggregates."
  },
  {
    "objectID": "EPPS6356_Assignment1.html#anscombe-1973-analysis-solutions",
    "href": "EPPS6356_Assignment1.html#anscombe-1973-analysis-solutions",
    "title": "Assignment 1",
    "section": "",
    "text": "The classic Anscombe’s quartet consists of four (x, y) datasets that share the same means, variances, correlations, and regression coefficients — yet their scatterplots are visually very different. Below is my reproduction of the four datasets with their common least-squares line.\n\n\n\n\nAnscombe’s quartet demonstrates that datasets with identical statistical summaries can tell very different stories when visualized. While all four share the same means, variances, correlations, and regression lines, their scatterplots reveal distinct problems: a roughly linear set, a curved relationship, a single extreme outlier, and a high-leverage point that determines the slope. Without visualization, these issues remain hidden and lead to incorrect conclusions. To address this, start with graphs (scatterplots plus a smooth), then check residuals, leverage, and Cook’s distance to test assumptions. When assumptions are violated, use robust regression, apply transformations, add nonlinear terms (e.g., a quadratic), or collect additional data to improve coverage across (x). This workflow ensures models reflect the true pattern rather than just matching aggregates."
  },
  {
    "objectID": "EPPS6356_Assignment1.html#seasonal-plot-custom-winter-palette",
    "href": "EPPS6356_Assignment1.html#seasonal-plot-custom-winter-palette",
    "title": "Assignment 1",
    "section": "2) Seasonal Plot — Custom “Winter” Palette",
    "text": "2) Seasonal Plot — Custom “Winter” Palette\nI re-themed the plot by replacing the original fall color with a cool-tone winter palette."
  },
  {
    "objectID": "EPPS6356_Assignment1.html#critique-of-a-published-chart",
    "href": "EPPS6356_Assignment1.html#critique-of-a-published-chart",
    "title": "Assignment 1",
    "section": "3) Critique of a Published Chart",
    "text": "3) Critique of a Published Chart\nSource: The New York Times — “COVID-19 in the U.S.: Latest Map and Case Counts”\nhttps://www.nytimes.com/interactive/2021/us/covid-cases.html\nClaim & audience. The interactive aims to show how COVID-19 cases evolve across U.S. geographies over time, serving a general audience tracking regional surges and declines.\nWhat works. - The map + time-series pairing supports both geographic and temporal exploration. - Clean styling minimizes clutter; hover interactions surface detail on demand.\nIssues:\n\nColor encoding/contrast. Mid-range shades can be hard to distinguish on some displays, reducing perceptual separation between moderate and high values.\n\nAxis/baseline clarity. In some views the y-axis does not start at zero; if truncation isn’t explicitly noted, fluctuations can appear exaggerated.\n\nContext/annotation. Major events (e.g., vaccine rollout, variant waves) are not annotated on timelines, forcing readers to infer causes of spikes.\n\nLegend proximity & cognitive load. Legends/scales can sit far from the immediate focus, increasing eye travel during comparison.\n\nAccessibility. Palette choices may challenge color-vision deficiencies; thin strokes lower legibility on mobile.\n\nStatic use case. Exporting a static image removes tooltips/hover states, so meaning can be lost without added labels.\n\nImprovements:\n\nAdopt a colorblind-safe, high-contrast sequential palette (e.g., viridis) to better separate mid/high values.\n\nStart y-axes at zero where feasible, or clearly mark breaks when truncation is necessary.\n\nAdd lightweight annotations at key dates (eligibility expansions, variant peaks) to anchor interpretation.\n\nMove legend/scale closer to the plot area or directly label lines/peaks to reduce eye travel.\n\nUse slightly thicker strokes and on-chart labels to retain meaning in static exports."
  },
  {
    "objectID": "EPPS6356_Assignment3.html",
    "href": "EPPS6356_Assignment3.html",
    "title": "Assignment 3 — Base R and ggplot2",
    "section": "",
    "text": "The six classic base R plots were recreated and displayed together in a 3×2 layout.\nThe figure below includes:\n- A manually constructed scatterplot (“Bird 131”) with customized axes and dual labels.\n- A histogram showing normally distributed data with a superimposed normal curve.\n- A stacked bar chart for the VADeaths dataset with values labeled within each segment.\n- Boxplots comparing ToothGrowth length by supplement type (OJ vs VC) and dose.\n- A 3D perspective surface generated using the persp() function.\n- A pie chart illustrating category shares using a gray tone color ramp.\n\n\n\nI chose the ToothGrowth boxplot. It compares tooth length across Vitamin C doses (0.5, 1, 2 mg) and supplement type (OJ vs VC).\nEach box shows the median (center line) and IQR (box height), with whiskers extending to data within 1.5×IQR; any points beyond would be outliers.\nFrom the figure, tooth length increases with dose, and at lower doses (0.5, 1 mg), OJ often shows higher medians than VC; by 2 mg, the two supplements are similar, indicating a diminishing difference at the highest dose."
  },
  {
    "objectID": "EPPS6356_Assignment3.html#a-explain-one-chart-boxplot-toothgrowth",
    "href": "EPPS6356_Assignment3.html#a-explain-one-chart-boxplot-toothgrowth",
    "title": "Assignment 3 — Base R and ggplot2",
    "section": "",
    "text": "I chose the ToothGrowth boxplot. It compares tooth length across Vitamin C doses (0.5, 1, 2 mg) and supplement type (OJ vs VC).\nEach box shows the median (center line) and IQR (box height), with whiskers extending to data within 1.5×IQR; any points beyond would be outliers.\nFrom the figure, tooth length increases with dose, and at lower doses (0.5, 1 mg), OJ often shows higher medians than VC; by 2 mg, the two supplements are similar, indicating a diminishing difference at the highest dose."
  },
  {
    "objectID": "EPPS6356_Assignment3.html#a-compare-the-regression-models",
    "href": "EPPS6356_Assignment3.html#a-compare-the-regression-models",
    "title": "Assignment 3 — Base R and ggplot2",
    "section": "2(a) Compare the regression models",
    "text": "2(a) Compare the regression models\nAll four datasets in Anscombe’s Quartet produce nearly identical regression statistics — similar slopes, intercepts, and (R^2) values (approximately 0.67).\nHowever, their visual patterns are completely different.\nSet 1 represents a true linear relationship, Set 2 has a curved (nonlinear) trend, Set 3 contains a strong outlier that influences the regression, and Set 4 includes one high-leverage point that distorts the fitted line.\nThis demonstrates that numerical summaries alone can be misleading and highlights the importance of data visualization."
  },
  {
    "objectID": "EPPS6356_Assignment3.html#b-compare-different-plotting-styles",
    "href": "EPPS6356_Assignment3.html#b-compare-different-plotting-styles",
    "title": "Assignment 3 — Base R and ggplot2",
    "section": "2(b) Compare different plotting styles",
    "text": "2(b) Compare different plotting styles\nThe first figure shows the default base plots for the four Anscombe datasets, while the second uses customized colors, symbols, and line types for clearer differentiation and readability.\n\n\nComparison paragraph:\nThe styled version communicates the differences between datasets much more effectively.\nIn Set 1, the linear trend is clear; in Set 2, curvature is visible; in Set 3, an outlier becomes obvious; and in Set 4, one leverage point dominates.\nThis comparison shows that thoughtful visual design — through colors, line types, and shapes — enhances interpretability and brings out patterns hidden in plain summaries."
  },
  {
    "objectID": "EPPS6356_Assignment2.html",
    "href": "EPPS6356_Assignment2.html",
    "title": "Assignment 2 — Base R Graphics (Murrell)",
    "section": "",
    "text": "Run Paul Murrell’s base R graphics programs (murrell01.R) line by line. Note the changes as low-level functions are added.\n\n\n\n# Start plotting from basics\nplot(pressure, pch = 16,\n     xlab = \"Temperature (Celsius)\",\n     ylab = \"Pressure (mm Hg)\",\n     main = \"Pressure vs Temperature\")\ntext(150, 600, \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\nplot(pressure, pch = 5,\n     xlab = \"Temperature (Celsius)\",\n     ylab = \"Pressure (mm Hg)\",\n     main = \"Pressure vs Temperature (pch=5)\")\ntext(150, 600, \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n\n\n# Window + axes + layers\npar(mfrow = c(1,1), las = 1, mar = c(4,4,2,4), cex = 0.9)\n\nx  &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, 0.8, 0.5, 0.45, 0.4, 0.3)\n\nplot.new()\nplot.window(xlim = range(as.numeric(x), na.rm = TRUE), ylim = c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch = 16, cex = 1.0)\npoints(x, y2, pch = 1, cex = 1.0)\naxis(1, at = seq(0, 16, 4))\naxis(2, at = seq(0, 6, 2))\naxis(4, at = seq(0, 6, 2))\nbox(bty = \"u\")\nmtext(\"Travel Time (s)\", side = 1, line = 2)\nmtext(\"Responses per Travel\", side = 2, line = 2)\nmtext(\"Responses per Second\", side = 4, line = 2)\ntext(4, 5, \"Bird 131\")\n\n\n\n\n\n\n\n\n\n# Histogram\nset.seed(1)\nY &lt;- rnorm(50)\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA\nxg &lt;- seq(-3.5, 3.5, 0.1)\nhist(Y, breaks = seq(-3.5, 3.5), col = \"gray80\", freq = FALSE,\n     main = \"Normal sample\", xlab = \"Z\", ylab = \"Density\")\nlines(xg, dnorm(xg), lwd = 2)\n\n\n\n\n\n\n\n\n\n# Barplot\npar(mar = c(2, 3.1, 2, 2.1))\nmidpts &lt;- barplot(VADeaths,\n                  col = gray(0.1 + seq(1,9,2)/11),\n                  names = rep(\"\", 4),\n                  ylab = \"Death rate (per 1000)\",\n                  main = \"VADeaths\")\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at = midpts, side = 1, line = 0.5, cex = 0.7)\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# Boxplot\npar(mar = c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset = supp == \"VC\", col = \"white\",\n        xlab = \"\", ylab = \"tooth length\", ylim = c(0, 35),\n        main = \"ToothGrowth by dose & supplement\")\nmtext(\"Vitamin C dose (mg)\", side = 1, line = 2.5, cex = 0.9)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        subset = supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"),\n       fill = c(\"white\",\"gray\"), bty = \"n\")\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# persp() demo surface\nx &lt;- seq(-10, 10, length = 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2 + y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f); z[is.na(z)] &lt;- 1\npar(mar = c(2, 2.5, 3, 2))\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5,\n      ticktype = \"detailed\", xlab = \"X\", ylab = \"Y\", zlab = \"Z\",\n      main = \"persp(): demo surface\")\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# Pie chart\npar(mar = c(0, 2, 1, 2), xpd = FALSE, cex = 0.8)\npie.sales &lt;- c(0.12, 0.30, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\",\"Cherry\",\"Apple\",\"Boston Cream\",\"Other\",\"Vanilla\")\npie(pie.sales, col = gray(seq(0.3, 1.0, length = 6)),\n    main = \"Pie chart demo\")"
  },
  {
    "objectID": "EPPS6356_Assignment2.html#original",
    "href": "EPPS6356_Assignment2.html#original",
    "title": "Assignment 2 — Base R Graphics (Murrell)",
    "section": "",
    "text": "# Start plotting from basics\nplot(pressure, pch = 16,\n     xlab = \"Temperature (Celsius)\",\n     ylab = \"Pressure (mm Hg)\",\n     main = \"Pressure vs Temperature\")\ntext(150, 600, \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\nplot(pressure, pch = 5,\n     xlab = \"Temperature (Celsius)\",\n     ylab = \"Pressure (mm Hg)\",\n     main = \"Pressure vs Temperature (pch=5)\")\ntext(150, 600, \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n\n\n# Window + axes + layers\npar(mfrow = c(1,1), las = 1, mar = c(4,4,2,4), cex = 0.9)\n\nx  &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, 0.8, 0.5, 0.45, 0.4, 0.3)\n\nplot.new()\nplot.window(xlim = range(as.numeric(x), na.rm = TRUE), ylim = c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch = 16, cex = 1.0)\npoints(x, y2, pch = 1, cex = 1.0)\naxis(1, at = seq(0, 16, 4))\naxis(2, at = seq(0, 6, 2))\naxis(4, at = seq(0, 6, 2))\nbox(bty = \"u\")\nmtext(\"Travel Time (s)\", side = 1, line = 2)\nmtext(\"Responses per Travel\", side = 2, line = 2)\nmtext(\"Responses per Second\", side = 4, line = 2)\ntext(4, 5, \"Bird 131\")\n\n\n\n\n\n\n\n\n\n# Histogram\nset.seed(1)\nY &lt;- rnorm(50)\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA\nxg &lt;- seq(-3.5, 3.5, 0.1)\nhist(Y, breaks = seq(-3.5, 3.5), col = \"gray80\", freq = FALSE,\n     main = \"Normal sample\", xlab = \"Z\", ylab = \"Density\")\nlines(xg, dnorm(xg), lwd = 2)\n\n\n\n\n\n\n\n\n\n# Barplot\npar(mar = c(2, 3.1, 2, 2.1))\nmidpts &lt;- barplot(VADeaths,\n                  col = gray(0.1 + seq(1,9,2)/11),\n                  names = rep(\"\", 4),\n                  ylab = \"Death rate (per 1000)\",\n                  main = \"VADeaths\")\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at = midpts, side = 1, line = 0.5, cex = 0.7)\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# Boxplot\npar(mar = c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset = supp == \"VC\", col = \"white\",\n        xlab = \"\", ylab = \"tooth length\", ylim = c(0, 35),\n        main = \"ToothGrowth by dose & supplement\")\nmtext(\"Vitamin C dose (mg)\", side = 1, line = 2.5, cex = 0.9)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        subset = supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"),\n       fill = c(\"white\",\"gray\"), bty = \"n\")\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# persp() demo surface\nx &lt;- seq(-10, 10, length = 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2 + y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f); z[is.na(z)] &lt;- 1\npar(mar = c(2, 2.5, 3, 2))\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5,\n      ticktype = \"detailed\", xlab = \"X\", ylab = \"Y\", zlab = \"Z\",\n      main = \"persp(): demo surface\")\n\n\n\n\n\n\n\npar(mar = c(5.1, 4.1, 4.1, 2.1))\n\n\n# Pie chart\npar(mar = c(0, 2, 1, 2), xpd = FALSE, cex = 0.8)\npie.sales &lt;- c(0.12, 0.30, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\",\"Cherry\",\"Apple\",\"Boston Cream\",\"Other\",\"Vanilla\")\npie(pie.sales, col = gray(seq(0.3, 1.0, length = 6)),\n    main = \"Pie chart demo\")"
  },
  {
    "objectID": "EPPS6356_Assignment2.html#base-r-functions-used",
    "href": "EPPS6356_Assignment2.html#base-r-functions-used",
    "title": "Assignment 2 — Base R Graphics (Murrell)",
    "section": "2.1 Base R Functions Used",
    "text": "2.1 Base R Functions Used\n\npar() – Sets or queries graphical parameters such as margins, axis style, and layout.\n\nlines() – Adds connected line segments to an existing plot.\n\npoints() – Adds individual points (symbols) to an existing plot.\n\naxis() – Draws tick marks and axis labels on plots.\n\nbox() – Adds a box (border) around the current plotting region.\n\ntext() – Writes text inside the plotting area at specified coordinates.\n\nmtext() – Adds text in the margins (outer areas) of the plot.\n\nhist() – Creates histograms for frequency distributions.\n\nboxplot() – Displays data distributions by quartiles.\n\nlegend() – Adds legends identifying plotted elements.\n\npersp() – Produces 3-D perspective surface plots.\n\nnames() – Retrieves or sets the names of variables in a data frame.\n\npie() – Creates pie-chart visualizations."
  },
  {
    "objectID": "EPPS6356_Assignment2.html#plots-and-results",
    "href": "EPPS6356_Assignment2.html#plots-and-results",
    "title": "Assignment 2 — Base R Graphics (Murrell)",
    "section": "2.2 Plots and Results",
    "text": "2.2 Plots and Results\n\nMake sure the file HPI_data.csv is in the same folder as this .qmd.\n\n\n# Read and normalize column names from the HPI CSV. This maps your Excel-like\n# headers to the short names used below, and ensures numeric types.\nHPI_raw &lt;- read.csv(\"HPI_data.csv\", stringsAsFactors = FALSE, check.names = TRUE)\n\npick &lt;- function(df, ...) {\n  opts &lt;- c(...)\n  hit &lt;- opts[opts %in% names(df)]\n  if (!length(hit)) stop(\"Missing expected columns: \", paste(opts, collapse=\" | \"))\n  df[[hit[1]]]\n}\n\nHPI &lt;- data.frame(\n  GDP_capita = suppressWarnings(as.numeric(pick(HPI_raw, \"GDP_capita\", \"GDP.per.capita....\"))),\n  LifeExp    = suppressWarnings(as.numeric(pick(HPI_raw, \"LifeExp\", \"Life.Expectancy..years.\"))),\n  Wellbeing  = suppressWarnings(as.numeric(pick(HPI_raw, \"Wellbeing\", \"Ladder.of.life..Wellbeing...0.10.\"))),\n  Carbon     = suppressWarnings(as.numeric(pick(HPI_raw, \"Carbon\", \"Carbon.Footprint..tCO2e.\"))),\n  HPI        = suppressWarnings(as.numeric(pick(HPI_raw, \"HPI\"))),\n  Population = suppressWarnings(as.numeric(pick(HPI_raw, \"Population\", \"Population..thousands.\"))),\n  Continent  = pick(HPI_raw, \"Continent\")\n)\n\n\n# Scatter with regression line — small, solid points\nscat &lt;- subset(HPI, is.finite(LifeExp) & is.finite(Wellbeing))\npar(las = 1, mar = c(4,4,2,1), cex = 0.9)\nplot(scat$LifeExp, scat$Wellbeing, pch = 16, cex = 1.0, col = \"steelblue\",\n     xlab = \"Life Expectancy (years)\", ylab = \"Wellbeing (0–10)\",\n     main = \"Life Expectancy vs Wellbeing (HPI)\")\nabline(lm(Wellbeing ~ LifeExp, data = scat), lwd = 2)\nbox()\n\n\n\n\n\n\n\n\n\n# Custom axes & labels — same point size as above\nplot(scat$LifeExp, scat$Wellbeing, axes = FALSE, pch = 16, cex = 1.0, col = \"steelblue\",\n     xlab = \"\", ylab = \"\", main = \"Custom axes & labels (HPI)\")\naxis(1); axis(2); box()\ntext(x = mean(scat$LifeExp, na.rm = TRUE),\n     y = max(scat$Wellbeing, na.rm = TRUE),\n     labels = \"Positive association\", pos = 3)\nmtext(\"Life Expectancy (Years)\", side = 1, line = 3)\nmtext(\"Wellbeing (0–10)\",      side = 2, line = 3)\n\n\n\n\n\n\n\n\n\n# Histogram + density\nwb &lt;- scat$Wellbeing[is.finite(scat$Wellbeing)]\nhist(wb, breaks = 6, col = \"gray80\",\n     main = \"Distribution of Wellbeing (HPI)\",\n     xlab = \"Wellbeing (0–10)\", ylab = \"Density\")\nif (length(unique(wb)) &gt; 1) lines(density(wb), lwd = 2)\n\n\n\n\n\n\n\n\n\n# Boxplot by continent\nbp &lt;- subset(HPI, is.finite(HPI) & !is.na(Continent))\nboxplot(HPI ~ as.factor(Continent), data = bp, col = \"white\",\n        ylab = \"Happy Planet Index (HPI)\", xlab = \"Continent\",\n        main = \"HPI by Continent\")\nmtext(\"Grouped distribution across regions\", side = 3, line = 0.5, cex = 0.8)\n\n\n\n\n\n\n\n\n\n# GDP vs Life Expectancy — solid points\ngdp &lt;- subset(HPI, is.finite(GDP_capita) & is.finite(LifeExp))\nplot(gdp$GDP_capita, gdp$LifeExp, pch = 16, cex = 1.0, col = \"tomato\",\n     xlab = \"GDP per Capita (USD)\", ylab = \"Life Expectancy (years)\",\n     main = \"GDP vs Life Expectancy (HPI)\")\nlegend(\"bottomright\", legend = \"Country points\", pch = 16, col = \"tomato\", bty = \"n\")\n\n\n\n\n\n\n\n\n\n# persp(): demo surface (with clear axes & ticks)\nx &lt;- seq(-10, 10, length = 30); y &lt;- x\nf &lt;- function(x,y) {\n  r &lt;- sqrt(x^2 + y^2)\n  out &lt;- 10 * sin(r) / r\n  out[!is.finite(out)] &lt;- 10\n  out\n}\nz &lt;- outer(x, y, f)\npar(mar = c(2, 2.5, 3, 2))\npersp(x, y, z, theta = 30, phi = 30, expand = 0.5,\n      ticktype = \"detailed\",\n      xlab = \"X\", ylab = \"Y\", zlab = \"Z\",\n      col = \"lightblue\", border = \"gray30\",\n      main = \"persp(): demo surface\")\n\n\n\n\n\n\n\n\n\n# OPTIONAL: Interpolated HPI surface using akima (only if enough data)\nif (!requireNamespace(\"akima\", quietly = TRUE)) {\n  install.packages(\"akima\")\n}\nif (requireNamespace(\"akima\", quietly = TRUE)) {\n  library(akima)\n  df &lt;- subset(HPI, is.finite(GDP_capita) & is.finite(Carbon) & is.finite(HPI))\n  if (nrow(df) &gt; 10 && length(unique(df$GDP_capita)) &gt; 3 && length(unique(df$Carbon)) &gt; 3) {\n    interp_result &lt;- with(df, akima::interp(x = GDP_capita, y = Carbon, z = HPI, duplicate = \"mean\"))\n    par(mar = c(2, 2.5, 3, 2))\n    persp(interp_result$x, interp_result$y, interp_result$z,\n          theta = 30, phi = 30, expand = 0.5,\n          ticktype = \"detailed\",\n          xlab = \"GDP per Capita\", ylab = \"Carbon (tCO2e/capita)\", zlab = \"HPI\",\n          col = \"lightblue\", border = \"gray30\",\n          main = \"HPI Surface: GDP vs Carbon (interpolated)\")\n  } else {\n    message(\"Not enough distinct points for interpolation; skipping HPI surface.\")\n  }\n}"
  },
  {
    "objectID": "PrepareforClass2.html",
    "href": "PrepareforClass2.html",
    "title": "Prepare for Class 2",
    "section": "",
    "text": "“Teaching to See” is a 37-minute primer on visual literacy and studio discipline. Inge Druckrey, drawing on the Basel School tradition, shows how designers learn to see—not merely to look at objects, but to perceive relationships: proportion, rhythm, weight, contrast, spacing, and alignment. The film’s early exercises are intentionally humble—parallel lines, curves, letterforms, cropping—because they train attention. Druckrey’s core idea is that the eye must be calibrated through deliberate practice so it can make fine distinctions and judge quality without relying on rules of thumb. That calibration then guides the hand, which becomes a precise instrument rather than a source of accidental marks.\nTwo themes recur. Sensitivity to contrast (thin/thick, light/dark, dense/open): students push differences until they can control them, which later allows subtlety. Structure and continuity: how a curve carries energy across space; how a line’s direction implies movement; how negative space shapes the figure as much as strokes do. By iterating studies and pinning work to the wall, students learn comparative judgment—why one arrangement breathes while another feels cramped; why a tiny shift in alignment or spacing can turn awkwardness into clarity. Druckrey frames this as ethics as much as technique: careful seeing is respect for the viewer’s time.\nThe film also connects craft to technology. Good digital typography, interface spacing, and iconography all sit on the same foundations as pen-and-ink calligraphy: consistent stroke logic, optical corrections for perception, and hierarchy established through size, weight, and white space. The cameo references to Steve Jobs underline this continuity—humanist calligraphy informed the Macintosh’s attention to typography, kerning, and proportionally spaced fonts—reminding us that useful software inherits from analog craft. Ultimately, “Teaching to See” argues that design education is less about memorizing rules and more about training judgment. The reward is work that is quiet, exact, and humane—design that “disappears” because its structure is so well seen."
  },
  {
    "objectID": "PrepareforClass2.html#one-page-review",
    "href": "PrepareforClass2.html#one-page-review",
    "title": "Prepare for Class 2",
    "section": "",
    "text": "“Teaching to See” is a 37-minute primer on visual literacy and studio discipline. Inge Druckrey, drawing on the Basel School tradition, shows how designers learn to see—not merely to look at objects, but to perceive relationships: proportion, rhythm, weight, contrast, spacing, and alignment. The film’s early exercises are intentionally humble—parallel lines, curves, letterforms, cropping—because they train attention. Druckrey’s core idea is that the eye must be calibrated through deliberate practice so it can make fine distinctions and judge quality without relying on rules of thumb. That calibration then guides the hand, which becomes a precise instrument rather than a source of accidental marks.\nTwo themes recur. Sensitivity to contrast (thin/thick, light/dark, dense/open): students push differences until they can control them, which later allows subtlety. Structure and continuity: how a curve carries energy across space; how a line’s direction implies movement; how negative space shapes the figure as much as strokes do. By iterating studies and pinning work to the wall, students learn comparative judgment—why one arrangement breathes while another feels cramped; why a tiny shift in alignment or spacing can turn awkwardness into clarity. Druckrey frames this as ethics as much as technique: careful seeing is respect for the viewer’s time.\nThe film also connects craft to technology. Good digital typography, interface spacing, and iconography all sit on the same foundations as pen-and-ink calligraphy: consistent stroke logic, optical corrections for perception, and hierarchy established through size, weight, and white space. The cameo references to Steve Jobs underline this continuity—humanist calligraphy informed the Macintosh’s attention to typography, kerning, and proportionally spaced fonts—reminding us that useful software inherits from analog craft. Ultimately, “Teaching to See” argues that design education is less about memorizing rules and more about training judgment. The reward is work that is quiet, exact, and humane—design that “disappears” because its structure is so well seen."
  },
  {
    "objectID": "PrepareforClass2.html#guided-questions",
    "href": "PrepareforClass2.html#guided-questions",
    "title": "Prepare for Class 2",
    "section": "Guided Questions",
    "text": "Guided Questions\n\n1) Learn thin/thick and curve/linearity from Druckrey\nDruckrey teaches contrast and line quality through repetitive, controlled studies. Using broad- and fine-nib tools, students draw series of parallel strokes to feel how pressure and angle create thin–thick modulation. Curve drills—long, continuous S-curves and circles—build continuity and pacing, while straight-line studies enforce steadiness and alignment. By comparing dozens of iterations on the wall, students learn to judge consistency of stroke, even spacing, clean joins at corners, and transitions where a curve straightens into a line. The aim is sensitivity: to control contrast deliberately rather than by accident.\n\n\n2) How did Holmes describe and contrast the eye and the hand in writing?\nNigel Holmes’s point (as echoed in the film’s commentary) is that the eye is fast, comparative, and judgment-driven, while the hand is slow, mechanical, and trainable. The eye anticipates form—spacing, direction, and weight—and immediately perceives unevenness; the hand must practice until it can execute what the eye intends. Good drawing and writing align the two: the seeing leads, the hand follows. When they are out of sync you get wobbly lines, inconsistent spacing, and tired letterforms; when aligned, the marks feel alive and effortless.\n\n\n3) Why is calligraphy important to Steve Jobs and the design of Mac computers?\nJobs famously audited a calligraphy course at Reed College. From it he absorbed the grammar of letterforms—stroke contrast, serif logic, rhythm, kerning, and optical spacing. Those lessons shaped the original Macintosh: multiple typefaces, proportionally spaced fonts, and attention to typographic detail were prioritized in software and hardware. The humanist values of calligraphy—clarity, proportion, and respect for the reader—became product principles.\n\n\n4) Differentiate geometric accuracy and optical accuracy\nGeometric accuracy is what a ruler or coordinates say is equal; optical accuracy is what looks equal. Because of human perception, purely geometric equality often appears wrong. Designers make “optical corrections”: circles and curved letters overshoot the baseline and cap height; counters are opened to avoid clogging; horizontal and vertical spacing is adjusted so masses feel even; centered objects are nudged to compensate for visual weight. In training the eye, Druckrey stresses judging with perception first and letting geometry serve that judgment."
  },
  {
    "objectID": "EPPS6356_PrepareforClass2.html",
    "href": "EPPS6356_PrepareforClass2.html",
    "title": "Prepare for Class 2",
    "section": "",
    "text": "“Teaching to See” is a 37-minute primer on visual literacy and studio discipline. Inge Druckrey, drawing on the Basel School tradition, shows how designers learn to see—not merely to look at objects, but to perceive relationships: proportion, rhythm, weight, contrast, spacing, and alignment. The film’s early exercises are intentionally humble—parallel lines, curves, letterforms, cropping—because they train attention. Druckrey’s core idea is that the eye must be calibrated through deliberate practice so it can make fine distinctions and judge quality without relying on rules of thumb. That calibration then guides the hand, which becomes a precise instrument rather than a source of accidental marks.\nTwo themes recur. Sensitivity to contrast (thin/thick, light/dark, dense/open): students push differences until they can control them, which later allows subtlety. Structure and continuity: how a curve carries energy across space; how a line’s direction implies movement; how negative space shapes the figure as much as strokes do. By iterating studies and pinning work to the wall, students learn comparative judgment—why one arrangement breathes while another feels cramped; why a tiny shift in alignment or spacing can turn awkwardness into clarity. Druckrey frames this as ethics as much as technique: careful seeing is respect for the viewer’s time.\nThe film also connects craft to technology. Good digital typography, interface spacing, and iconography all sit on the same foundations as pen-and-ink calligraphy: consistent stroke logic, optical corrections for perception, and hierarchy established through size, weight, and white space. The cameo references to Steve Jobs underline this continuity—humanist calligraphy informed the Macintosh’s attention to typography, kerning, and proportionally spaced fonts—reminding us that useful software inherits from analog craft. Ultimately, “Teaching to See” argues that design education is less about memorizing rules and more about training judgment. The reward is work that is quiet, exact, and humane—design that “disappears” because its structure is so well seen."
  },
  {
    "objectID": "EPPS6356_PrepareforClass2.html#one-page-review",
    "href": "EPPS6356_PrepareforClass2.html#one-page-review",
    "title": "Prepare for Class 2",
    "section": "",
    "text": "“Teaching to See” is a 37-minute primer on visual literacy and studio discipline. Inge Druckrey, drawing on the Basel School tradition, shows how designers learn to see—not merely to look at objects, but to perceive relationships: proportion, rhythm, weight, contrast, spacing, and alignment. The film’s early exercises are intentionally humble—parallel lines, curves, letterforms, cropping—because they train attention. Druckrey’s core idea is that the eye must be calibrated through deliberate practice so it can make fine distinctions and judge quality without relying on rules of thumb. That calibration then guides the hand, which becomes a precise instrument rather than a source of accidental marks.\nTwo themes recur. Sensitivity to contrast (thin/thick, light/dark, dense/open): students push differences until they can control them, which later allows subtlety. Structure and continuity: how a curve carries energy across space; how a line’s direction implies movement; how negative space shapes the figure as much as strokes do. By iterating studies and pinning work to the wall, students learn comparative judgment—why one arrangement breathes while another feels cramped; why a tiny shift in alignment or spacing can turn awkwardness into clarity. Druckrey frames this as ethics as much as technique: careful seeing is respect for the viewer’s time.\nThe film also connects craft to technology. Good digital typography, interface spacing, and iconography all sit on the same foundations as pen-and-ink calligraphy: consistent stroke logic, optical corrections for perception, and hierarchy established through size, weight, and white space. The cameo references to Steve Jobs underline this continuity—humanist calligraphy informed the Macintosh’s attention to typography, kerning, and proportionally spaced fonts—reminding us that useful software inherits from analog craft. Ultimately, “Teaching to See” argues that design education is less about memorizing rules and more about training judgment. The reward is work that is quiet, exact, and humane—design that “disappears” because its structure is so well seen."
  },
  {
    "objectID": "EPPS6356_PrepareforClass2.html#guided-questions",
    "href": "EPPS6356_PrepareforClass2.html#guided-questions",
    "title": "Prepare for Class 2",
    "section": "Guided Questions",
    "text": "Guided Questions\n\n1) Learn thin/thick and curve/linearity from Druckrey\nDruckrey teaches contrast and line quality through repetitive, controlled studies. Using broad- and fine-nib tools, students draw series of parallel strokes to feel how pressure and angle create thin–thick modulation. Curve drills—long, continuous S-curves and circles—build continuity and pacing, while straight-line studies enforce steadiness and alignment. By comparing dozens of iterations on the wall, students learn to judge consistency of stroke, even spacing, clean joins at corners, and transitions where a curve straightens into a line. The aim is sensitivity: to control contrast deliberately rather than by accident.\n\n\n2) How did Holmes describe and contrast the eye and the hand in writing?\nNigel Holmes’s point (as echoed in the film’s commentary) is that the eye is fast, comparative, and judgment-driven, while the hand is slow, mechanical, and trainable. The eye anticipates form—spacing, direction, and weight—and immediately perceives unevenness; the hand must practice until it can execute what the eye intends. Good drawing and writing align the two: the seeing leads, the hand follows. When they are out of sync you get wobbly lines, inconsistent spacing, and tired letterforms; when aligned, the marks feel alive and effortless.\n\n\n3) Why is calligraphy important to Steve Jobs and the design of Mac computers?\nJobs famously audited a calligraphy course at Reed College. From it he absorbed the grammar of letterforms—stroke contrast, serif logic, rhythm, kerning, and optical spacing. Those lessons shaped the original Macintosh: multiple typefaces, proportionally spaced fonts, and attention to typographic detail were prioritized in software and hardware. The humanist values of calligraphy—clarity, proportion, and respect for the reader—became product principles.\n\n\n4) Differentiate geometric accuracy and optical accuracy\nGeometric accuracy is what a ruler or coordinates say is equal; optical accuracy is what looks equal. Because of human perception, purely geometric equality often appears wrong. Designers make “optical corrections”: circles and curved letters overshoot the baseline and cap height; counters are opened to avoid clogging; horizontal and vertical spacing is adjusted so masses feel even; centered objects are nudged to compensate for visual weight. In training the eye, Druckrey stresses judging with perception first and letting geometry serve that judgment."
  },
  {
    "objectID": "EPPS6356_PrepareforClass3.html",
    "href": "EPPS6356_PrepareforClass3.html",
    "title": "Prepare for Class 3",
    "section": "",
    "text": "Geoff McGhee’s documentary Journalism in the Age of Data explores how visualization became central to modern storytelling. Through interviews with data journalists at The New York Times, The Guardian, and Gapminder, the film reveals a shift: journalists no longer just write stories—they design arguments that readers can explore visually. The craft moves beyond illustration; graphics are the narrative.\nAcademic visualization, by contrast, tends to emphasize analytical precision, statistical rigor, and reproducibility. In scholarly work, clarity and neutrality are paramount; visuals must represent data truthfully, often at the expense of aesthetic engagement. Journalistic visualization, on the other hand, embraces emotion and immediacy. It borrows from design to guide attention—using color, motion, and annotation to shape interpretation. McGhee frames this as a collaboration between story and structure: the journalist curates context so that readers can navigate complex issues like climate change or budgets intuitively.\nAnother key distinction lies in audience. Academic visualizations address expert readers who can decode abstract charts; journalistic ones target general publics who need the visualization to invite curiosity rather than prove a hypothesis. Yet both domains share a mission: to make invisible systems visible. As McGhee notes, good visualization demands technical literacy and moral restraint—knowing what to emphasize and what to omit.\nUltimately, Journalism in the Age of Data bridges the gap between scientific visualization and narrative design. It argues that the journalist’s role has expanded from storyteller to explainer, translator, and data designer. When done well, this synthesis turns raw information into understanding—an act both analytic and artistic."
  },
  {
    "objectID": "EPPS6356_PrepareforClass3.html#one-page-review-journalism-in-the-age-of-data-geoff-mcghee-2011",
    "href": "EPPS6356_PrepareforClass3.html#one-page-review-journalism-in-the-age-of-data-geoff-mcghee-2011",
    "title": "Prepare for Class 3",
    "section": "",
    "text": "Geoff McGhee’s documentary Journalism in the Age of Data explores how visualization became central to modern storytelling. Through interviews with data journalists at The New York Times, The Guardian, and Gapminder, the film reveals a shift: journalists no longer just write stories—they design arguments that readers can explore visually. The craft moves beyond illustration; graphics are the narrative.\nAcademic visualization, by contrast, tends to emphasize analytical precision, statistical rigor, and reproducibility. In scholarly work, clarity and neutrality are paramount; visuals must represent data truthfully, often at the expense of aesthetic engagement. Journalistic visualization, on the other hand, embraces emotion and immediacy. It borrows from design to guide attention—using color, motion, and annotation to shape interpretation. McGhee frames this as a collaboration between story and structure: the journalist curates context so that readers can navigate complex issues like climate change or budgets intuitively.\nAnother key distinction lies in audience. Academic visualizations address expert readers who can decode abstract charts; journalistic ones target general publics who need the visualization to invite curiosity rather than prove a hypothesis. Yet both domains share a mission: to make invisible systems visible. As McGhee notes, good visualization demands technical literacy and moral restraint—knowing what to emphasize and what to omit.\nUltimately, Journalism in the Age of Data bridges the gap between scientific visualization and narrative design. It argues that the journalist’s role has expanded from storyteller to explainer, translator, and data designer. When done well, this synthesis turns raw information into understanding—an act both analytic and artistic."
  },
  {
    "objectID": "PrepareforClass4.html",
    "href": "PrepareforClass4.html",
    "title": "Prepare for Class 4",
    "section": "",
    "text": "Edward Tufte’s lecture The Future of Data Analysis extends his lifelong campaign for clarity and integrity in how information is displayed. He opens with a simple but radical idea: seeing well is thinking well. In a world overflowing with dashboards, metrics, and automated analytics, Tufte argues that the next frontier of data analysis is not technological—it is philosophical. What matters most is cultivating the human capacity to notice patterns, relationships, and meaning hidden within complexity. Tools can calculate, but only the human mind can interpret.\nThroughout the talk, Tufte revisits his central design principles—truthfulness, precision, and beauty—and re-frames them for an era dominated by algorithms. He traces a line from Galileo’s telescopic sketches to today’s interactive data dashboards, reminding the audience that visualization has always been a cognitive technology: a way to extend perception and reasoning. When we visualize data, we are literally “making thought visible.” Yet he warns that as graphics become more automated, the analyst’s curiosity and skepticism can erode. “Never let the software think for you,” he says, stressing that each chart must emerge from a clear analytical question rather than a template.\nA major theme is integration—of quantitative accuracy with qualitative understanding. Tufte envisions the analyst of the future as both scientist and artist, someone who can write statistical code and also judge visual proportion, hierarchy, and rhythm. This combination of reasoning and design, he suggests, produces visualizations that work—they inform without distorting and persuade without manipulating. He calls for “high data density with low noise,” meaning that every pixel should carry meaning and every visual choice should serve evidence, not decoration.\nThe talk also critiques the overuse of dashboards and “chartjunk.” When organizations rely on flashy but shallow summaries, they risk mistaking performance metrics for knowledge. Tufte argues that genuine understanding requires context: side-by-side comparisons, annotated evidence, and time-series continuity. His principle of small multiples—repeated, comparable views—remains one of the most powerful ways to show change without exaggeration. He contrasts this disciplined approach with sensational graphs that truncate axes or hide uncertainty, calling such distortions ethical failures, not mere design flaws.\nAnother striking idea is the role of narrative in analysis. Tufte insists that visualization should not replace verbal explanation but enhance it. Numbers and stories must coexist: data show what happened, while narrative clarifies why it matters. The most effective visualizations invite the viewer to reason, not just react. They slow perception down enough for reflection—a quality increasingly rare in the fast-scroll world of media and business intelligence tools.\nBy the end, Tufte situates the “future” of data analysis in a moral framework. The abundance of data amplifies both our power and our responsibility. Analysts and designers are now interpreters of reality; their choices can clarify truth or conceal it. He challenges his audience to treat every visualization as an ethical document—something that must earn trust through accuracy, transparency, and aesthetic restraint.\nUltimately, Tufte’s message is timeless: technology may evolve, but the principles of clear thinking do not. The real progress in data analysis will come not from more dashboards, but from deeper attention. The analyst of the future is not a technician running models, but a human observer capable of transforming raw information into understanding.\n\n\n“The future of data analysis is not a machine. It’s a human being with better eyesight.” — Edward Tufte"
  },
  {
    "objectID": "PrepareforClass4.html#one-page-review-the-future-of-data-analysis-edward-tufte-2016",
    "href": "PrepareforClass4.html#one-page-review-the-future-of-data-analysis-edward-tufte-2016",
    "title": "Prepare for Class 4",
    "section": "",
    "text": "Edward Tufte’s lecture The Future of Data Analysis extends his lifelong campaign for clarity and integrity in how information is displayed. He opens with a simple but radical idea: seeing well is thinking well. In a world overflowing with dashboards, metrics, and automated analytics, Tufte argues that the next frontier of data analysis is not technological—it is philosophical. What matters most is cultivating the human capacity to notice patterns, relationships, and meaning hidden within complexity. Tools can calculate, but only the human mind can interpret.\nThroughout the talk, Tufte revisits his central design principles—truthfulness, precision, and beauty—and re-frames them for an era dominated by algorithms. He traces a line from Galileo’s telescopic sketches to today’s interactive data dashboards, reminding the audience that visualization has always been a cognitive technology: a way to extend perception and reasoning. When we visualize data, we are literally “making thought visible.” Yet he warns that as graphics become more automated, the analyst’s curiosity and skepticism can erode. “Never let the software think for you,” he says, stressing that each chart must emerge from a clear analytical question rather than a template.\nA major theme is integration—of quantitative accuracy with qualitative understanding. Tufte envisions the analyst of the future as both scientist and artist, someone who can write statistical code and also judge visual proportion, hierarchy, and rhythm. This combination of reasoning and design, he suggests, produces visualizations that work—they inform without distorting and persuade without manipulating. He calls for “high data density with low noise,” meaning that every pixel should carry meaning and every visual choice should serve evidence, not decoration.\nThe talk also critiques the overuse of dashboards and “chartjunk.” When organizations rely on flashy but shallow summaries, they risk mistaking performance metrics for knowledge. Tufte argues that genuine understanding requires context: side-by-side comparisons, annotated evidence, and time-series continuity. His principle of small multiples—repeated, comparable views—remains one of the most powerful ways to show change without exaggeration. He contrasts this disciplined approach with sensational graphs that truncate axes or hide uncertainty, calling such distortions ethical failures, not mere design flaws.\nAnother striking idea is the role of narrative in analysis. Tufte insists that visualization should not replace verbal explanation but enhance it. Numbers and stories must coexist: data show what happened, while narrative clarifies why it matters. The most effective visualizations invite the viewer to reason, not just react. They slow perception down enough for reflection—a quality increasingly rare in the fast-scroll world of media and business intelligence tools.\nBy the end, Tufte situates the “future” of data analysis in a moral framework. The abundance of data amplifies both our power and our responsibility. Analysts and designers are now interpreters of reality; their choices can clarify truth or conceal it. He challenges his audience to treat every visualization as an ethical document—something that must earn trust through accuracy, transparency, and aesthetic restraint.\nUltimately, Tufte’s message is timeless: technology may evolve, but the principles of clear thinking do not. The real progress in data analysis will come not from more dashboards, but from deeper attention. The analyst of the future is not a technician running models, but a human observer capable of transforming raw information into understanding.\n\n\n“The future of data analysis is not a machine. It’s a human being with better eyesight.” — Edward Tufte"
  },
  {
    "objectID": "PrepareforClass5.html",
    "href": "PrepareforClass5.html",
    "title": "Prepare for Class 5",
    "section": "",
    "text": "Big data has reshaped how we understand social behavior, public health, and markets—but with its scale comes illusion. The Parable of Google Flu, published in Nature, remains a cautionary tale. In 2008, Google’s algorithm predicted flu outbreaks by analyzing search queries, claiming to forecast epidemics faster than the CDC. For a brief period, the results looked revolutionary—until they stopped working. The algorithm overestimated flu cases by more than 140%, revealing that the apparent success of “big data” can mask deeper analytical flaws.\nThe first pitfall is data without theory. Massive datasets tempt analysts to treat correlation as causation. Without grounding in domain knowledge, we misinterpret noise as signal. In the Google Flu case, rising media attention and public anxiety—rather than infection rates—drove search trends. This created a self-reinforcing feedback loop that the model mistook for real-world growth. Tufte’s warning about “seeing without thinking” echoes here: visualization and data alike require interpretation, not just computation.\nAnother pitfall is algorithmic opacity. Big data often relies on proprietary, ever-changing systems. When inputs, models, or weighting methods are unknown, replication becomes impossible. This erodes scientific transparency and accountability—two pillars of credible data analysis. Furthermore, as datasets grow, bias does not disappear; it scales. If underlying data are incomplete, unrepresentative, or contextually skewed, bigger datasets simply reproduce error with greater confidence.\nClosely related is the danger of overfitting and overparameterization. In large models, especially machine learning systems, it’s easy to fit data so perfectly that the model memorizes patterns rather than generalizes them. This creates high accuracy in training but failure in prediction. Overfitting is a form of overconfidence disguised as precision—it reflects modeling power used without restraint. The solution, as both Hadley Wickham and Andrew Gelman emphasize, lies in simplicity, cross-validation, and clarity: build models that explain rather than impress.\nEthically, the pitfalls of big data remind us that quantity does not equal quality. A million observations cannot compensate for poor measurement, biased sampling, or conceptual confusion. The role of the analyst is to filter wisely, visualize responsibly, and remain skeptical of “too-good-to-be-true” correlations. In the end, meaningful data analysis depends not on how much data we have, but on how thoughtfully we use it."
  },
  {
    "objectID": "PrepareforClass5.html#one-page-note-big-data-pitfalls-and-overfitting",
    "href": "PrepareforClass5.html#one-page-note-big-data-pitfalls-and-overfitting",
    "title": "Prepare for Class 5",
    "section": "",
    "text": "Big data has reshaped how we understand social behavior, public health, and markets—but with its scale comes illusion. The Parable of Google Flu, published in Nature, remains a cautionary tale. In 2008, Google’s algorithm predicted flu outbreaks by analyzing search queries, claiming to forecast epidemics faster than the CDC. For a brief period, the results looked revolutionary—until they stopped working. The algorithm overestimated flu cases by more than 140%, revealing that the apparent success of “big data” can mask deeper analytical flaws.\nThe first pitfall is data without theory. Massive datasets tempt analysts to treat correlation as causation. Without grounding in domain knowledge, we misinterpret noise as signal. In the Google Flu case, rising media attention and public anxiety—rather than infection rates—drove search trends. This created a self-reinforcing feedback loop that the model mistook for real-world growth. Tufte’s warning about “seeing without thinking” echoes here: visualization and data alike require interpretation, not just computation.\nAnother pitfall is algorithmic opacity. Big data often relies on proprietary, ever-changing systems. When inputs, models, or weighting methods are unknown, replication becomes impossible. This erodes scientific transparency and accountability—two pillars of credible data analysis. Furthermore, as datasets grow, bias does not disappear; it scales. If underlying data are incomplete, unrepresentative, or contextually skewed, bigger datasets simply reproduce error with greater confidence.\nClosely related is the danger of overfitting and overparameterization. In large models, especially machine learning systems, it’s easy to fit data so perfectly that the model memorizes patterns rather than generalizes them. This creates high accuracy in training but failure in prediction. Overfitting is a form of overconfidence disguised as precision—it reflects modeling power used without restraint. The solution, as both Hadley Wickham and Andrew Gelman emphasize, lies in simplicity, cross-validation, and clarity: build models that explain rather than impress.\nEthically, the pitfalls of big data remind us that quantity does not equal quality. A million observations cannot compensate for poor measurement, biased sampling, or conceptual confusion. The role of the analyst is to filter wisely, visualize responsibly, and remain skeptical of “too-good-to-be-true” correlations. In the end, meaningful data analysis depends not on how much data we have, but on how thoughtfully we use it."
  },
  {
    "objectID": "PrepareforClass5.html#hadley-wickham-data-visualization-and-data-science-embl-summary-and-commentary",
    "href": "PrepareforClass5.html#hadley-wickham-data-visualization-and-data-science-embl-summary-and-commentary",
    "title": "Prepare for Class 5",
    "section": "Hadley Wickham: Data Visualization and Data Science (EMBL) — Summary and Commentary",
    "text": "Hadley Wickham: Data Visualization and Data Science (EMBL) — Summary and Commentary\nIn his EMBL talk, Hadley Wickham explains that data visualization is the bridge between data and understanding. He introduces the “grammar of graphics” — a structured way to describe visualizations as combinations of data, aesthetic mappings, and geometries. This framework, implemented in his ggplot2 package, allows analysts to move from ad-hoc plotting to reproducible, composable graphics. Every visual element in a chart—points, lines, bars, colors, or facets—corresponds to a decision in the grammar, making visual design transparent and logical.\nWickham also connects visualization to the broader workflow of data science: importing, tidying, transforming, visualizing, and modeling. He emphasizes that visualization is not the end of analysis but the middle of thinking—a feedback loop that helps analysts notice patterns, spot errors, and refine questions. The best insights often emerge while visualizing, not after modeling.\nAnother key contribution is his advocacy for tidy data principles, where each variable forms a column, each observation a row, and each dataset a table. This structure allows visualization tools and analytical packages to interoperate seamlessly. Combined with ggplot2, dplyr, and tidyr, the tidyverse creates an ecosystem where clarity, readability, and reproducibility come first.\nWickham’s main message parallels Tufte’s: clarity is ethics in data visualization. The goal is not to impress audiences with complexity but to reveal truth with simplicity. In practice, this means designing graphics that emphasize structure over style, accuracy over decoration, and transparency over spectacle. His work has transformed visualization from a craft of intuition into a system of reasoning—a grammar that turns data into readable, trustworthy stories.\n\n\n“The purpose of visualization is not to show data; it’s to make people think about data.” — Hadley Wickham"
  },
  {
    "objectID": "EPPS6356_PrepareforClass5.html",
    "href": "EPPS6356_PrepareforClass5.html",
    "title": "Prepare for Class 5",
    "section": "",
    "text": "Big data has reshaped how we understand social behavior, public health, and markets but with its scale comes illusion. The Parable of Google Flu, published in Nature, remains a cautionary tale. In 2008, Google’s algorithm predicted flu outbreaks by analyzing search queries, claiming to forecast epidemics faster than the CDC. For a brief period, the results looked revolutionary until they stopped working. The algorithm overestimated flu cases by more than 140%, revealing that the apparent success of “big data” can mask deeper analytical flaws.\nThe first pitfall is data without theory. Massive datasets tempt analysts to treat correlation as causation. Without grounding in domain knowledge, we misinterpret noise as signal. In the Google Flu case, rising media attention and public anxiety, rather than infection rates, drove search trends. This created a self-reinforcing feedback loop that the model mistook for real-world growth. Tufte’s warning about “seeing without thinking” echoes here: visualization and data alike require interpretation, not just computation.\nAnother pitfall is algorithmic opacity. Big data often relies on proprietary, ever-changing systems. When inputs, models, or weighting methods are unknown, replication becomes impossible. This erodes scientific transparency and accountability, two pillars of credible data analysis. Furthermore, as datasets grow, bias does not disappear; it scales. If underlying data are incomplete, unrepresentative, or contextually skewed, bigger datasets simply reproduce error with greater confidence.\nClosely related is the danger of overfitting and overparameterization. In large models, especially machine learning systems, it’s easy to fit data so perfectly that the model memorizes patterns rather than generalizes them. This creates high accuracy in training but failure in prediction. Overfitting is a form of overconfidence disguised as precision it reflects modeling power used without restraint. The solution, as both Hadley Wickham and Andrew Gelman emphasize, lies in simplicity, cross-validation, and clarity: build models that explain rather than impress.\nEthically, the pitfalls of big data remind us that quantity does not equal quality. A million observations cannot compensate for poor measurement, biased sampling, or conceptual confusion. The role of the analyst is to filter wisely, visualize responsibly, and remain skeptical of “too-good-to-be-true” correlations. In the end, meaningful data analysis depends not on how much data we have, but on how thoughtfully we use it."
  },
  {
    "objectID": "EPPS6356_PrepareforClass5.html#one-page-note-big-data-pitfalls-and-overfitting",
    "href": "EPPS6356_PrepareforClass5.html#one-page-note-big-data-pitfalls-and-overfitting",
    "title": "Prepare for Class 5",
    "section": "",
    "text": "Big data has reshaped how we understand social behavior, public health, and markets—but with its scale comes illusion. The Parable of Google Flu, published in Nature, remains a cautionary tale. In 2008, Google’s algorithm predicted flu outbreaks by analyzing search queries, claiming to forecast epidemics faster than the CDC. For a brief period, the results looked revolutionary—until they stopped working. The algorithm overestimated flu cases by more than 140%, revealing that the apparent success of “big data” can mask deeper analytical flaws.\nThe first pitfall is data without theory. Massive datasets tempt analysts to treat correlation as causation. Without grounding in domain knowledge, we misinterpret noise as signal. In the Google Flu case, rising media attention and public anxiety—rather than infection rates—drove search trends. This created a self-reinforcing feedback loop that the model mistook for real-world growth. Tufte’s warning about “seeing without thinking” echoes here: visualization and data alike require interpretation, not just computation.\nAnother pitfall is algorithmic opacity. Big data often relies on proprietary, ever-changing systems. When inputs, models, or weighting methods are unknown, replication becomes impossible. This erodes scientific transparency and accountability—two pillars of credible data analysis. Furthermore, as datasets grow, bias does not disappear; it scales. If underlying data are incomplete, unrepresentative, or contextually skewed, bigger datasets simply reproduce error with greater confidence.\nClosely related is the danger of overfitting and overparameterization. In large models, especially machine learning systems, it’s easy to fit data so perfectly that the model memorizes patterns rather than generalizes them. This creates high accuracy in training but failure in prediction. Overfitting is a form of overconfidence disguised as precision—it reflects modeling power used without restraint. The solution, as both Hadley Wickham and Andrew Gelman emphasize, lies in simplicity, cross-validation, and clarity: build models that explain rather than impress.\nEthically, the pitfalls of big data remind us that quantity does not equal quality. A million observations cannot compensate for poor measurement, biased sampling, or conceptual confusion. The role of the analyst is to filter wisely, visualize responsibly, and remain skeptical of “too-good-to-be-true” correlations. In the end, meaningful data analysis depends not on how much data we have, but on how thoughtfully we use it."
  },
  {
    "objectID": "EPPS6356_PrepareforClass5.html#hadley-wickham-data-visualization-and-data-science-embl-summary-and-commentary",
    "href": "EPPS6356_PrepareforClass5.html#hadley-wickham-data-visualization-and-data-science-embl-summary-and-commentary",
    "title": "Prepare for Class 5",
    "section": "Hadley Wickham: Data Visualization and Data Science (EMBL) — Summary and Commentary",
    "text": "Hadley Wickham: Data Visualization and Data Science (EMBL) — Summary and Commentary\nIn his EMBL talk, Hadley Wickham explains that data visualization is the bridge between data and understanding. He introduces the “grammar of graphics” — a structured way to describe visualizations as combinations of data, aesthetic mappings, and geometries. This framework, implemented in his ggplot2 package, allows analysts to move from ad-hoc plotting to reproducible, composable graphics. Every visual element in a chart—points, lines, bars, colors, or facets—corresponds to a decision in the grammar, making visual design transparent and logical.\nWickham also connects visualization to the broader workflow of data science: importing, tidying, transforming, visualizing, and modeling. He emphasizes that visualization is not the end of analysis but the middle of thinking—a feedback loop that helps analysts notice patterns, spot errors, and refine questions. The best insights often emerge while visualizing, not after modeling.\nAnother key contribution is his advocacy for tidy data principles, where each variable forms a column, each observation a row, and each dataset a table. This structure allows visualization tools and analytical packages to interoperate seamlessly. Combined with ggplot2, dplyr, and tidyr, the tidyverse creates an ecosystem where clarity, readability, and reproducibility come first.\nWickham’s main message parallels Tufte’s: clarity is ethics in data visualization. The goal is not to impress audiences with complexity but to reveal truth with simplicity. In practice, this means designing graphics that emphasize structure over style, accuracy over decoration, and transparency over spectacle. His work has transformed visualization from a craft of intuition into a system of reasoning—a grammar that turns data into readable, trustworthy stories.\n\n\n“The purpose of visualization is not to show data; it’s to make people think about data.” — Hadley Wickham"
  },
  {
    "objectID": "EPPS6356_PrepareforClass4.html",
    "href": "EPPS6356_PrepareforClass4.html",
    "title": "Prepare for Class 4",
    "section": "",
    "text": "Edward Tufte’s lecture The Future of Data Analysis extends his lifelong campaign for clarity and integrity in how information is displayed. He opens with a simple but radical idea: seeing well is thinking well. In a world overflowing with dashboards, metrics, and automated analytics, Tufte argues that the next frontier of data analysis is not technological—it is philosophical. What matters most is cultivating the human capacity to notice patterns, relationships, and meaning hidden within complexity. Tools can calculate, but only the human mind can interpret.\nThroughout the talk, Tufte revisits his central design principles—truthfulness, precision, and beauty—and re-frames them for an era dominated by algorithms. He traces a line from Galileo’s telescopic sketches to today’s interactive data dashboards, reminding the audience that visualization has always been a cognitive technology: a way to extend perception and reasoning. When we visualize data, we are literally “making thought visible.” Yet he warns that as graphics become more automated, the analyst’s curiosity and skepticism can erode. “Never let the software think for you,” he says, stressing that each chart must emerge from a clear analytical question rather than a template.\nA major theme is integration—of quantitative accuracy with qualitative understanding. Tufte envisions the analyst of the future as both scientist and artist, someone who can write statistical code and also judge visual proportion, hierarchy, and rhythm. This combination of reasoning and design, he suggests, produces visualizations that work—they inform without distorting and persuade without manipulating. He calls for “high data density with low noise,” meaning that every pixel should carry meaning and every visual choice should serve evidence, not decoration.\nThe talk also critiques the overuse of dashboards and “chartjunk.” When organizations rely on flashy but shallow summaries, they risk mistaking performance metrics for knowledge. Tufte argues that genuine understanding requires context: side-by-side comparisons, annotated evidence, and time-series continuity. His principle of small multiples—repeated, comparable views—remains one of the most powerful ways to show change without exaggeration. He contrasts this disciplined approach with sensational graphs that truncate axes or hide uncertainty, calling such distortions ethical failures, not mere design flaws.\nAnother striking idea is the role of narrative in analysis. Tufte insists that visualization should not replace verbal explanation but enhance it. Numbers and stories must coexist: data show what happened, while narrative clarifies why it matters. The most effective visualizations invite the viewer to reason, not just react. They slow perception down enough for reflection—a quality increasingly rare in the fast-scroll world of media and business intelligence tools.\nBy the end, Tufte situates the “future” of data analysis in a moral framework. The abundance of data amplifies both our power and our responsibility. Analysts and designers are now interpreters of reality; their choices can clarify truth or conceal it. He challenges his audience to treat every visualization as an ethical document—something that must earn trust through accuracy, transparency, and aesthetic restraint.\nUltimately, Tufte’s message is timeless: technology may evolve, but the principles of clear thinking do not. The real progress in data analysis will come not from more dashboards, but from deeper attention. The analyst of the future is not a technician running models, but a human observer capable of transforming raw information into understanding.\n\n\n“The future of data analysis is not a machine. It’s a human being with better eyesight.” — Edward Tufte"
  },
  {
    "objectID": "EPPS6356_PrepareforClass4.html#one-page-review-the-future-of-data-analysis-edward-tufte-2016",
    "href": "EPPS6356_PrepareforClass4.html#one-page-review-the-future-of-data-analysis-edward-tufte-2016",
    "title": "Prepare for Class 4",
    "section": "",
    "text": "Edward Tufte’s lecture The Future of Data Analysis extends his lifelong campaign for clarity and integrity in how information is displayed. He opens with a simple but radical idea: seeing well is thinking well. In a world overflowing with dashboards, metrics, and automated analytics, Tufte argues that the next frontier of data analysis is not technological—it is philosophical. What matters most is cultivating the human capacity to notice patterns, relationships, and meaning hidden within complexity. Tools can calculate, but only the human mind can interpret.\nThroughout the talk, Tufte revisits his central design principles—truthfulness, precision, and beauty—and re-frames them for an era dominated by algorithms. He traces a line from Galileo’s telescopic sketches to today’s interactive data dashboards, reminding the audience that visualization has always been a cognitive technology: a way to extend perception and reasoning. When we visualize data, we are literally “making thought visible.” Yet he warns that as graphics become more automated, the analyst’s curiosity and skepticism can erode. “Never let the software think for you,” he says, stressing that each chart must emerge from a clear analytical question rather than a template.\nA major theme is integration—of quantitative accuracy with qualitative understanding. Tufte envisions the analyst of the future as both scientist and artist, someone who can write statistical code and also judge visual proportion, hierarchy, and rhythm. This combination of reasoning and design, he suggests, produces visualizations that work—they inform without distorting and persuade without manipulating. He calls for “high data density with low noise,” meaning that every pixel should carry meaning and every visual choice should serve evidence, not decoration.\nThe talk also critiques the overuse of dashboards and “chartjunk.” When organizations rely on flashy but shallow summaries, they risk mistaking performance metrics for knowledge. Tufte argues that genuine understanding requires context: side-by-side comparisons, annotated evidence, and time-series continuity. His principle of small multiples—repeated, comparable views—remains one of the most powerful ways to show change without exaggeration. He contrasts this disciplined approach with sensational graphs that truncate axes or hide uncertainty, calling such distortions ethical failures, not mere design flaws.\nAnother striking idea is the role of narrative in analysis. Tufte insists that visualization should not replace verbal explanation but enhance it. Numbers and stories must coexist: data show what happened, while narrative clarifies why it matters. The most effective visualizations invite the viewer to reason, not just react. They slow perception down enough for reflection—a quality increasingly rare in the fast-scroll world of media and business intelligence tools.\nBy the end, Tufte situates the “future” of data analysis in a moral framework. The abundance of data amplifies both our power and our responsibility. Analysts and designers are now interpreters of reality; their choices can clarify truth or conceal it. He challenges his audience to treat every visualization as an ethical document—something that must earn trust through accuracy, transparency, and aesthetic restraint.\nUltimately, Tufte’s message is timeless: technology may evolve, but the principles of clear thinking do not. The real progress in data analysis will come not from more dashboards, but from deeper attention. The analyst of the future is not a technician running models, but a human observer capable of transforming raw information into understanding.\n\n\n“The future of data analysis is not a machine. It’s a human being with better eyesight.” — Edward Tufte"
  },
  {
    "objectID": "EPPS6302_Assignment4.html",
    "href": "EPPS6302_Assignment4.html",
    "title": "Assignment 4: Webscraping 1",
    "section": "",
    "text": "library(rvest) library(xml2) library(dplyr) library(readr)"
  },
  {
    "objectID": "EPPS6302_Assignment4.html#load-required-packages",
    "href": "EPPS6302_Assignment4.html#load-required-packages",
    "title": "Assignment 4: Webscraping 1",
    "section": "",
    "text": "library(rvest)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union"
  },
  {
    "objectID": "EPPS6302_Assignment4.html#scrape-foreign-exchange-reserves-table-original-task",
    "href": "EPPS6302_Assignment4.html#scrape-foreign-exchange-reserves-table-original-task",
    "title": "Assignment 4: Webscraping 1",
    "section": "2. Scrape Foreign Exchange Reserves Table (Original Task)",
    "text": "2. Scrape Foreign Exchange Reserves Table (Original Task)\n\nurl_fx &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves\"\n\nfx_page &lt;- read_html(url_fx)\n\n# Extract all tables\nfx_tables &lt;- html_table(html_nodes(fx_page, \"table\"), fill = TRUE)\n\n# Use the first table with country-wise reserves\nfx_df &lt;- fx_tables[[1]]\n\n# Fix duplicated column names\nnames(fx_df) &lt;- make.names(names(fx_df), unique = TRUE)\n\n# Rename and clean\nfx_clean &lt;- fx_df %&gt;%\n  rename(\n    Country = 1,\n    Reserves_USD_Billion = 2,\n    Date = 3,\n    Source = 4\n  ) %&gt;%\n  filter(!is.na(Reserves_USD_Billion), Country != \"\") %&gt;%\n  mutate(\n    Reserves_USD_Billion = as.numeric(gsub(\",\", \"\", Reserves_USD_Billion)),\n    Date = str_trim(Date)\n  )\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `Reserves_USD_Billion = as.numeric(gsub(\",\", \"\",\n  Reserves_USD_Billion))`.\nCaused by warning:\n! NAs introduced by coercion\n\nhead(fx_clean)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\nReserves_USD_Billion\nDate\nSource\nForeign.exchange.reserves.2\nForeign.exchange.reserves.3\nLast.reporteddate\nRef.\n\n\n\n\nCountry(as recognized by the U.N.)\nNA\nIncluding gold\nIncluding gold\nExcluding gold\nExcluding gold\nLast reporteddate\nRef.\n\n\nCountry(as recognized by the U.N.)\nNA\nmillions U.S.$\nChange\nmillions U.S.$\nChange\nLast reporteddate\nRef.\n\n\nChina\nNA\n3,643,149\n41,079\n3,389,306\n31,221\n31 Aug 2025\n[3]\n\n\nJapan\nNA\n1,324,210\n19,774\n1,230,940\n16,230\n31 Aug 2025\n[4]\n\n\nSwitzerland\nNA\n1,007,710\n13,935\n897,295\n14,490\n31 Jul 2025\n[5]\n\n\nRussia\nNA\n713,300\n700\n434,487\n1,517\n26 Sep 2025\n[6]"
  },
  {
    "objectID": "EPPS6302_Assignment4.html#scrape-gdp-data-by-country-modified-table",
    "href": "EPPS6302_Assignment4.html#scrape-gdp-data-by-country-modified-table",
    "title": "Assignment 4: Webscraping 1",
    "section": "3. Scrape GDP Data by Country (Modified Table)",
    "text": "3. Scrape GDP Data by Country (Modified Table)\n\nurl_gdp &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n\ngdp_page &lt;- read_html(url_gdp)\n\ngdp_tables &lt;- html_table(html_nodes(gdp_page, \"table\"), fill = TRUE)\n\n# We'll use the second table\ngdp_df &lt;- gdp_tables[[2]]\n\nhead(gdp_df)\n\n\n\n\n\n\n\n\n\nX1\nX2\n\n\n\n\n.mw-parser-output .legend{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .legend-color{display:inline-block;min-width:1.25em;height:1.25em;line-height:1.25;margin:1px 0;text-align:center;border:1px solid black;background-color:transparent;color:black}.mw-parser-output .legend-text{}  &gt; $20 trillion   $10–20 trillion   $5–10 trillion   $1–5 trillion   $750 billion – $1 trillion   $500–750 billion\n$250–500 billion   $100–250 billion   $50–100 billion   $25–50 billion   $5–25 billion   &lt; $5 billion"
  },
  {
    "objectID": "EPPS6302_Assignment4.html#suggest-a-data-plan-for-research",
    "href": "EPPS6302_Assignment4.html#suggest-a-data-plan-for-research",
    "title": "Assignment 4: Webscraping 1",
    "section": "5. Suggest a Data Plan for Research",
    "text": "5. Suggest a Data Plan for Research\n\nTo study global financial preparedness and development levels, I propose merging Wikipedia data on foreign reserves and GDP by country. This will allow researchers to calculate the ratio of foreign reserves to GDP, a useful indicator for macroeconomic stability and risk exposure. Future work may include scraping literacy rate, inflation, or debt data to analyze patterns across regions using R visualizations, regressions, and mapping tools."
  },
  {
    "objectID": "EPPS6302_Assignment4.html#conclusion",
    "href": "EPPS6302_Assignment4.html#conclusion",
    "title": "Assignment 4: Webscraping 1",
    "section": "Conclusion",
    "text": "Conclusion\nThe assignment requirements are satisfied by: (1) reproducing the base scrape, (2) modifying it to another table on the same page, (3) cleaning with a DateCollected variable and removal of extra rows/columns, and (4) outlining a practical acquisition plan. Both cleaned datasets are exported and linked above for download."
  },
  {
    "objectID": "EPPS6302_Assignment5.html",
    "href": "EPPS6302_Assignment5.html",
    "title": "Assignment 5: Government Data API",
    "section": "",
    "text": "1) Ten Most Recent Documents — Linked PDFs\nThe following PDFs are the files produced by my R script (govtdata01-style) and saved in this working folder. I’m linking them directly here as proof of retrieval.\n\nyourpathgovfiles_1.pdf\nyourpathgovfiles_2.pdf\nyourpathgovfiles_3.pdf\nyourpathgovfiles_4.pdf\nyourpathgovfiles_5.pdf\nyourpathgovfiles_6.pdf\nyourpathgovfiles_7.pdf\nyourpathgovfiles_8.pdf\nyourpathgovfiles_9.pdf\nyourpathgovfiles_10.pdf\n\n\n\n2) Simple Report on Difficulties Encountered in Scraping\n\nNon-uniform metadata across collections: The govinfo ecosystem aggregates many collections (e.g., Congressional Record, bills, resolutions). Fields like committeeName, pdfLink, or dateIssued are present in some collections but missing or named differently in others. This created brittle parsing unless I coded fallbacks (e.g., scan nested download fields or search all character fields for a *.pdf URL).\nPDF URL field inconsistency: Some packages expose download$pdfLink, others surface a pdfUrl or nest the link deeper. A robust extractor must check multiple candidate fields.\nQuery ambiguity for committees: The search endpoint is text-first. A literal query like \"Foreign Relations Committee\" can retrieve highly relevant items and tangential mentions. A tradeoff exists between recall and precision unless we add post-filters (e.g., by collection or chamber) and manual validation.\nOccasional transient HTTP issues: Intermittent timeouts or gateway errors required retry logic (e.g., httr::RETRY) and gentle rate limiting.\n\n\n\n\n3) How Useable is the Scraped Data?\nStrengths - Authoritative sources with stable identifiers (e.g., packageId) improve reproducibility. - JSON summaries are machine-readable and work well for building tables, indexes, and archival workflows. - Direct links to canonical PDFs make citation straightforward for course deliverables.\nLimitations - Inconsistent field names/coverage across collections complicates filtering by committee or subject. - Some items lack structured tags (e.g., standardized committee codes), so text search can over/under-include. - Occasional missing or moved PDF links require defensive code paths.\nBottom line: The data are usable and citable, but require schema-aware cleaning and manual spot checks when precision matters (e.g., “only Foreign Relations Committee” rather than general foreign-relations content).\n\n\n\n4) How to Improve the Workflow / API\n\nStandardize committee facets across collections (committeeCode, committeeName) for reliable filtering.\nExpose a single canonical pdf_link in package summaries and document endpoints.\nPublish richer examples combining text and structured filters (committee, collection, date range, chamber).\nReturn explicit 429 rate-limit signals and offer a batch summary endpoint for multiple packageId lookups.\nProvide collection-specific schemas with field presence/absence matrices to reduce trial-and-error coding.\n\n\n\n\n5) Reproducibility Note (What I Ran)\n\nI first ran an R script modeled on govtdata01.R to query govinfo, resolve package summaries, and download the top 10 most recent Foreign-Relations–related documents as yourpathgovfiles_1.pdf … yourpathgovfiles_10.pdf.\nThis report links those local PDFs as evidence of retrieval and provides an index table above.\nIf re-running on a different day, you may see a different set of documents (the API is time-ordered)."
  },
  {
    "objectID": "EPPS6302_Assignment5.html#objective",
    "href": "EPPS6302_Assignment5.html#objective",
    "title": "Assignment 5: Government Data API",
    "section": "",
    "text": "This assignment demonstrates how to access and download government data using the GovInfo API, focusing on the ten most recent documents from the Foreign Relations Committee.\nThe script automates data retrieval, cleaning, and export into a CSV file."
  },
  {
    "objectID": "EPPS6302_Assignment5.html#r-script-govt-data-retrieval",
    "href": "EPPS6302_Assignment5.html#r-script-govt-data-retrieval",
    "title": "Assignment 5: Government Data API",
    "section": "2. R Script: Govt Data Retrieval",
    "text": "2. R Script: Govt Data Retrieval\n\n# -------------------------------\n# Load Required Libraries\n# -------------------------------\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(dplyr)\n\n# -------------------------------\n# Define API URL and Parameters\n# -------------------------------\nbase_url &lt;- \"https://api.govinfo.gov/search\"\napi_key &lt;- \"DEMO_KEY\"  # Replace with your actual API key from https://api.govinfo.gov/docs/api/key/\n\nparams &lt;- list(\n  query = \"Foreign Relations Committee\",\n  pageSize = 10,\n  sortBy = \"date\",\n  sortOrder = \"desc\",\n  api_key = api_key\n)\n\n# -------------------------------\n# API Request\n# -------------------------------\nresponse &lt;- GET(base_url, query = params)\n\nif (status_code(response) == 200) {\n  data &lt;- content(response, as = \"text\", encoding = \"UTF-8\")\n  json_data &lt;- fromJSON(data, flatten = TRUE)\n  \n  # Extract relevant fields\n  docs &lt;- json_data$results %&gt;%\n    select(title, collectionCode, dateIssued, govInfoLink = link)\n  \n  # Save to CSV\n  write.csv(docs, \"ForeignRelations_10Docs.csv\", row.names = FALSE)\n  \n  print(\"✅ Data successfully saved as ForeignRelations_10Docs.csv\")\n  head(docs)\n  \n} else {\n  print(paste(\"❌ API request failed with status:\", status_code(response)))\n}\n\n[1] \"❌ API request failed with status: 400\""
  },
  {
    "objectID": "EPPS6302_Assignment5.html#output-preview",
    "href": "EPPS6302_Assignment5.html#output-preview",
    "title": "Assignment 5: Government Data API",
    "section": "3. Output Preview",
    "text": "3. Output Preview\n\n# View the first few results (preview)\nhead(docs)"
  },
  {
    "objectID": "EPPS6302_Assignment5.html#reflection-report",
    "href": "EPPS6302_Assignment5.html#reflection-report",
    "title": "Assignment 5: Government Data API",
    "section": "4. Reflection Report",
    "text": "4. Reflection Report\n\n4.1 Difficulties Encountered\n\nAPI Key Access — The GovInfo API requires a registered key; using the demo key can restrict access or limit the number of requests.\n\nComplex JSON Structure — Extracting relevant elements like title, date, and links required flattening nested lists.\n\nQuery Sensitivity — Generic searches for “Foreign Relations Committee” sometimes returned unrelated documents without filters.\n\nPagination & Sorting — Ensuring the newest records appeared first required sortBy = \"date\" and sortOrder = \"desc\".\n\n\n\n\n4.2 How Usable Was the Scraped Data?\nThe resulting dataset was structured and well-formatted, containing document metadata (titles, collection codes, dates, and links).\nWhile the data is clean and suitable for statistical analysis or visualization, the document content itself (e.g., full hearing transcripts) is not included — only the metadata and URLs.\n\n\n\n4.3 How to Improve\n\nText Extraction: Use pdftools to download and extract text from linked documents for deeper content analysis.\n\nAutomated Scheduling: Implement cronR or taskscheduleR to update datasets regularly.\n\nAdvanced Filtering: Include additional parameters like collectionCode = 'CHRG' to narrow to congressional hearings.\n\nVisualization: Add summary charts (e.g., document counts by year or committee)."
  },
  {
    "objectID": "EPPS6302_Assignment5.html#conclusion",
    "href": "EPPS6302_Assignment5.html#conclusion",
    "title": "Assignment 5: Government Data API",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nThis assignment highlights the importance of API-based government data access for reproducible research.\nThrough this workflow, analysts can directly retrieve structured data, analyze policy documents, and maintain transparency and replicability in social research."
  },
  {
    "objectID": "EPPS6302_Assignment5.html#references",
    "href": "EPPS6302_Assignment5.html#references",
    "title": "Assignment 5: Government Data API",
    "section": "6. References",
    "text": "6. References\n\nU.S. Government Publishing Office. (n.d.). GovInfo API Documentation. https://api.govinfo.gov/docs/\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. O’Reilly Media."
  },
  {
    "objectID": "EPPS6356_Assignment4.html",
    "href": "EPPS6356_Assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Height = mean flipper length (mm); Width = sample share per species.\n\nvw &lt;- peng |&gt;\n  group_by(species) |&gt;\n  summarise(n = n(),\n            mean_flipper = mean(flipper_length_mm, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  arrange(desc(n)) |&gt;\n  mutate(w_share = n / sum(n),\n         xmax = cumsum(w_share),\n         xmin = lag(xmax, default = 0),\n         xmid = (xmin + xmax)/2,\n         fill = c(col_setosa, col_versicolor, col_virginica))\n\np1 &lt;- ggplot(vw) +\n  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = 0, ymax = mean_flipper),\n            fill = col_fill_soft, color = \"#8C9A96\", linewidth = 0.6) +\n  geom_text(aes(x = xmid, y = mean_flipper + 5, label = species),\n            size = 4, color = \"#333333\") +\n  scale_y_continuous(\"Mean flipper length (mm)\",\n                     expand = expansion(mult = c(0, .12))) +\n  scale_x_continuous(\"Share of samples\",\n                     labels = label_percent(accuracy = 1),\n                     breaks = seq(0,1,0.2),\n                     expand = expansion(mult = c(0,0))) +\n  labs(title = \"Average Flipper Length with Population-Weighted Widths\",\n       subtitle = \"Each column's width is proportional to species' sample share\",\n       caption = \"Data: palmerpenguins\") +\n  coord_cartesian(clip = \"off\")\n\np1"
  },
  {
    "objectID": "EPPS6356_Assignment4_Shapes.html",
    "href": "EPPS6356_Assignment4_Shapes.html",
    "title": "Assignment 4 — Shapes 1–4 Matched",
    "section": "",
    "text": "1 Goal\nReplicate the shapes shown in the Thought‑Starter for 1–4:\n1) Variable‑Width Column, 2) Table with Embedded Charts (grid), 3) Bar Chart (many items, horizontal, no gaps), 4) Column Chart (few items, grouped).\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n\n\n2 1) Variable‑Width Column Chart (two variables per item) — state.x77\n\nHeight = Income\n\nWidth = Population share\n\nShape: variable bar widths (base R is the simplest)\n\n\ndf &lt;- as.data.frame(state.x77); df$State &lt;- rownames(df)\n\ntop8 &lt;- df |&gt;\n  select(State, Population, Income) |&gt;\n  arrange(desc(Population)) |&gt;\n  slice(1:8)\n\nw &lt;- top8$Population / sum(top8$Population)  # widths\n\npar(mar = c(6, 4, 2, 1))\nbp &lt;- barplot(\n  height = top8$Income,\n  width  = w,\n  col    = \"gray70\",\n  border = \"gray35\",\n  main   = \"Variable‑Width Columns: Income (height) & Population Share (width)\",\n  ylab   = \"Per‑Capita Income (USD)\",\n  xaxt   = \"n\"\n)\naxis(1, at = bp, labels = top8$State, las = 2, cex.axis = 0.85)\nbox(bty = \"l\")\n\n\n\n\n\n\n\n\n\n\n\n3 2) Table with Embedded Charts (grid of small bars) — iris\n\nShape: a grid like a table; each cell shows a tiny bar chart.\n\nWe show mean of each measurement inside a Species × Measurement grid.\n\n\nmeans &lt;- iris |&gt;\n  group_by(Species) |&gt;\n  summarise(across(ends_with(\"Length\") | ends_with(\"Width\"), mean), .groups=\"drop\") |&gt;\n  pivot_longer(-Species, names_to = \"Measurement\", values_to = \"Mean\")\n\nggplot(means, aes(x = Measurement, y = Mean)) +\n  geom_col(fill = \"gray55\", width = 0.8) +\n  facet_grid(Species ~ .) +\n  labs(title = \"Table with Embedded Charts — Means per Species\",\n       x = NULL, y = NULL) +\n  theme_minimal(base_size = 13) +\n  theme(strip.text = element_text(face = \"bold\"),\n        axis.text.x = element_text(angle = 15, hjust = 1),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n4 3) Bar Chart (many items, horizontal, no gaps) — USArrests\n\nShape: long ranked horizontal bars without spaces between them.\n\n\nua &lt;- USArrests |&gt;\n  as.data.frame() |&gt;\n  mutate(State = rownames(USArrests)) |&gt;\n  arrange(desc(Murder))\n\nggplot(ua, aes(x = reorder(State, Murder), y = Murder)) +\n  geom_col(width = 1, fill = \"gray55\", color = \"gray40\") +\n  coord_flip() +\n  scale_x_discrete(expand = c(0,0)) +\n  labs(title = \"Bar Chart — Murder Rate by State (ranked)\",\n       x = NULL, y = \"Murder (per 100,000)\") +\n  theme_minimal(base_size = 13) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n5 4) Column Chart (few items, grouped) — mpg\n\nShape: grouped vertical columns (few categories).\n\nShow Avg city MPG by cylinders, grouped by drivetrain (f vs r). Only a few groups.\n\n\nfew &lt;- mpg |&gt;\n  filter(drv %in% c(\"f\",\"r\"), cyl %in% c(4,6,8)) |&gt;\n  group_by(cyl, drv) |&gt;\n  summarise(avg_cty = mean(cty, na.rm = TRUE), .groups = \"drop\")\n\nggplot(few, aes(x = factor(cyl), y = avg_cty, fill = drv)) +\n  geom_col(position = position_dodge(width = 0.7), width = 0.65, color = \"gray30\") +\n  scale_x_discrete(expand = c(0,0)) +\n  scale_fill_manual(values = c(\"f\" = \"gray85\", \"r\" = \"gray55\"), labels = c(\"FWD\",\"RWD\")) +\n  labs(title = \"Column Chart — Avg City MPG by Cylinders (grouped by drivetrain)\",\n       x = \"Cylinders\", y = \"Avg City MPG\", fill = NULL) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"bottom\",\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n6 Team & Synergy (short)\nCoordinator: Eman Ajmal. Split by chart; verified labels and shapes match the brief."
  },
  {
    "objectID": "EPPS6356_Assignment4_AshleyStyle.html",
    "href": "EPPS6356_Assignment4_AshleyStyle.html",
    "title": "Assignment 4 — Charts 1–4 (Ashley‑Style, Different Data)",
    "section": "",
    "text": "1 Setup\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\ntheme_set(theme_minimal(base_size = 14))\n\n\n\n\n2 1) Variable‑Width Column Chart (Two variables per item) — state.x77\n\nHeight = per‑capita Income\n\nWidth = Population share (top 8 states)\n\nSimple base R barplot() because it natively supports variable widths.\n\n\ndf &lt;- as.data.frame(state.x77); df$State &lt;- rownames(df)\ntop8 &lt;- df |&gt; select(State, Population, Income) |&gt; arrange(desc(Population)) |&gt; slice(1:8)\nw &lt;- top8$Population / sum(top8$Population)\n\npar(mar = c(6, 5, 3, 1))\nbp &lt;- barplot(top8$Income,\n              width = w,\n              col = \"grey70\", border = \"grey40\",\n              main = \"Variable‑Width Columns — Income (height) & Population (width)\",\n              ylab = \"Per‑Capita Income (USD)\", xaxt = \"n\")\naxis(1, at = bp, labels = top8$State, las = 2, cex.axis = 0.9)\nbox(bty = \"l\")\n\n\n\n\n\n\n\n\n\n\n\n3 2) Table with Embedded Charts (Many categories) — iris\nGrid of tiny bars: mean of each measurement per species (looks like a table with minis).\n\nmeans &lt;- iris |&gt;\n  group_by(Species) |&gt;\n  summarise(across(ends_with(\"Length\") | ends_with(\"Width\"), mean), .groups=\"drop\") |&gt;\n  pivot_longer(-Species, names_to = \"Measurement\", values_to = \"Mean\")\n\nggplot(means, aes(Measurement, Mean)) +\n  geom_col(fill = \"grey55\", width = 0.8) +\n  facet_grid(Species ~ .) +\n  labs(title = \"Table with Embedded Charts — Means per Species\",\n       x = NULL, y = NULL) +\n  theme(axis.text.x = element_text(angle = 15, hjust = 1),\n        strip.text = element_text(face = \"bold\"),\n        panel.grid.minor = element_blank(),\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n4 3) Bar Chart (Many items; horizontal; no gaps) — USArrests\nRanked Murder rate by state — long bars, width = 1 and no spacing.\n\nua &lt;- USArrests |&gt; as.data.frame() |&gt; mutate(State = rownames(USArrests)) |&gt; arrange(desc(Murder))\n\nggplot(ua, aes(reorder(State, Murder), Murder)) +\n  geom_col(width = 1, fill = \"grey60\", color = \"grey45\") +\n  coord_flip() +\n  scale_x_discrete(expand = c(0,0)) +\n  labs(title = \"Bar Chart — Murder Rate by State (ranked)\",\n       x = NULL, y = \"Murder (per 100,000)\") +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n5 4) Column Chart (Few items; grouped) — mpg\nFew items, grouped columns (dodge) like the sketch.\n\nfew &lt;- mpg |&gt;\n  filter(drv %in% c(\"f\",\"r\"), cyl %in% c(4,6,8)) |&gt;\n  group_by(cyl, drv) |&gt;\n  summarise(avg_cty = mean(cty, na.rm = TRUE), .groups = \"drop\")\n\nggplot(few, aes(factor(cyl), avg_cty, fill = drv)) +\n  geom_col(position = position_dodge(width = 0.7), width = 0.65, color = \"grey40\") +\n  scale_fill_manual(values = c(\"f\"=\"grey85\",\"r\"=\"grey60\"), labels = c(\"FWD\",\"RWD\")) +\n  scale_x_discrete(expand = c(0,0)) +\n  labs(title = \"Column Chart — Avg City MPG by Cylinders (grouped by drivetrain)\",\n       x = \"Cylinders\", y = \"Avg City MPG\", fill = NULL) +\n  theme(legend.position = \"bottom\",\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n6 Team (short)\nCoordinator: Eman Ajmal. Split by chart; kept shapes minimal and clean."
  },
  {
    "objectID": "EPPS6356_Assignment4_Fixed_Simple.html",
    "href": "EPPS6356_Assignment4_Fixed_Simple.html",
    "title": "Assignment 4 — Charts 1–4 (Fixed & Simple)",
    "section": "",
    "text": "1 Setup\n\nlibrary(ggplot2)\nlibrary(dplyr)\ntheme_set(theme_minimal(base_size = 14))\n\n\n\n\n2 1) Variable‑Width Column — Income (height) & Population Share (width)\nFixes: correct x‑axis label positions (use bp midpoints), clear outlines, sorted by population.\n\ndf &lt;- as.data.frame(state.x77); df$State &lt;- rownames(df)\ntop6 &lt;- df |&gt; select(State, Population, Income) |&gt; arrange(desc(Population)) |&gt; slice(1:6)\nw &lt;- top6$Population / sum(top6$Population)\n\npar(mar = c(7, 5, 3, 1))                    # more bottom space for labels\nbp &lt;- barplot(height = top6$Income,\n              width  = w,\n              col    = \"grey85\",\n              border = \"grey40\",\n              main   = \"Income (height) & Population Share (width) — Top 6 States\",\n              ylab   = \"Per‑Capita Income (USD)\",\n              xaxt   = \"n\")\naxis(1, at = bp, labels = top6$State, las = 2, cex.axis = 0.95)  # &lt;- use bp!\nbox(bty = \"l\")\n\n\n\n\n\n\n\n\n\n\n\n3 2) Table with Embedded Charts — Mini Bars in a Grid (readable)\nFixes: true grid of mini charts (one bar per cell), no crowded axes.\n\n# Build a 3 (Species) x 4 (Measurements) grid with one bar per cell\nmeans &lt;- aggregate(. ~ Species, data = iris, FUN = mean)\nmeans_long &lt;- data.frame(\n  Species     = rep(means$Species, each = 4),\n  Measurement = factor(rep(names(means)[2:5], times = nrow(means)),\n                       levels = c(\"Petal.Length\",\"Petal.Width\",\"Sepal.Length\",\"Sepal.Width\")),\n  Mean        = c(t(as.matrix(means[, 2:5])))\n)\n\nggplot(means_long, aes(x = 1, y = Mean)) +\n  geom_col(fill = \"grey60\", width = 0.6) +\n  facet_grid(Species ~ Measurement) +\n  coord_cartesian(ylim = c(0, max(means_long$Mean) * 1.1)) +\n  labs(title = \"Table with Embedded Charts — Means per Species (mini-bars)\",\n       x = NULL, y = NULL) +\n  theme(\n    axis.text  = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank(),\n    strip.text = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n4 3) Bar Chart — Murder Rate by State (readable, spacing + two panels)\nFixes: add space between bars (width = 0.85), split into two bar charts for readability, readable labels, shaded bars by value.\n\nua &lt;- USArrests |&gt; as.data.frame() |&gt; mutate(State = rownames(USArrests)) |&gt; arrange(desc(Murder))\nua$rank &lt;- seq_len(nrow(ua))\nua$panel &lt;- ifelse(ua$rank &lt;= 25, \"Top 25 Murder Rate\", \"Next 25 Murder Rate\")\n\nggplot(ua, aes(x = reorder(State, Murder), y = Murder, fill = Murder)) +\n  geom_col(width = 0.85, color = \"grey50\") +                 # spacing between bars\n  coord_flip() +\n  facet_wrap(~ panel, ncol = 2, scales = \"free_y\") +         # two bar charts\n  scale_fill_gradient(low = \"grey85\", high = \"grey40\") +     # shaded bars\n  labs(title = \"Murder Rate by State (ranked — split into two readable panels)\",\n       x = NULL, y = \"Murder per 100,000\", fill = NULL) +\n  theme(\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\n\n\n\n5 4) Column Chart — Avg MPG by Cylinders (two columns per group)\n(You said this one is fine; unchanged except slightly narrower bars for clarity.)\n\nmc &lt;- mtcars\nmc$cyl &lt;- factor(mc$cyl)\nmc$am  &lt;- factor(mc$am, labels = c(\"Auto\",\"Manual\"))\navg &lt;- aggregate(mpg ~ cyl + am, data = mc, FUN = mean)\n\nggplot(avg, aes(x = cyl, y = mpg, fill = am)) +\n  geom_col(position = position_dodge(width = 0.75), width = 0.6, color = \"grey40\") +\n  scale_fill_manual(values = c(\"grey85\",\"grey60\")) +\n  labs(title = \"Average MPG by Cylinders (two columns per group)\",\n       x = \"Cylinders\", y = \"Average MPG\", fill = NULL) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "EPPS6356_Assignment4_SameData_Colored.html",
    "href": "EPPS6356_Assignment4_SameData_Colored.html",
    "title": "Assignment 4 — Charts 1–4 on the SAME Dataset (Colored, Simple)",
    "section": "",
    "text": "1 Setup (ONE dataset for all charts: iris)\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\ntheme_set(theme_minimal(base_size = 14))\n\n# simple color palette (3 species + 2-variable pair later)\npal_species &lt;- c(\"#4C78A8\",\"#F58518\",\"#54A24B\")      # setosa, versicolor, virginica\npal_pair    &lt;- c(\"#9E9E9E\",\"#5D5D5D\")                # two bars in chart 4\n\n\n\n\n2 1) Variable‑Width Column Chart (two variables per item) — iris\nTwo variables per item using one dataset:\n- Height = average Petal.Length in Sepal.Length group\n- Width = number of flowers in that group (proportional)\n\n# Bin sepal length into 3 groups to vary widths\niris_bins &lt;- iris |&gt;\n  mutate(group = cut(Sepal.Length, breaks = c(4.0,5.6,7.0,8.0),\n                     labels = c(\"Short (4.0–5.6)\",\"Medium (5.6–7.0)\",\"Long (7.0–8.0)\"),\n                     include.lowest = TRUE)) |&gt;\n  group_by(group) |&gt;\n  summarise(height = mean(Petal.Length), n = n(), .groups = \"drop\") |&gt;\n  mutate(width = n / sum(n)) |&gt;\n  arrange(group)\n\n# base R is the simplest way to do variable widths\npar(mar = c(6,5,2,1))\nw &lt;- iris_bins$width\nbp &lt;- barplot(iris_bins$height, width = w, col = \"#B0BEC5\", border = \"#455A64\",\n              main = \"Avg Petal Length by Sepal Length Group\\n(width = group size)\",\n              ylab = \"Average Petal Length (cm)\", xaxt = \"n\")\naxis(1, at = bp, labels = iris_bins$group, las = 2, cex.axis = 0.9)\n\n\n\n\n\n\n\n\n\n\n\n3 2) Table with Embedded Charts (same dataset; colored; readable grid)\nGrid of mini bar charts: mean of each measurement per species.\n\nmeans &lt;- iris |&gt;\n  group_by(Species) |&gt;\n  summarise(across(c(Petal.Length, Petal.Width, Sepal.Length, Sepal.Width), mean), .groups = \"drop\") |&gt;\n  pivot_longer(-Species, names_to = \"Measurement\", values_to = \"Mean\")\n\nggplot(means, aes(Measurement, Mean, fill = Species)) +\n  geom_col(width = 0.7, color = \"white\") +\n  facet_wrap(~ Species, ncol = 3) +\n  scale_fill_manual(values = pal_species) +\n  labs(title = \"Table with Embedded Charts — Mean Measurements per Species\",\n       x = \"Measurement\", y = \"Mean Value\") +\n  theme(panel.grid.major.y = element_line(color = \"grey85\"),\n        axis.text.x = element_text(angle = 25, hjust = 1),\n        strip.text = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n4 3) Bar Chart (Many Items) — same dataset; colored; readable\nHorizontal bars of Petal.Length for all 150 flowers, grouped by species.\nBars are narrow with small spacing for readability (the model sketch shows tight stacks).\n\niris_many &lt;- iris |&gt;\n  group_by(Species) |&gt;\n  arrange(Species, desc(Petal.Length)) |&gt;\n  mutate(id = row_number())\n\nggplot(iris_many, aes(x = reorder(paste(Species, id), Petal.Length), y = Petal.Length, fill = Species)) +\n  geom_col(width = 0.9, color = \"white\") +        # small visible spacing\n  coord_flip() +\n  facet_wrap(~ Species, ncol = 3, scales = \"free_y\") +   # mini bar charts from same data\n  scale_fill_manual(values = pal_species) +\n  labs(title = \"Petal Length for Many Flowers (ranked within species)\",\n       x = NULL, y = \"Petal Length (cm)\") +\n  theme(panel.grid.major.y = element_blank(),\n        axis.text.y = element_blank(),\n        strip.text = element_text(face = \"bold\"),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n5 4) Column Chart (Few Items; two columns each) — same dataset; colored\nFew items = 3 species. For each species, compare two variables (Petal vs Sepal Length means) → two columns per species.\n\npair_means &lt;- iris |&gt;\n  group_by(Species) |&gt;\n  summarise(Petal.Length = mean(Petal.Length),\n            Sepal.Length = mean(Sepal.Length), .groups = \"drop\") |&gt;\n  pivot_longer(-Species, names_to = \"Measure\", values_to = \"Mean\")\n\nggplot(pair_means, aes(x = Species, y = Mean, fill = Measure)) +\n  geom_col(position = position_dodge(width = 0.75), width = 0.65, color = \"white\") +\n  scale_fill_manual(values = pal_pair, labels = c(\"Petal Length\",\"Sepal Length\")) +\n  labs(title = \"Species Comparison — Two Columns per Group (Petal vs Sepal Length)\",\n       x = \"Species\", y = \"Mean (cm)\", fill = NULL) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "EPPS6356_Assignment4 Update.html",
    "href": "EPPS6356_Assignment4 Update.html",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "",
    "text": "Brief: Four chart types from the Chart Thought Starter using the same object.\nObject: Penguin measurements (palmerpenguins).\nPalette: Muted (RColorBrewer Set2/Greys)."
  },
  {
    "objectID": "EPPS6356_Assignment4 Update.html#setup",
    "href": "EPPS6356_Assignment4 Update.html#setup",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "Setup",
    "text": "Setup"
  },
  {
    "objectID": "EPPS6356_Assignment4 Update.html#variable-width-column-chart-two-variables-per-item",
    "href": "EPPS6356_Assignment4 Update.html#variable-width-column-chart-two-variables-per-item",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "1) Variable-Width Column Chart — Two variables per item",
    "text": "1) Variable-Width Column Chart — Two variables per item\nHeight = mean flipper length; Width = sample share per species."
  },
  {
    "objectID": "EPPS6356_Assignment4 Update.html#table-with-embedded-charts-many-categories",
    "href": "EPPS6356_Assignment4 Update.html#table-with-embedded-charts-many-categories",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "2) Table with Embedded Charts — Many categories",
    "text": "2) Table with Embedded Charts — Many categories\nSmall-multiple histograms by Species × Measurement."
  },
  {
    "objectID": "EPPS6356_Assignment4 Update.html#bar-chart-many-items-few-categories",
    "href": "EPPS6356_Assignment4 Update.html#bar-chart-many-items-few-categories",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "3) Bar Chart — Many items, few categories",
    "text": "3) Bar Chart — Many items, few categories\nHorizontal bars of body mass for all Adelie penguins."
  },
  {
    "objectID": "EPPS6356_Assignment4 Update.html#column-chart-few-items",
    "href": "EPPS6356_Assignment4 Update.html#column-chart-few-items",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "4) Column Chart — Few items",
    "text": "4) Column Chart — Few items\nGrouped columns: means of four measurements by species."
  },
  {
    "objectID": "EPPS6356_Assignment4 Update.html#reproducibility-ai-use",
    "href": "EPPS6356_Assignment4 Update.html#reproducibility-ai-use",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "Reproducibility & AI Use",
    "text": "Reproducibility & AI Use\n\nSoftware: R () 4.2; packages: ggplot2, dplyr, tidyr, scales, palmerpenguins, RColorBrewer.\n\nAI disclosure: Chart logic drafted with ChatGPT (GPT-5 Thinking). Prompts focused on implementing the four Chart Thought Starter types using a single object and muted palettes."
  },
  {
    "objectID": "EPPS6356_Assignment4.html#setup",
    "href": "EPPS6356_Assignment4.html#setup",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "Setup",
    "text": "Setup"
  },
  {
    "objectID": "EPPS6356_Assignment4.html#variable-width-column-chart-two-variables-per-item",
    "href": "EPPS6356_Assignment4.html#variable-width-column-chart-two-variables-per-item",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "1) Variable-Width Column Chart — Two variables per item",
    "text": "1) Variable-Width Column Chart — Two variables per item\nHeight = mean flipper length; Width = sample share per species."
  },
  {
    "objectID": "EPPS6356_Assignment4.html#table-with-embedded-charts-many-categories",
    "href": "EPPS6356_Assignment4.html#table-with-embedded-charts-many-categories",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "2) Table with Embedded Charts — Many categories",
    "text": "2) Table with Embedded Charts — Many categories\nSmall-multiple histograms by Species × Measurement."
  },
  {
    "objectID": "EPPS6356_Assignment4.html#bar-chart-many-items-few-categories",
    "href": "EPPS6356_Assignment4.html#bar-chart-many-items-few-categories",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "3) Bar Chart — Many items, few categories",
    "text": "3) Bar Chart — Many items, few categories\nHorizontal bars of body mass for all Adelie penguins."
  },
  {
    "objectID": "EPPS6356_Assignment4.html#column-chart-few-items",
    "href": "EPPS6356_Assignment4.html#column-chart-few-items",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "4) Column Chart — Few items",
    "text": "4) Column Chart — Few items\nGrouped columns: means of four measurements by species."
  },
  {
    "objectID": "EPPS6356_Assignment4.html#reproducibility-ai-use",
    "href": "EPPS6356_Assignment4.html#reproducibility-ai-use",
    "title": "EPPS6356 — Assignment 4: Multiple Works on the Same Object",
    "section": "Reproducibility & AI Use",
    "text": "Reproducibility & AI Use\n\nSoftware: R () 4.2; packages: ggplot2, dplyr, tidyr, scales, palmerpenguins, RColorBrewer.\n\nAI disclosure: Chart logic drafted with ChatGPT (GPT-5 Thinking). Prompts focused on implementing the four Chart Thought Starter types using a single object and muted palettes."
  },
  {
    "objectID": "EPPS6356_Assignment4_Clean.html",
    "href": "EPPS6356_Assignment4_Clean.html",
    "title": "Assignment 4",
    "section": "",
    "text": "# Packages\nreq &lt;- c(\"ggplot2\",\"dplyr\",\"tidyr\",\"scales\",\"palmerpenguins\")\nto_install &lt;- req[!req %in% rownames(installed.packages())]\nif (length(to_install)) install.packages(to_install, repos = \"https://cloud.r-project.org\")\n\nlibrary(ggplot2); library(dplyr); library(tidyr); library(scales); library(palmerpenguins)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\n# ------------------\n# Global theme + palette\n# ------------------\ntheme_set(\n  theme_minimal(base_size = 13) +\n    theme(\n      plot.title = element_text(face = \"bold\", size = 20, margin = margin(b = 6)),\n      plot.subtitle = element_text(size = 13, color = \"#555555\", margin = margin(b = 10)),\n      plot.caption = element_text(size = 10, color = \"#777777\", hjust = 1),\n      axis.title.x = element_text(margin = margin(t = 8)),\n      axis.title.y = element_text(margin = margin(r = 8)),\n      panel.grid.minor = element_blank(),\n      panel.grid.major.x = element_line(color = \"#ECECEC\"),\n      panel.grid.major.y = element_line(color = \"#ECECEC\")\n    )\n)\n\n# Accessible, muted palette (Okabe-Ito)\ncol_setosa    &lt;- \"#3F7BB6\"  # blue\ncol_versicolor&lt;- \"#E69F00\"  # orange\ncol_virginica &lt;- \"#59A14F\"  # green\ncol_neutral   &lt;- \"#9AA3A6\"  # grey accent\ncol_fill_soft &lt;- \"#C6D9D2\"  # soft green-grey for singles\n\npeng &lt;- penguins |&gt; filter(!is.na(species), !is.na(island))"
  },
  {
    "objectID": "EPPS6356_Assignment4_Clean.html#variable-width-column-chart-two-variables-per-item",
    "href": "EPPS6356_Assignment4_Clean.html#variable-width-column-chart-two-variables-per-item",
    "title": "EPPS6356 — Assignment 4 (Clean Edition): Multiple Works on the Same Object",
    "section": "1) Variable-Width Column Chart — Two variables per item",
    "text": "1) Variable-Width Column Chart — Two variables per item\nHeight = mean flipper length (mm); Width = sample share per species."
  },
  {
    "objectID": "EPPS6356_Assignment4_Clean.html#table-with-embedded-charts-many-categories",
    "href": "EPPS6356_Assignment4_Clean.html#table-with-embedded-charts-many-categories",
    "title": "EPPS6356 — Assignment 4 (Clean Edition): Multiple Works on the Same Object",
    "section": "2) Table with Embedded Charts — Many categories",
    "text": "2) Table with Embedded Charts — Many categories\nSmall-multiple histograms (Species × Measurement). Muted fills; units shown in facet labels."
  },
  {
    "objectID": "EPPS6356_Assignment4_Clean.html#bar-chart-many-items-few-categories",
    "href": "EPPS6356_Assignment4_Clean.html#bar-chart-many-items-few-categories",
    "title": "EPPS6356 — Assignment 4 (Clean Edition): Multiple Works on the Same Object",
    "section": "3) Bar Chart — Many items, few categories",
    "text": "3) Bar Chart — Many items, few categories\nTwo parallel charts (same design) so it’s easier to read — Adelie and Chinstrap body mass for every observed penguin."
  },
  {
    "objectID": "EPPS6356_Assignment4_Clean.html#column-chart-few-items",
    "href": "EPPS6356_Assignment4_Clean.html#column-chart-few-items",
    "title": "EPPS6356 — Assignment 4 (Clean Edition): Multiple Works on the Same Object",
    "section": "4) Column Chart — Few items",
    "text": "4) Column Chart — Few items\nFew items shown cleanly: means of four measurements by species, faceted to respect units."
  },
  {
    "objectID": "EPPS6356_Assignment4_Clean.html#variable-width-column-chart",
    "href": "EPPS6356_Assignment4_Clean.html#variable-width-column-chart",
    "title": "Assignment 4",
    "section": "1) Variable-Width Column Chart",
    "text": "1) Variable-Width Column Chart\nHeight = mean flipper length (mm); Width = sample share per species.\n\nvw &lt;- peng |&gt;\n  group_by(species) |&gt;\n  summarise(n = n(),\n            mean_flipper = mean(flipper_length_mm, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  arrange(desc(n)) |&gt;\n  mutate(w_share = n / sum(n),\n         xmax = cumsum(w_share),\n         xmin = lag(xmax, default = 0),\n         xmid = (xmin + xmax)/2,\n         fill = c(col_setosa, col_versicolor, col_virginica))\n\np1 &lt;- ggplot(vw) +\n  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = 0, ymax = mean_flipper),\n            fill = col_fill_soft, color = \"#8C9A96\", linewidth = 0.6) +\n  geom_text(aes(x = xmid, y = mean_flipper + 5, label = species),\n            size = 4, color = \"#333333\") +\n  scale_y_continuous(\"Mean flipper length (mm)\",\n                     expand = expansion(mult = c(0, .12))) +\n  scale_x_continuous(\"Share of samples\",\n                     labels = label_percent(accuracy = 1),\n                     breaks = seq(0,1,0.2),\n                     expand = expansion(mult = c(0,0))) +\n  labs(title = \"Average Flipper Length with Population-Weighted Widths\",\n       subtitle = \"Each column's width is proportional to species' sample share\",\n       caption = \"Data: palmerpenguins\") +\n  coord_cartesian(clip = \"off\")\n\np1"
  },
  {
    "objectID": "EPPS6356_Assignment4_Clean.html#table-with-embedded-charts",
    "href": "EPPS6356_Assignment4_Clean.html#table-with-embedded-charts",
    "title": "Assignment 4",
    "section": "2) Table with Embedded Charts",
    "text": "2) Table with Embedded Charts\nSmall-multiple histograms (Species × Measurement). Muted fills; units shown in facet labels.\n\npeng_long &lt;- peng |&gt;\n  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt;\n  pivot_longer(-species, names_to = \"measurement\", values_to = \"value\") |&gt;\n  mutate(measurement = factor(measurement,\n         levels = c(\"bill_length_mm\",\"bill_depth_mm\",\"sepal_dummy\",\"flipper_length_mm\",\"body_mass_g\"),\n         labels = c(\"Bill length (mm)\",\"Bill depth (mm)\",\"_\",\"Flipper length (mm)\",\"Body mass (g)\")))\n\n# Use species-specific muted colors\nfill_map &lt;- c(\"Adelie\" = col_setosa, \"Chinstrap\" = col_versicolor, \"Gentoo\" = col_virginica)\n\np2 &lt;- ggplot(peng_long, aes(value, fill = species)) +\n  geom_histogram(color = \"white\", bins = 12) +\n  facet_grid(species ~ measurement, scales = \"free_x\") +\n  scale_fill_manual(values = fill_map, guide = \"none\") +\n  labs(title = \"Distributions of Penguin Measurements\",\n       x = \"Measurement Value\", y = \"Count\",\n       subtitle = \"Each row is a species; each column is a measurement\",\n       caption = \"Data: palmerpenguins\") +\n  theme(\n    strip.text = element_text(face = \"bold\"),\n    panel.grid = element_blank(),\n    panel.border = element_rect(color = \"#E6E6E6\", fill = NA)\n  )\n\np2\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "EPPS6356_Assignment4_Clean.html#bar-chart",
    "href": "EPPS6356_Assignment4_Clean.html#bar-chart",
    "title": "Assignment 4",
    "section": "3) Bar Chart",
    "text": "3) Bar Chart\nTwo parallel charts (same design) so it’s easier to read — Adelie and Chinstrap body mass for every observed penguin.\n\nmany &lt;- peng |&gt;\n  dplyr::filter(species %in% c(\"Adelie\",\"Chinstrap\"), !is.na(body_mass_g)) |&gt;\n  dplyr::group_by(species) |&gt;\n  dplyr::arrange(body_mass_g, .by_group = TRUE) |&gt;\n  dplyr::mutate(id = factor(row_number())) |&gt;\n  dplyr::ungroup()\n\nfill_map &lt;- c(\"Adelie\" = col_setosa, \"Chinstrap\" = col_versicolor)\n\np3 &lt;- ggplot(many, aes(id, body_mass_g, fill = species)) +\n  geom_col(width = 0.65, color = \"white\") +\n  coord_flip() +\n  facet_wrap(~ species, ncol = 2, scales = \"free_y\") +\n  scale_fill_manual(values = fill_map, guide = \"none\") +\n  scale_y_continuous(\"Body mass (g)\",\n                     breaks = scales::pretty_breaks(n = 6),\n                     labels = label_number(accuracy = 1, scale_cut = cut_si(\"\")),\n                     expand = expansion(mult = c(0.02, 0.06))) +\n  labs(title = \"Body Mass by Individual — Many Items, Few Categories\",\n       subtitle = \"Each panel shows one species; bars ordered by body mass\",\n       x = NULL, caption = \"Data: palmerpenguins\") +\n  theme(\n    axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n    axis.text.x = element_text(color = \"#333333\")\n  )\n\np3"
  },
  {
    "objectID": "EPPS6356_Assignment4_Clean.html#column-chart",
    "href": "EPPS6356_Assignment4_Clean.html#column-chart",
    "title": "Assignment 4",
    "section": "4) Column Chart",
    "text": "4) Column Chart\nFew items shown cleanly: means of four measurements by species, faceted to respect units.\n\nmeans_long &lt;- peng |&gt;\n  dplyr::group_by(species) |&gt;\n  dplyr::summarise(\n    `Bill depth (mm)`     = mean(bill_depth_mm,     na.rm = TRUE),\n    `Bill length (mm)`    = mean(bill_length_mm,    na.rm = TRUE),\n    `Body mass (g)`       = mean(body_mass_g,       na.rm = TRUE),\n    `Flipper length (mm)` = mean(flipper_length_mm, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  tidyr::pivot_longer(-species, names_to = \"measurement\", values_to = \"mean_value\") |&gt;\n  dplyr::group_by(measurement) |&gt;\n  dplyr::mutate(index = 100 * mean_value / mean(mean_value, na.rm = TRUE)) |&gt;\n  dplyr::ungroup()\n\n# 2) Manual x positions -&gt; bars touch inside each trio, gap between trios\nmeas_levels &lt;- c(\"Bill depth (mm)\", \"Bill length (mm)\", \"Body mass (g)\", \"Flipper length (mm)\")\nspec_levels &lt;- c(\"Adelie\",\"Chinstrap\",\"Gentoo\")\n\nbars_per_group &lt;- length(spec_levels)  # 3\ngap_units      &lt;- 1                    # tweak this to widen/narrow gaps between categories\n\nplot_df &lt;- means_long |&gt;\n  dplyr::mutate(\n    measurement = factor(measurement, levels = meas_levels),\n    species     = factor(species,     levels = spec_levels),\n    g_idx = as.integer(measurement) - 1,\n    s_idx = as.integer(species)     - 1,\n    x     = g_idx * (bars_per_group + gap_units) + s_idx\n  )\n\n# axis breaks at the center of each trio\nbreaks &lt;- ((0:(length(meas_levels)-1)) * (bars_per_group + gap_units)) + (bars_per_group - 1)/2\nlabels &lt;- meas_levels\n\n# 3) Muted palette\npal_grouped &lt;- c(\"Adelie\"=\"#5B7EA3\", \"Chinstrap\"=\"#B58B45\", \"Gentoo\"=\"#6BAA75\")\n\np4 &lt;- ggplot(plot_df, aes(x = x, y = index, fill = species)) +\n  geom_col(width = 1, color = NA) +  # 0.98 so bars in a trio visually touch\n  scale_x_continuous(breaks = breaks, labels = labels, expand = expansion(mult = c(0.02, 0.04))) +\n  scale_fill_manual(values = pal_grouped, name = \"Species\") +\n  scale_y_continuous(\"Index (measurement mean = 100)\", expand = expansion(mult = c(0, 0.05))) +\n  labs(\n    title = \"Mean Penguin Measurements by Species (Indexed)\",\n    x = \"Measurement\",\n    caption = \"Data: palmerpenguins\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title  = element_text(face = \"bold\", size = 18),\n    axis.text.x = element_text(angle = 12, hjust = 1, vjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\np4"
  },
  {
    "objectID": "EPPS6356_Assignment4.html#variable-width-column-chart",
    "href": "EPPS6356_Assignment4.html#variable-width-column-chart",
    "title": "Assignment 4",
    "section": "",
    "text": "Height = mean flipper length (mm); Width = sample share per species.\n\nvw &lt;- peng |&gt;\n  group_by(species) |&gt;\n  summarise(n = n(),\n            mean_flipper = mean(flipper_length_mm, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  arrange(desc(n)) |&gt;\n  mutate(w_share = n / sum(n),\n         xmax = cumsum(w_share),\n         xmin = lag(xmax, default = 0),\n         xmid = (xmin + xmax)/2,\n         fill = c(col_setosa, col_versicolor, col_virginica))\n\np1 &lt;- ggplot(vw) +\n  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = 0, ymax = mean_flipper),\n            fill = col_fill_soft, color = \"#8C9A96\", linewidth = 0.6) +\n  geom_text(aes(x = xmid, y = mean_flipper + 5, label = species),\n            size = 4, color = \"#333333\") +\n  scale_y_continuous(\"Mean flipper length (mm)\",\n                     expand = expansion(mult = c(0, .12))) +\n  scale_x_continuous(\"Share of samples\",\n                     labels = label_percent(accuracy = 1),\n                     breaks = seq(0,1,0.2),\n                     expand = expansion(mult = c(0,0))) +\n  labs(title = \"Average Flipper Length with Population-Weighted Widths\",\n       subtitle = \"Each column's width is proportional to species' sample share\",\n       caption = \"Data: palmerpenguins\") +\n  coord_cartesian(clip = \"off\")\n\np1"
  },
  {
    "objectID": "EPPS6356_Assignment4.html#table-with-embedded-charts",
    "href": "EPPS6356_Assignment4.html#table-with-embedded-charts",
    "title": "Assignment 4",
    "section": "2) Table with Embedded Charts",
    "text": "2) Table with Embedded Charts\nSmall-multiple histograms (Species × Measurement). Muted fills; units shown in facet labels.\n\npeng_long &lt;- peng |&gt;\n  select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt;\n  pivot_longer(-species, names_to = \"measurement\", values_to = \"value\") |&gt;\n  mutate(measurement = factor(measurement,\n         levels = c(\"bill_length_mm\",\"bill_depth_mm\",\"sepal_dummy\",\"flipper_length_mm\",\"body_mass_g\"),\n         labels = c(\"Bill length (mm)\",\"Bill depth (mm)\",\"_\",\"Flipper length (mm)\",\"Body mass (g)\")))\n\n# Use species-specific muted colors\nfill_map &lt;- c(\"Adelie\" = col_setosa, \"Chinstrap\" = col_versicolor, \"Gentoo\" = col_virginica)\n\np2 &lt;- ggplot(peng_long, aes(value, fill = species)) +\n  geom_histogram(color = \"white\", bins = 12) +\n  facet_grid(species ~ measurement, scales = \"free_x\") +\n  scale_fill_manual(values = fill_map, guide = \"none\") +\n  labs(title = \"Distributions of Penguin Measurements\",\n       x = \"Measurement Value\", y = \"Count\",\n       subtitle = \"Each row is a species; each column is a measurement\",\n       caption = \"Data: palmerpenguins\") +\n  theme(\n    strip.text = element_text(face = \"bold\"),\n    panel.grid = element_blank(),\n    panel.border = element_rect(color = \"#E6E6E6\", fill = NA)\n  )\n\np2\n\nWarning: Removed 8 rows containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "EPPS6356_Assignment4.html#bar-chart",
    "href": "EPPS6356_Assignment4.html#bar-chart",
    "title": "Assignment 4",
    "section": "3) Bar Chart",
    "text": "3) Bar Chart\nTwo parallel charts (same design) so it’s easier to read — Adelie and Chinstrap body mass for every observed penguin.\n\nmany &lt;- peng |&gt;\n  dplyr::filter(species %in% c(\"Adelie\",\"Chinstrap\"), !is.na(body_mass_g)) |&gt;\n  dplyr::group_by(species) |&gt;\n  dplyr::arrange(body_mass_g, .by_group = TRUE) |&gt;\n  dplyr::mutate(id = factor(row_number())) |&gt;\n  dplyr::ungroup()\n\nfill_map &lt;- c(\"Adelie\" = col_setosa, \"Chinstrap\" = col_versicolor)\n\np3 &lt;- ggplot(many, aes(id, body_mass_g, fill = species)) +\n  geom_col(width = 0.65, color = \"white\") +\n  coord_flip() +\n  facet_wrap(~ species, ncol = 2, scales = \"free_y\") +\n  scale_fill_manual(values = fill_map, guide = \"none\") +\n  scale_y_continuous(\"Body mass (g)\",\n                     breaks = scales::pretty_breaks(n = 6),\n                     labels = label_number(accuracy = 1, scale_cut = cut_si(\"\")),\n                     expand = expansion(mult = c(0.02, 0.06))) +\n  labs(title = \"Body Mass by Individual — Many Items, Few Categories\",\n       subtitle = \"Each panel shows one species; bars ordered by body mass\",\n       x = NULL, caption = \"Data: palmerpenguins\") +\n  theme(\n    axis.text.y = element_blank(), axis.ticks.y = element_blank(),\n    axis.text.x = element_text(color = \"#333333\")\n  )\n\np3"
  },
  {
    "objectID": "EPPS6356_Assignment4.html#column-chart",
    "href": "EPPS6356_Assignment4.html#column-chart",
    "title": "Assignment 4",
    "section": "4) Column Chart",
    "text": "4) Column Chart\nFew items shown cleanly: means of four measurements by species, faceted to respect units.\n\nmeans_long &lt;- peng |&gt;\n  dplyr::group_by(species) |&gt;\n  dplyr::summarise(\n    `Bill depth (mm)`     = mean(bill_depth_mm,     na.rm = TRUE),\n    `Bill length (mm)`    = mean(bill_length_mm,    na.rm = TRUE),\n    `Body mass (g)`       = mean(body_mass_g,       na.rm = TRUE),\n    `Flipper length (mm)` = mean(flipper_length_mm, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  tidyr::pivot_longer(-species, names_to = \"measurement\", values_to = \"mean_value\") |&gt;\n  dplyr::group_by(measurement) |&gt;\n  dplyr::mutate(index = 100 * mean_value / mean(mean_value, na.rm = TRUE)) |&gt;\n  dplyr::ungroup()\n\n# 2) Manual x positions -&gt; bars touch inside each trio, gap between trios\nmeas_levels &lt;- c(\"Bill depth (mm)\", \"Bill length (mm)\", \"Body mass (g)\", \"Flipper length (mm)\")\nspec_levels &lt;- c(\"Adelie\",\"Chinstrap\",\"Gentoo\")\n\nbars_per_group &lt;- length(spec_levels)  # 3\ngap_units      &lt;- 1                    # tweak this to widen/narrow gaps between categories\n\nplot_df &lt;- means_long |&gt;\n  dplyr::mutate(\n    measurement = factor(measurement, levels = meas_levels),\n    species     = factor(species,     levels = spec_levels),\n    g_idx = as.integer(measurement) - 1,\n    s_idx = as.integer(species)     - 1,\n    x     = g_idx * (bars_per_group + gap_units) + s_idx\n  )\n\n# axis breaks at the center of each trio\nbreaks &lt;- ((0:(length(meas_levels)-1)) * (bars_per_group + gap_units)) + (bars_per_group - 1)/2\nlabels &lt;- meas_levels\n\n# 3) Muted palette\npal_grouped &lt;- c(\"Adelie\"=\"#5B7EA3\", \"Chinstrap\"=\"#B58B45\", \"Gentoo\"=\"#6BAA75\")\n\np4 &lt;- ggplot(plot_df, aes(x = x, y = index, fill = species)) +\n  geom_col(width = 1, color = NA) +  # 0.98 so bars in a trio visually touch\n  scale_x_continuous(breaks = breaks, labels = labels, expand = expansion(mult = c(0.02, 0.04))) +\n  scale_fill_manual(values = pal_grouped, name = \"Species\") +\n  scale_y_continuous(\"Index (measurement mean = 100)\", expand = expansion(mult = c(0, 0.05))) +\n  labs(\n    title = \"Mean Penguin Measurements by Species (Indexed)\",\n    x = \"Measurement\",\n    caption = \"Data: palmerpenguins\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(\n    plot.title  = element_text(face = \"bold\", size = 18),\n    axis.text.x = element_text(angle = 12, hjust = 1, vjust = 1),\n    panel.grid.minor = element_blank()\n  )\n\np4"
  },
  {
    "objectID": "EPPS6302_Assignment5.html#quick-index",
    "href": "EPPS6302_Assignment5.html#quick-index",
    "title": "EPPS6302 — Assignment 5: Government Data API",
    "section": "",
    "text": "Index of Retrieved PDFs\n\n\nfile\nsize_kb\nmodified\n\n\n\n\nyourpathgovfiles_1.pdf\n227.7\n2025-10-13 20:04:17\n\n\nyourpathgovfiles_10.pdf\n267.1\n2025-10-13 20:05:18.175671\n\n\nyourpathgovfiles_2.pdf\n226.5\n2025-10-13 20:04:58.541353\n\n\nyourpathgovfiles_3.pdf\n213.1\n2025-10-13 20:05:00.343799\n\n\nyourpathgovfiles_4.pdf\n225.1\n2025-10-13 19:51:48\n\n\nyourpathgovfiles_5.pdf\n225.1\n2025-10-13 20:05:06.081634\n\n\nyourpathgovfiles_6.pdf\n225.3\n2025-10-13 19:51:53\n\n\nyourpathgovfiles_7.pdf\n225.1\n2025-10-13 20:05:10.71113\n\n\nyourpathgovfiles_8.pdf\n309.0\n2025-10-13 19:51:58\n\n\nyourpathgovfiles_9.pdf\n269.3\n2025-10-13 19:52:01"
  },
  {
    "objectID": "EPPS6302_Assignment5_links_only_WORKING.html",
    "href": "EPPS6302_Assignment5_links_only_WORKING.html",
    "title": "EPPS6302 — Assignment 5: Government Data API",
    "section": "",
    "text": "The following PDFs are the files produced by my R script (govtdata01-style) and saved in this working folder. I’m linking them directly here as proof of retrieval. (If fewer than 10 appear, that reflects what the script returned at run time on my machine.)\n\nyourpathgovfiles_1.pdf\nyourpathgovfiles_2.pdf\nyourpathgovfiles_3.pdf\nyourpathgovfiles_4.pdf\nyourpathgovfiles_5.pdf\nyourpathgovfiles_6.pdf\nyourpathgovfiles_7.pdf\nyourpathgovfiles_8.pdf\nyourpathgovfiles_9.pdf\nyourpathgovfiles_10.pdf\n\n\n\n\n\n\nIndex of Retrieved PDFs\n\n\nfile\nsize_kb\nmodified\n\n\n\n\nyourpathgovfiles_1.pdf\n227.7\n2025-10-13 20:04:17\n\n\nyourpathgovfiles_2.pdf\n226.5\n2025-10-13 20:04:58.541353\n\n\nyourpathgovfiles_3.pdf\n213.1\n2025-10-13 20:05:00.343799\n\n\nyourpathgovfiles_4.pdf\n225.1\n2025-10-13 19:51:48\n\n\nyourpathgovfiles_5.pdf\n225.1\n2025-10-13 20:05:06.081634\n\n\nyourpathgovfiles_6.pdf\n225.3\n2025-10-13 19:51:53\n\n\nyourpathgovfiles_7.pdf\n225.1\n2025-10-13 20:05:10.71113\n\n\nyourpathgovfiles_8.pdf\n309.0\n2025-10-13 19:51:58\n\n\nyourpathgovfiles_9.pdf\n269.3\n2025-10-13 19:52:01\n\n\nyourpathgovfiles_10.pdf\n267.1\n2025-10-13 20:05:18.175671"
  },
  {
    "objectID": "EPPS6302_Assignment5_links_only_WORKING.html#quick-index",
    "href": "EPPS6302_Assignment5_links_only_WORKING.html#quick-index",
    "title": "EPPS6302 — Assignment 5: Government Data API",
    "section": "",
    "text": "Index of Retrieved PDFs\n\n\nfile\nsize_kb\nmodified\n\n\n\n\nyourpathgovfiles_1.pdf\n227.7\n2025-10-13 20:04:17\n\n\nyourpathgovfiles_2.pdf\n226.5\n2025-10-13 20:04:58.541353\n\n\nyourpathgovfiles_3.pdf\n213.1\n2025-10-13 20:05:00.343799\n\n\nyourpathgovfiles_4.pdf\n225.1\n2025-10-13 19:51:48\n\n\nyourpathgovfiles_5.pdf\n225.1\n2025-10-13 20:05:06.081634\n\n\nyourpathgovfiles_6.pdf\n225.3\n2025-10-13 19:51:53\n\n\nyourpathgovfiles_7.pdf\n225.1\n2025-10-13 20:05:10.71113\n\n\nyourpathgovfiles_8.pdf\n309.0\n2025-10-13 19:51:58\n\n\nyourpathgovfiles_9.pdf\n269.3\n2025-10-13 19:52:01\n\n\nyourpathgovfiles_10.pdf\n267.1\n2025-10-13 20:05:18.175671"
  },
  {
    "objectID": "EPPS6356_PrepareforClass6.html",
    "href": "EPPS6356_PrepareforClass6.html",
    "title": "Prepare for Class 6",
    "section": "",
    "text": "How to use this file: These are my completed notes and answers for discussion. I kept them concise, specific, and actionable."
  },
  {
    "objectID": "EPPS6356_PrepareforClass6.html#reading-notes",
    "href": "EPPS6356_PrepareforClass6.html#reading-notes",
    "title": "Prepare for Class 6",
    "section": "1. Reading Notes",
    "text": "1. Reading Notes\nRequired: Cleveland, W. S., & McGill, R. (1984). Graphical perception: Theory, experimentation, and application to the development of graphical methods. JASA, 79(387), 531–554.\nOptional: Cleveland (1993) Visualizing Data; KH Ch. 2–4.\n\n1.1 Core Ideas (3–5 bullets)\n\nPerceptual task hierarchy: Judgments are most accurate with position on a common scale, then position on nonaligned scales, length, angle/slope, area, volume, and finally color hue/saturation. Design should push comparisons toward the top of this ladder.\nExperimental method: Subjects made pairwise quantitative judgments across encodings. Error increased as encodings moved down the hierarchy; direct labeling and consistent baselines improved accuracy.\nImplications: Prefer dot plots, bars (zero baseline), and aligned lines over pies, bubbles, and 3D shapes. Use small multiples for multi-category trends instead of dual y-axes.\nCaveats: Context matters—range, aspect ratio, and scale choice can change difficulty; over-aggregation and clutter add cognitive load even with “good” encodings.\n\n\n\n1.2 Examples (good vs. weak)\n\nGood: A small-multiple dot plot comparing graduation rates by district and year on a shared x-axis. Viewers compare position quickly and precisely.\nWeak: A pie chart showing six programs’ budget shares that differ by only 1–3%. Angles are hard to discriminate; replace with a sorted bar chart (0 baseline), add direct labels and percent signs, and show total n for context."
  },
  {
    "objectID": "EPPS6356_PrepareforClass6.html#mckinsey-week-in-charts-year-in-charts-2024",
    "href": "EPPS6356_PrepareforClass6.html#mckinsey-week-in-charts-year-in-charts-2024",
    "title": "Prepare for Class 6",
    "section": "2. McKinsey “Week in Charts” & “Year in Charts” (2024)",
    "text": "2. McKinsey “Week in Charts” & “Year in Charts” (2024)\n(I reviewed multiple pieces from each series and focused on recurring design patterns rather than one-off exceptions.)\n\n2.1 Quick Scan — What they do well\n\nClear headlines that state the takeaway (“X overtakes Y by 2028”).\n\nAnnotation-first style: callouts, arrows, and direct labels reduce legend hopping.\n\nTidy scales with restrained grids; frequent use of small multiples for regional splits.\n\nSane color use: neutrals for context, accent color for the focal series.\n\n\n\n2.2 Side-by-Side Compare (Picked representative patterns)\nCreate a small comparison table:\n\n\n\n\n\n\n\n\n\nCriterion\nWeek in Charts — Example A\nYear in Charts — Example B\nNotes\n\n\n\n\nQuestion clarity\nSharp, one-sentence claim (“Hiring cooled vs. last quarter”)\nBroader synthesis (“Decade of decarbonization progress”)\nWeekly posts target one nugget; yearly posts summarize themes.\n\n\nEncoding choice\nLine or bar emphasizing position/length\nMix of small multiples, slopegraphs, and stacked bars\nBoth mostly align with Cleveland–McGill; stacks used sparingly.\n\n\nScale & axis\nUsually linear; bars start at zero\nMix of linear/log for wide ranges; clear tick labels\nYearly pieces sometimes justify non-zero baselines or log.\n\n\nAnnotation\nShort text plus direct labels at ends of lines\nLonger captions; subtitles add context and caveats\nAnnotation density higher in yearly roundups.\n\n\nCognitive load\nLow—single idea per chart\nMedium—comparisons across sectors/regions\nYearly: add section dividers and mini legends to chunk info.\n\n\n\n\n\n2.3 What could be better (actionable fixes)\n\nReplace remaining pies and stacked area with bars + small multiples when precise part-to-whole or category change matters.\n\nFavor direct labeling over color legends in dense multi-series lines.\n\nStandardize y-axis ranges across small multiples to support scanning.\n\nWhere uncertainty is material, add CIs/ribbons or data source notes near the chart.\n\nAdd accessible alt text and ensure colorblind-safe palettes (e.g., avoid red–green pairings)."
  },
  {
    "objectID": "EPPS6356_PrepareforClass6.html#library-guides-what-to-borrow",
    "href": "EPPS6356_PrepareforClass6.html#library-guides-what-to-borrow",
    "title": "Prepare for Class 6",
    "section": "3. Library Guides — What to Borrow",
    "text": "3. Library Guides — What to Borrow\nLinks: - Duke University Library Data Viz Guide: https://guides.library.duke.edu/datavis\n- UC Berkeley Data Visualization: https://guides.lib.berkeley.edu/data-visualization\n\n3.1 Takeaways from Duke\n\nStart with the question and audience; write the one-line message first.\n\nPrototyping loop: sketch → build minimal → get feedback → iterate.\n\nEthics & accessibility: show sources, methods, and uncertainty; meet color contrast; label units.\n\nMaintain a reproducible workflow (document code, versions, and data cleaning).\n\n\n\n3.2 Takeaways from Berkeley\n\nPipeline mindset: problem → data → encoding → iteration → evaluation.\n\nPrefer position/length encodings; avoid 3D and dual axes.\n\nBuild for readability at small sizes (12–14pt text, direct labels), and add alt text and ARIA for web.\n\n\n\n3.3 Answer: What are the “Four Pillars of Visualization”?\n\nPhrasing varies across sources. I adopt this framing, which is consistent with the guides above and the Cleveland–McGill evidence:\n\n\nPurpose — What question and who is the audience? Define the one-sentence takeaway and the action it should enable.\n\nData — Are the data suitable, trustworthy, and ethically sourced? Document cleaning, units, uncertainty, and limitations.\n\nDesign — Choose the strongest encodings, honest scales, and uncluttered layouts; use small multiples and direct labels.\n\nNarrative & Annotation — Titles, subtitles, callouts, and captions that tell the reader exactly how to read the chart and why it matters."
  },
  {
    "objectID": "EPPS6356_PrepareforClass6.html#my-design-checklist-for-upcoming-assignments",
    "href": "EPPS6356_PrepareforClass6.html#my-design-checklist-for-upcoming-assignments",
    "title": "Prepare for Class 6",
    "section": "4. My Design Checklist (for upcoming assignments)",
    "text": "4. My Design Checklist (for upcoming assignments)\n\nStart with the question & audience; write the takeaway first.\n\nChoose position/length encodings before angle/area/color; avoid 3D.\n\nUse honest scales; show zero for bars or justify exceptions; avoid dual axes.\n\nDirect labels where possible; minimize legend hopping; annotate the story beats.\n\nAccessibility: colorblind-safe palette; sufficient text contrast; alt text for web.\n\nShow uncertainty and sample sizes where relevant.\n\nPrefer small multiples over cluttered single panels.\n\nKeep data-ink high; remove chartjunk; test with a non-expert peer."
  },
  {
    "objectID": "EPPS6356_PrepareforClass6.html#optional-quick-r-setup-chunk",
    "href": "EPPS6356_PrepareforClass6.html#optional-quick-r-setup-chunk",
    "title": "Prepare for Class 6",
    "section": "5. (Optional) Quick R Setup Chunk",
    "text": "5. (Optional) Quick R Setup Chunk\n\n# Uncomment if you want a ready environment for sketching\n# install.packages(c(\"ggplot2\",\"readr\",\"dplyr\",\"tidyr\",\"forcats\",\"scales\"))\n# library(ggplot2); library(readr); library(dplyr); library(tidyr); library(forcats); library(scales)\n\n# Example skeleton for a comparison dot plot (replace with your data)\n# df &lt;- tibble::tibble(category = LETTERS[1:5], value = c(10, 14, 8, 13, 9))\n# ggplot(df, aes(x = value, y = forcats::fct_reorder(category, value))) +\n#   ggtitle(\"Comparison Dot Plot\") +\n#   geom_point(size = 3) +\n#   labs(x = \"Value (units)\", y = NULL, caption = \"Source: ...\") +\n#   theme_minimal(base_size = 13)"
  },
  {
    "objectID": "EPPS6356_PrepareforClass6.html#reflection-35-sentences",
    "href": "EPPS6356_PrepareforClass6.html#reflection-35-sentences",
    "title": "Prepare for Class 6",
    "section": "6. Reflection (3–5 sentences)",
    "text": "6. Reflection (3–5 sentences)\nI will convert any pie/stacked-area comparisons in my drafts to sorted bars or dot plots with direct labels. I will standardize axes across small multiples and keep bar charts pinned to zero unless a non-zero baseline is defensible and clearly signposted. I will add uncertainty ribbons or CIs when readers could otherwise over-read a trend. Finally, I will run a quick peer test: can a non-expert state the one-sentence takeaway in 10 seconds?"
  },
  {
    "objectID": "EPPS6356_PrepareforClass5.html#one-page-note-big-data-pitfalls-and-over-fitting",
    "href": "EPPS6356_PrepareforClass5.html#one-page-note-big-data-pitfalls-and-over-fitting",
    "title": "Prepare for Class 5",
    "section": "",
    "text": "Big data has reshaped how we understand social behavior, public health, and markets but with its scale comes illusion. The Parable of Google Flu, published in Nature, remains a cautionary tale. In 2008, Google’s algorithm predicted flu outbreaks by analyzing search queries, claiming to forecast epidemics faster than the CDC. For a brief period, the results looked revolutionary until they stopped working. The algorithm overestimated flu cases by more than 140%, revealing that the apparent success of “big data” can mask deeper analytical flaws.\nThe first pitfall is data without theory. Massive datasets tempt analysts to treat correlation as causation. Without grounding in domain knowledge, we misinterpret noise as signal. In the Google Flu case, rising media attention and public anxiety, rather than infection rates, drove search trends. This created a self-reinforcing feedback loop that the model mistook for real-world growth. Tufte’s warning about “seeing without thinking” echoes here: visualization and data alike require interpretation, not just computation.\nAnother pitfall is algorithmic opacity. Big data often relies on proprietary, ever-changing systems. When inputs, models, or weighting methods are unknown, replication becomes impossible. This erodes scientific transparency and accountability, two pillars of credible data analysis. Furthermore, as datasets grow, bias does not disappear; it scales. If underlying data are incomplete, unrepresentative, or contextually skewed, bigger datasets simply reproduce error with greater confidence.\nClosely related is the danger of overfitting and overparameterization. In large models, especially machine learning systems, it’s easy to fit data so perfectly that the model memorizes patterns rather than generalizes them. This creates high accuracy in training but failure in prediction. Overfitting is a form of overconfidence disguised as precision it reflects modeling power used without restraint. The solution, as both Hadley Wickham and Andrew Gelman emphasize, lies in simplicity, cross-validation, and clarity: build models that explain rather than impress.\nEthically, the pitfalls of big data remind us that quantity does not equal quality. A million observations cannot compensate for poor measurement, biased sampling, or conceptual confusion. The role of the analyst is to filter wisely, visualize responsibly, and remain skeptical of “too-good-to-be-true” correlations. In the end, meaningful data analysis depends not on how much data we have, but on how thoughtfully we use it."
  }
]